13
Video
14
Internet
15
Data
Data
files
16
Compressed
Backup
WinZip,
WinRar
types
were
incorporated
as
they
were
identified
during
the
audit.
On
average
the
audit
tool
was
able
to
automatically
classify
more
than
75
of
files
within
a
users
file
space
and
in
many
cases
almost
95
see
table
7
in
the
results
section,
below
.
4.
RESULTS
The
results
of
the
study
are
presented
in
Figures
4
to
9
respectively
and
discussed
in
the
following
sections
under
the
five
areas
of
background,
personal
strategies,
improving
access
and
management,
file
space
audit
and
file
space
analysis.
4.1
Background
Of
the
40
engineers
studied,
only
twelve
use
a
single
computer,
twenty
use
two
computers,
and
eight
use
three
computers
part
a
of
Figure
4
.
Unsurprisingly,
almost
90
of
engineers
use
a
desktop
PC
as
their
primary
computer,
and
of
the
28
who
use
two
computers,
almost
70
use
a
laptop
as
their
secondary
computer.
Of
the
eight
engineers
who
use
a
third
computer,
no
real
preference
emerged
for
its
type,
although
two
participants
specifically
describe
accessing
UNIX
Workstations.
Those
who
accessed
the
UNIX
Workstations
did
so
because
they
needed
to
run
software
which
ran
only
on
those
computers.
4.1.1
Archiving
and
Backup.
For
the
purpose
of
this
study
a
distinction
is
made
between
the
tasks
of
archiving
and
backup.
Archiving
denotes
the
organization
and
storage
of
files
for
their
long-term
preservation
and
typically
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
23,
Publication
date:
September
2008.
23:14
B.
J.
Hicks
et
al.
Table
V.
File
Classes
and
Associated
File
Types
Extensions
File
Class
Text
Spreadsheet
Presentation
Database
Project
Graphics
CAD
Software
Applications
MS
Word,
Adobe,
Text,
Rich
Text,
Lotus
notes
MS
Excel,
Lotus
MS
PowerPoint
MS
Access,
dBase,
SQL
Microsoft
Project,
MPMM,
Visio
Adobe
PhotoShop,
Corel
Draw,
MS
Publisher,
Solid
Edge,
UniGraphics,
CATIA,
PRO
Engineer,
AutoCAD
Visual
Basic,
C,
C
,
PHP,
Perl,
Lisp,
Fortran,
Pascal
File
Types
extensions
doc,
rtf,
pdf,
txt,
ps,
dot,
one
xls,
imp,
wk
pps,
ppt,
pot
mdb,
db
,
odb,
sql
mpp,
mpm,
vsd
pub,
cdr,
psp
sld,
prt,
dxf,
dwg,
dwf,
prt,
asm,
drw,
igs,
mod,
dlv,
exp,
stl,
step
vbp,
vba,
Visual
FoxPro,
c,
c
,
php,
pl,
aps,
f,
f77,
h,
h
,
p,
l,
pas
fil,
mdl,
inp,
mat,
ms,
mcd,
sav,
ctt,
vit,
mfr,
mll,
std
Code
Simulation
Applications
Image
ABAQUS,
ANSYS,
LS-DYNA,
Maple,
MathCAD,
Simulink,
MAtrixX,
LabView,
SPSS,
Moldflow,
CFD,
3
D
Studio
Executables,
Installation,
Initialization
Variety
of
standard
formats
exe,
msi,
ini
jpeg,
jpg,
gif,
tif,
tiff,
bmp,
png,
wmf,
eps,
pic
mp3,
mpeg,
wav,
wma,
cda
mpeg,
avi,
qt
html,
htm,
js,
jss,
php,
swf,
xsl,
asp,
cfm
dat,
csv,
xml
zip,
gzip,
tar,
zip,
bak,
tar,
arc,
tgz
Audio
Video
Internet
Variety
of
standard
formats
Variety
of
standard
formats
Variety
of
standard
formats
Data
Compressed
Data
files
and
Comma
Separated
Variables
WinZip,
WinRar,
Backup
files,
Archives
includes
a
master
version
of
a
file
that
will
be
used
for
future
reference
including,
for
example,
commercial
or
legal
disputes.
In
engineering
such
archives
are
commonly
produced
for
each
project
and
for
a
particular
customer.
In
contrast
to
archiving,
the
task
of
backup
is
a
shorter-term
activity
involving
the
generation
of
a
copy
of
a
file
system
or
drive
for
the
purpose
of
restoring
working
files
should
the
need
arise.
These
backups
are
typically
generated
at
regular
intervals
and
hence
organized
in
a
chronological
manner.
In
terms
of
electronic
file
storage,
primary
computers
are
used
by
over
90
of
respondents
to
store
files,
by
70
to
archive
older
files
and
by
55
to
backup
files
part
b
of
Figure
4
.
The
general
trend
is
a
decreasing
percentage
of
participants
storing,
archiving
and
backing-up
files
on
subsequent
computers,
despite
the
fact
that
the
values
are
calculated
using
only
the
number
of
participants
using
a
second
or
third
computer.
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
23,
Publication
date:
September
2008.
Organizing
and
Managing
Personal
Electronic
Files
23:15
Fig.
4.
Background
data
and
levels
of
file
sharing
of
participants.
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
23,
Publication
date:
September
2008.
23:16
B.
J.
Hicks
et
al.
4.1.2
File
Exchange.
In
order
to
investigate
the
level
of
file
exchange
a
distinction
is
made
between
the
modes
of
file
sharing
and
file
exchange.
File
sharing
is
considered
to
involve
the
exchange
of
personal
files
from
one
computer
to
another,
whilst
file
sharing
occurs
when
multiple
users
computers
have
access
to
the
same
instance
of
a
file
residing
in
a
particular
location.
If
a
user
transfers
a
file
from
one
personal
computer
to
another
via
methods
including
email
as
an
attachment
,
file
copying,
or
USB
memory
sticks,
this
is
considered
file
exchange.
If
a
user
shares
a
file
using
shared
disk
drives
including
Windows
file
sharing
or
network
file
servers,
this
is
considered
file
sharing.
Of
particular,
interest
in
this
study
was
the
level
of
exchange
and
sharing
that
occurs
across
departments,
within
project
teams
and
with
suppliers,
customers,
subcontractors,
and
partners.
The
results
are
shown
in
parts
c
and
d
of
Figure
4
and
reveal
that
the
majority
of
participants
rarely
exchange
or
share
files
with
suppliers,
customers
or
subcontractors.
However,
a
significant
proportion
of
the
sample
45
were
based
in
a
university
research
centre,
and
whilst
these
projects
involve
a
large
number
of
industrial
collaborators,
it
is
arguable
that,
due
to
the
nature
of
the
work,
fewer
suppliers
and
or
customers
are
likely
to
be
involved.
The
results
suggest
that
files
are
most
commonly
exchanged
and
shared
at
a
project
level,
with
60
of
participants
claiming
to
frequently
exchange
files
with
fellow
project
members.
Files
are
also
regularly
exchanged
and
shared
across
different
departments
and
with
collaborators.
One
overall
trend
that
emerges
from
the
study
is
that
files
are
more
commonly
exchanged
than
they
are
shared,
regardless
of
whether
the
recipient
is
internal
or
external.
4.2
Personal
Strategies
When
considering
file
naming
strategies,
over
75
of
engineers
use
the
title
of
the
document,
60
also
use
the
purpose
or
function
e.g.,
maximum
stress
in
wing
spar
or
revised
costs
for
project
meeting
,
50
the
project
title
e.g.,
A380M
or
LG
carrier
and
45
the
date.
The
latter
is
particularly
common
for
files
containing
meeting
minutes
and
report
drafts.
The
relative
utilization
of
naming
criteria
is
shown
in
part
a
of
Figure
5.
A
number
of
additional
criteria
were
also
listed
by
the
participants
and
include:
--
The
name
of
the
person
to
whom
the
file
is
being
sent,
for
example,
project
costs
for
tony.xls
--
A
memorable
nickname
to
make
the
content
easily
recognizable
--
Document
revision
information,
for
example,
annual
report
version
2.doc
--
A
brief
description
of
the
file
content,
for
example,
output
from
model
of
cantilever.ans
--
The
file
type,
for
example,
word
document.doc.
Some
of
the
participants
also
described
using
different
naming
criteria
for
files
depending
on
their
level
in
the
directory
structure.
Where
directories
are
considered,
two
criteria
emerge
as
by
far
the
most
common
see
part
b
of
Figure
5
.
These
are
the
purpose
or
function
of
the
files
contained
in
the
directory
for
example,
budgets
85
of
participants
and
the
project
title
55
of
participants
.
This
suggests
that
directories
are
commonly
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
23,
Publication
date:
September
2008.
Organizing
and
Managing
Personal
Electronic
Files
23:17
Fig.
5.
The
naming
criteria
used
by
engineers
for
files
and
directories.
organized
using
a
function
and
project
hierarchy.
Following
these
two
criteria,
the
date
was
the
next
most
prevalent,
although
only
20
of
participants
used
this.
Other
criteria
listed
by
the
participants
for
naming
and
organizing
directories
are:
--
Subject
for
example,
aerodynamics
--
Work
theme
for
example,
maintenance
--
The
year
as
opposed
to
the
full
date
--
File
type
for
example,
presentations
As
part
of
the
study
participants
were
asked
to
indicate
if
their
strategy
was
personal
or
recommended
by
the
organization.
For
all
of
the
participants
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
23,
Publication
date:
September
2008.
23:18
B.
J.
Hicks
et
al.
their
strategies
were
largely
personal.
There
was
only
one
exception
where
the
engineer--who
had
been
saving
the
majority
of
files
on
the
desktop--had
been
instructed
by
Computer
Services
to
clear
all
files
from
their
desktop
and
place
them
in
theme-based
directories.
The
prevalence
of
cognitive
reference
points
relating
to
the
purpose
and
use
of
a
file
is
congruent
with
results
from
studies
by
Kwasnik
1989
,
where
it
was
shown
that
these
non-concrete
dimensions
have
a
greater
influence
in
determining
the
classification
of
a
document
within
a
personal
office.
4.3
Improving
Access
and
Management
The
third
section
of
the
questionnaire
required
the
participants
to
rate
the
relative
importance
of
file
indexing
and
searching
criteria,
and
the
functionality
and
performance
of
file
management
systems
on
a
scale
of
1
to
5.
The
results
are
shown
in
parts
a
and
b
of
Figure
6.
In
terms
of
indexing
or
search
criteria,
twelve
alternatives
were
presented,
including
specific
context,
project
value,
project
team,
engineer,
customer
supplier,
file
size,
keywords,
filename,
date
modified,
date
created,
author,
and
organization.
What
is
immediately
obvious
from
part
a
is
that
nearly
all
criteria
are
rated
above
three
with
the
exception
of
file
size
and
project
value
which
are
rated
at
2
and
2.5
respectively.
The
most
highly
ranked
criterion
was
specific
context
with
an
average
score
of
4.1.
This
correlates
well
with
the
findings
in
5.2,
where
specific
purpose
or
function
are
commonly
used
to
name
and
organize
files
and
directories.
Of
the
remaining
criteria,
filename
and
keywords,
both
score
an
average
of
four,
with
project
team
and
author
also
scoring
above
average.
In
part
b
of
Figure
6,
the
relative
importance
of
functionality
and
performance
of
the
file
management
system
are
presented.
In
total
seven
aspects
were
considered:
the
ability
to
save
to
multiple
locations,
a
classification
feature,
archiving
older
files,
project
specific
structure,
security
access
restrictions,
accuracy
reliability
of
search,
speed
response
of
search,
speed
of
naming
renaming
file.
The
results
reveal
that
accuracy
and
reliability
of
searching
average
score
4.2
,
as
well
as
the
response
time
of
a
search
are
the
most
highly
ranked
functions
for
an
electronic
file
management
system.
With
an
average
score
of
3.8,
the
speed
of
naming
and
manipulating
files
is
also
important.
In
a
similar
manner
to
the
indexing
criteria,
all
of
the
functions
presented
in
the
study
returned
an
above
average
score.
4.4
File
Space
Audit
The
results
of
the
audits
of
the
40
file
spaces
are
presented
in
Table
VI.
For
each
dimension,
the
average
value,
highest
value,
lowest
value
and
standard
deviation
are
presented.
As
previously
described
in
Section
3.2.2
the
audit
tool
provided
the
option
to
include
directories
and
files
across
multiple
root
directories.
However,
of
the
40
engineers
that
participated
in
the
study,
only
4
had
more
than
one
root
directory.
In
one
case,
five
root
directories
were
used
to
store
files.
In
general,
the
directories
were
all
hierarchical
in
nature,
structured
over
an
average
of
8.7
levels
with
a
standard
deviation
of
2.67
.
The
maximum
number
of
directory
levels
was
14
and
the
lowest
was
3.
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
23,
Publication
date:
September
2008.
Organizing
and
Managing
Personal
Electronic
Files
23:19
Fig.
6.
The
relative
importance
of
functionality
and
performance
of
file
management
systems
by
engineers.
In
addition
to
this,
the
average
size
of
all
files
stored
in
each
directory
is
also
determined.
This
shows
that
the
average
size
of
files
stored
within
a
single
level
of
the
hierarchy
is
approximately
285Mb.
The
average
size
of
all
files
in
an
individual
directory
is
7.8Mb
and
613Kb
for
an
individual
file.
In
addition
to
auditing
the
properties
of
all
directories
and
files,
it
is
also
interesting
to
evaluate
the
percentage
of
total
directories
and
files
residing
within
each
level
of
the
hierarchy
14
levels
in
the
highest
case
.
The
overall
percentage
of
total
directories
and
files
within
each
directory
level
is
shown
in
Figure
7.
This
reveals
a
bell
shaped
distribution
curve
with
a
slight
skew
to
the
left.
It
has
been
previously
observed
that
the
mean
number
of
directory
levels
contained
within
an
engineer
s
file
space
is
8.6.
Figure
7
also
reveals
that
the
majority
of
files
are
stored
in
the
third,
fourth
and
fifth
levels,
with
the
fourth
directory
level
being
the
most
heavily
used.
In
fact,
54.6
of
all
directories
and
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
23,
Publication
date:
September
2008.
23:20
B.
J.
Hicks
et
al.
Table
VI.
Personal
File
Space
Properties
File
Space
Properties
Average
High
1.19
5
8.67
14
155
774
406
2,314
4,660
30,849
2,578,076
12,675,176
Properties
per
Directory
Level,
Directory
and
File
Number
of
Directories
per
Directory
Level
41.8
192.8
Number
of
Files
in
Directory
Level
494.8
2,570.8
Number
of
Files
in
each
Directory
11.6
33.6
Size
of
Directory
Level
KB
287,885
1,260,063
Size
of
each
Directory
KB
7,836
51,328
Average
File
Size
KB
613
4,865
Duplicate
Directory
and
Filenames
Directories
with
a
shared
name
161
1,035
of
Total
Directories
31.3
75.4
Files
with
a
shared
name
1,925
11,614
of
Total
Files
32.4
83.1
Total
Root
Directories
Directory
Levels
File
Types
Directories
Files
File
Size
KB
Low
1
3
5
13
200
5,856
4.3
40.0
3.07
1,171
145
29
0
0.0
4
1.1
SD
0.71
2.67
148
431
5,674
3,432,667
36.8
502
6.91
372,463
11,706
871
237
18.4
2,922
19.9
Fig.
7.
Cumulative
distribution
of
directories
and
files
over
participants
file
spaces.
53.5
of
all
the
files
are
contained
in
these
levels.
25.7
of
directories
and
24.6
of
all
files
are
contained
in
the
second
and
sixth
levels,
with
15.2
of
directories
and
15.4
of
files
maintained
in
the
seventh
and
eighth
levels.
The
first
level
of
the
hierarchy
tends
to
be
used
primarily
for
subdirectories
hence
the
proportion
of
total
directories
and
files
contained
is
very
low.
The
lower
levels
beyond
the
eighth
level
will
also
tend
to
contain
a
very
low
proportion
of
the
total
directories
and
files,
if
indeed
they
exist
at
all.
These
observations
are
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
23,
Publication
date:
September
2008.
Organizing
and
Managing
Personal
Electronic
Files
Table
VII.
Personal
File
Space
Properties
File
Class
Text
Spreadsheet
Presentation
Database
Project
Graphics
CAD
Code
Simulation
Application
Image
Audio
Video
Internet
Data
Compressed
Unknown
Av.
20.5
4.27
1.73
0.57
0.21
0.38
3.99
7.48
1.22
1.45
20.0
1.25
0.19
5.91
4.52
2.47
23.9
File
Class
Analysis
Files
Total
High
Low
SD
77.0
0.00
18.9
40.4
0.00
7.38
7.35
0.00
1.96
11.0
0.00
1.70
6.28
0.00
0.97
3.59
0.00
0.74
57.6
0.00
10.5
31.3
0.00
8.34
25.0
0.00
4.22
6.09
0.00
1.77
47.2
0.00
12.1
35.8
0.00
5.56
1.53
0.00
0.30
26.2
0.00
6.89
17.5
0.00
4.93
87.0
0.00
13.4
100
0.18
21.2
23:21
Av.
24.3
4.24
7.50
2.21
0.10
0.20
2.02
0.76
2.05
3.09
10.5
3.84
6.98
0.51
2.69
4.92
24.2
File
Size
Total
High
Low
83.7
0.00
47.6
0.00
41.6
0.00
47.0
0.00
1.27
0.00
1.52
0.00
31.8
0.00
7.83
0.00
59.9
0.00
29.7
0.00
47.9
0.00
76.4
0.00
78.8
0.00
4.49
0.00
30.9
0.00
49.9
0.00
100
0.00
SD
20.5
9.03
9.80
8.48
0.28
0.33
6.15
1.45
9.76
5.53
10.5
13.0
16.6
0.91
5.47
8.91
24.9
further
supported
by
the
fact
that
the
proportion
of
total
directories
within
all
levels
of
the
cumulative
hierarchy
is
approximately
the
same
as
the
proportion
of
total
files,
implying
that
the
ratio
of
files
to
directories
will
be
roughly
constant
in
all
levels
of
the
folder
hierarchy
at
around
12
Table
VI
.
4.5
File
Space
Analysis
The
percentage
of
total
files
and
total
file
size
represented
by
each
class
of
file
is
given
in
Table
VII
and
shown
in
Figure
8.
The
results
indicate
that
text
files
and
image
files
are
the
most
commonly
occurring
file
types,
each
representing
an
average
of
just
over
20
of
all
files.
Following
text
and
images,
the
next
most
prevalent
file
types
are
code,
Internet,
data,
spreadsheet,
CAD,
presentation,
simulation
and
applications
which
each
represent
5
to
3
of
total
files.
Of
particular
note
is
the
fact
that
only
4
of
all
files
were
CAD
files.
This
is
surprising
given
the
ubiquitous
nature
of
CAD
systems
in
engineering
environments,
although
it
could
be
explained
by
the
critical
value
of
CAD
to
engineering
organizations
and
the
resulting
prevalence
of
well-developed
CAD
document
management
systems.
The
analysis
of
file
types
also
includes
the
relative
file
sizes
of
files
in
the
different
classes.
Unsurprisingly,
video
files
exhibit
the
highest
ratio
of
size
to
occurrence
37:1
occupying
7
of
the
total
audited
file
space,
but
only
0.2
of
total
files.
Similarly,
presentation
files,
most
commonly
produced
using
Microsoft
PowerPoint
have
a
ratio
over
4:1.
The
ratio
for
text
files,
spreadsheet
files
and
unknown
file
types
is
approximately
1:1,
whereas
images
have
a
smaller
ratio
of
1:2.
Of
particular
note
in
Table
VII
are
the
large
standard
deviations
calculated
for
each
of
the
various
file
classes.
These
values
are
included
to
highlight
the
wide
spread
of
measured
values
across
the
engineers
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
23,
Publication
date:
September
2008.
23:22
B.
J.
Hicks
et
al.
Fig.
8.
Relative
distribution
of
file
types
within
participants
file
spaces.
studied.
Whilst
it
is
arguable
that
large
standard
deviations
are
to
be
expected
given
the
diversity
and
variety
of
the
engineering
tasks
and
computational
tools
used
by
engineers,
such
large
values
were
not
anticipated.
One
possible
explanation
for
this
variation
is
the
inherent
variety
in
the
data
files
produced
by,
for
example,
finite
element
analysis
FEA
software
where
output
files
can
range
from
kilobytes
for
a
configuration
file,
to
gigabytes
for
the
results.
Furthermore,
significant
variation
is
also
likely
to
arise
from
the
widely
differing
levels
of
scale
and
complexity
of
the
systems
being
engineered.
For
example,
these
can
range
from
large
special
purpose
machinery
to
small
household
products,
and
can
be
modeled
using
wireframe
representations
or
solid
models,
the
latter
generating
significantly
larger
data
files.
This
explanation
is
supported
by
the
fact
that
the
some
of
the
highest
standard
deviations
observed
with
respect
to
the
calculated
averages
occur
for
CAD
files
simulation
files.
In
addition
to
considering
the
occurrence
and
distribution
of
file
types,
the
further
analysis
also
considered
the
modification
of
files
and
directories
and
the
duplication
of
filenames
within
a
file
space.
The
analysis
of
modification
was
intended
to
establish
the
number
of
individual
directories
and
files
modified
within
a
given
time
period.
For
the
purpose
of
this
study,
modification
is
identified
from
the
last
modified
date.
However,
it
is
possible
that
a
particular
file
could
have
been
modified
a
number
of
times
and
such
cumulative
modifications
are
not
measured
in
this
study.
Table
VIII
shows
the
mean
number
of
files
and
directories
modified
within
the
previous
day,
week,
month
and
year.
These
values
are
also
presented
graphically
in
Figure
9,
although
data
for
the
year
is
not
included
in
order
to
maintain
a
reasonable
scale.
The
analysis
of
modifications
for
the
previous
working
day
reveal
that
an
average
of
8
directories
and
17
files
are
modified.
In
one
working
week,
an
average
of
22
directories
and
50
files
are
modified.
In
one
month,
an
engineer
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
23,
Publication
date:
September
2008.
Organizing
and
Managing
Personal
Electronic
Files
Table
VIII.
Modification
of
Directories
and
Files
over
Time
Modification
Period
Last
Day
Last
Week
Last
Month
Last
Year
Modification
Analysis
Directories
High
Low
SD
Av.
55
0
14.0
16.5
217
0
40.0
49.5
297
1
78.2
183
2,312
8
368
1,531
Files
High
Low
253
0
281
0
1,265
5
8,657
60
23:23
Av.
8.21
21.9
58.6
295
SD
41.0
64.0
242
1,752
Fig.
9.
Number
of
directories
and
files
modified
over
recent
time
periods.
will
work
with
an
average
59
directories
and
183
files.
Over
the
course
of
an
entire
year,
an
engineer
will
work
with
around
300
directories
and
1500
files.
The
relative
ratios
of
files
modified
in
a
month
to
a
week
and
a
year
to
a
month
are
roughly
proportional
to
the
respective
ratios
of
working
time.
For
example,
the
working
time
in
a
month
could
be
considered
roughly
4
times
that
in
a
week,
taking
into
account
days
away
from
the
office
and
holidays
and
a
working
year
could
be
considered
to
be
roughly
10
times
a
working
month
for
similar
reasons.
The
relative
ratios
of
files
modified
in
these
time
periods
3.66
and
8.37
respectively
is
of
a
similar
magnitude
to
the
temporal
differences.
In
contrast,
to
this,
when
the
previous
week
to
previous
day
is
considered,
the
ratio
is
only
3.1.
The
likely
cause
is
that
over
a
week,
many
files
will
be
modified
more
than
once.
The
final
stage
of
the
analysis
involved
the
identification
of
directory
and
filename
duplication.
The
number
of
duplicate
filenames
and
directory
names
is
shown
in
the
lower
portion
of
Table
VI.
In
the
most
extreme
case
observed,
over
75
of
directories
and
83
of
files
used
shared
names
which
equated
to
over
1,000
directories
and
11,000
files
respectively.
The
average
results
are
also
surprisingly
with
32
of
files
sharing
the
same
name
and
30
of
directories
using
shared
or
nonunique
names.
Such
statistics
suggest
a
culture
of
using
common
names
to
describe
subdirectories
and
files
within
a
given
directory.
For
example,
an
individual
may
have
subdirectories
for
two
different
projects,
both
of
which
contain
a
directory
called
design
and
a
file
called
report.
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
23,
Publication
date:
September
2008.
23:24
B.
J.
Hicks
et
al.
5.
DISCUSSION
For
the
purpose
of
this
study
the
findings
of
the
audit
are
compared
and
contrasted
with
the
results
of
previously
reported
studies
and
also
discussed
with
respect
to
what
are
considered
to
be
engineering-specific
requirements.
5.1
General
File
Management
Observations
In
line
with
standard
practices
for
the
reporting
of
the
organization
of
hierarchical,
electronic
file
systems,
we
summarize
the
findings
in
relation
to
archiving,
maintenance,
and
the
use
of
hierarchical
structures,
Barreau
and
Nardi
1995,
1997
and
the
psychological
needs
of
users
Lansdale
1988
.
1.
Archiving
and
backup--The
majority
of
engineers
archive
electronic
files
70
and
around
half
backup
files
55
.
This
is
generally
performed
between
primary
and
secondary
computers.
Following
the
characterization
proposed
by
Whittaker
and
Hirschberg
2001
,
engineers
are
filers
rather
than
pilers
and
store
their
personal
electronic
files
in
structured
archives.
The
average
size
of
the
directory
structure
is
2.5Gb.
This
includes
406
subdirectories
and
4,660
files.
This
is
far
smaller
than
the
mean
of
8900
directories
previously
reported
by
Agrawal
et
al.
2007
.
However,
the
average
file
size
of
613Kb
is
nearly
three
times
larger
than
the
mean
file
size
reported
by
Agrawal
et
al.
This
difference
can
be
accounted
for
by
the
relatively
large
size
of
the
data
files
produced
by
engineering
simulation
packages
and
CAD
software,
which
account
for
about
4
of
the
total
file
space,
and
audio
and
video
files,
which
account
for
about
11
of
the
total
file
space.
This
finding
suggests
that
larger
block
sizes
for
file-systems
would
be
preferable
since
engineers
generally
store
fewer
files
of
large
sizes.
2.
Maintenance--In
an
average
day,
an
engineer
will
modify
17
files
from
8
different
directories;
in
a
week,
the
engineer
will
access
50
files
from
22
directories;
in
a
month,
an
engineer
will
access
183
files
and
59
directories,
and
in
a
year,
1500
files
from
295
directories.
In
all,
however,
the
engineers
accessed
only
about
33
of
the
files
stored
on
their
personal
computers
each
year
and
less
than
4
on
a
monthly
basis.
This
finding
suggests
that
once
a
file
has
been
created,
used,
and
then
archived,
it
is
relatively
unlikely
to
be
accessed
again.
The
engineers
in
the
study
do
not
report
significant
challenges
in
maintaining
their
file
systems
once
the
files
are
archived,
although
this
might
change
as
their
hard
disk
drives
fill
up.
One
surprising
finding,
though,
relates
to
duplicate
files
and
directories,
with
the
engineers
in
this
study
often
maintaining
two
subdirectories
for
two
different
projects,
both
of
which
contain
a
directory
called
design
and
a
file
called
report.
The
general
trend
for
engineers
is
to
have
two
working
computers,
which
for
the
purposes
of
this
work
are
described
as
the
primary
and
secondary
computer
respectively.
Primary
computers
have
a
strong
tendency
to
be
desktops,
whereas
secondary
computers
tend
to
be
laptops.
The
engineers
thus
manually
maintained
up-to-date
copies
of
files
on
both
computers.
That
is,
the
engineers
must
remember
to
update.
There
is
no
built-in
capability
in
the
versions
of
the
operating
systems
used
by
the
participants
in
the
study
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
23,
Publication
date:
September
2008.
Organizing
and
Managing
Personal
Electronic
Files
23:25
to
remind
the
engineers
to
update,
nor
did
the
engineers
report
knowing
about
or
using
a
software
tool
to
synchronize
multiple
copies
of
the
same
file.
Having
to
remember
that
files
have
been
changed
and
thus
require
synchronization
across
all
copies
is
not
included
as
a
cognitive
load
in
Lansdale
s
1988
scheme,
although
we
believe
that
remembering
to
do
such
a
task
would
present
difficulties
for
some
users.
Surprisingly,
the
engineers
we
studied
did
not
comment
on
the
effort
they
expended
in
maintaining
upto-date
copies
of
files
across
computers,
although
this
was
not
asked
of
them
explicitly.
3.
Hierarchical
structures
and
namespace
usage--The
two
most
common
criteria
used
for
naming
directories
are
i
the
purpose
or
function
of
the
directory,
that
is,
the
specific
context
of
the
file
that
it
contains
85
of
participants
and
ii
the
name
of
the
project
55
of
participants
.
This
finding
is
consistent
with
that
of
Boardman
and
Sasse
2004
who
also
found
that
the
project
name
was
a
common
top-level
name
for
a
directory
structure.
However,
in
this
study
purpose
and
function
are
the
most
widely
used,
whilst
Boardman
and
Sasse
2004
found
project
name
was
the
most
common.
One
possible
reason
for
this
could
be
the
underlying
function-based
perspective
and
practices
of
engineers
discussed
in
detail
in
the
next
section
.
This
difference
might
also
suggest
that
naming
strategies
and
directory
hierarchies
could
be
highly
context-dependent,
with
generalizations
across
industries
and
companies
difficult
to
make.
However,
the
findings
of
this
study--and
in
particular
the
use
of
cognitive
reference
points
relating
to
the
purpose
and
use--are
consistent
with
Kwasnik
1989
,
where
it
was
shown
that
such
nonconcrete
dimensions
have
a
greater
influence
in
determining
the
classification
of
a
document.
The
date
a
directory
was
created
is
also
used
in
some
cases
20
of
participants
,
indicating
a
last-option
for
file
naming
when
no
other
suitable
criteria
seemed
applicable.
The
directory
structure
includes
an
average
of
almost
nine
hierarchical
levels
with
42
subdirectories
and
495
files
per
level,
giving
an
average
of
11.6
files
per
directory.
In
addition,
it
was
observed
that
the
third,
fourth,
and
fifth
levels
of
the
hierarchy
hold
the
majority
of
all
the
files
53.5
,
regardless
of
any
hierarchy
imposed
by
the
software.
That
is,
the
hierarchies
are
user-defined
rather
than
defined
by
the
operating
system
or
a
particular
software
program.
The
number
of
files
by
name-space
depth
parallels
the
finding
by
Agrawal
et
al.
2007
,
who
reported
that
most
files
were
stored
within
a
directory
name-space
depth
of
two
to
four.
4.
Psychological
issues--Given
previous
studies
on
the
cognitive
load
of
locating
files
for
engineers
Crabtree
et
al.
1997
we
did
not
study
this
aspect
specifically.
Although
they
were
not
specifically
asked,
the
engineers
studied
did
not
report
whether
or
not
they
expended
considerable
cognitive
effort
in
naming
files.
The
most
useful
and
or
commonly
used
mechanisms
for
indexing
and
retrieving
files
are
context,
followed
by
filename
and
keywords,
then
finally
project
team
and
author.
The
personal
strategies
for
naming
files
based
primarily
on
the
title
of
the
document
75
of
files
and
the
purpose
or
function
of
the
files
in
a
directory
e.g.,
budgets
85
of
the
time
suggests
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
23,
Publication
date:
September
2008.
23:26
B.
J.
Hicks
et
al.
that
not
much
cognitive
effort
was
placed
into
naming
files.
This
is
again
consistent
with
Kwasnik
1989
,
who
concludes
that
document
names
are
chosen
that
have
the
most
usefulness
for
the
least
cognitive
effort.
5.2
Engineering-Specific
File
Management
Observations
Aside
from
the
file-system
management
aspects,
the
observed
behaviour
of
the
engineers
in
managing
their
personal
files
followed
common
engineering
work
practices.
Specifically,
we
find
that
the
way
that
the
engineers
approach
personal
electronic
file
management
parallels
both
the
way
that
engineers
commonly
think
and
the
collaboration
and
knowledge
reuse
requirements
of
the
practice
of
engineering.
1.
Engineers
follow
a
functional
approach
to
file
naming.
Engineers
named
files
based
primarily
on
the
title
of
the
document
75
of
files
and
the
purpose
or
function
of
the
files
in
a
directory.
This
functional
schema
for
file
naming
confirms
our
intuition
that
engineers
would
follow
a
function
based
approach
to
file
naming,
carrying
over
their
practice
of
function-based
design
methodologies
Pahl
and
Beitz
1999
.
Functional
reasoning
is
a
default
mode
of
thinking;
engineers
routinely
think
of
a
product
model
in
terms
of
a
product
s
overall
function
and
a
set
of
sub-functions.
This
functional
reasoning
proclivity,
evidenced
by
both
standard
engineering
work
practices
and
the
way
that
the
engineers
already
name
their
electronic
files,
suggests
that
functional
file
naming
i.e.,
name
the
file
for
what
it
would
be
used
for
would
be
a
suitable
recommendation
in
a
general
framework
for
naming
electronic
files.
In
addition
to
this,
it
has
been
shown
that
30
of
directories
use
shared
or
non-unique
namespaces.
This
again
reflects
the
structured
and
very
often
prescriptive
nature
of
engineering
practices
and
suggests
that
a
common
framework
for
the
naming
and
organization
of
directories
and
files
may
be
a
suitable
remedy.
2.
File
archival
to
support
situated
and
case-based
reasoning.
Engineering
design
involves
the
analysis
and
development
of
complex
systems.
A
design
case
can
be
drawn
from
a
complex
series
of
experiences
and
decisions.
Engineers
store
electronic
files
on
their
personal
computers
for
the
purpose
of
drawing
together
cases
to
support
their
engineering
work.
The
data
from
this
study
suggests
that
design
cases
will
be
drawn
from
a
broad
range
of
personal
electronic
files.
In
general
around
95
of
all
files
used
by
engineers
can
be
classified
as
Text,
Spreadsheet,
Presentation,
Database,
Project,
Graphics,
CAD,
Code,
Simulation,
Application,
Image,
Audio,
Video,
Internet,
Data
and
Compressed
files.
Text
files
and
image
files
are
the
most
commonly
occurring
file
types,
each
representing
an
average
of
just
over
20
of
all
files
used
by
engineers.
Following
text
and
images,
the
next
most
prevalent
file
types
are
code,
Internet,
data,
spreadsheet,
CAD,
presentation,
simulation,
and
applications,
which
each
represent
5
to
3
of
total
files.
This
broad
range
of
files
introduces
challenges
for
information
systems
designers
who
are
creating
design
rationale
capture
and
retrieval
systems
Regli
et
al.
2000
and
case-based
reasoning
systems
Maher
and
G
mez
de
Silva
Garza
1997
.
The
o
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
23,
Publication
date:
September
2008.
Organizing
and
Managing
Personal
Electronic
Files
23:27
reuse
of
design
knowledge
depends
on
two
things.
Firstly,
the
electronic
files
that
explain
why
certain
engineering
decisions
were
taken
must
be
successfully
retrieved.
This
is
the
problem
addressed
by
design
rationale
capture
systems.
Secondly,
the
integration
of
this
information
into
a
user-definable
design
case
to
support
the
current
reasoning
process,
which
is
the
problem
taken
up
by
case-based
reasoning
systems.
No
one
system
yet
exists
which
can
assist
in
both
processes
rationale
capture
retrieval
and
case-based
reasoning
simultaneously.
Furthermore,
the
many
files
that
engineers
store
and
access
exist
for
more
than
co-coordinative
practices
as
described
by
Schmidt
and
Wagner
2004
.
In
particular,
they
exist
to
support
situated
reasoning.
That
is,
the
collection
of
files
are
accessed
to
support
an
action
applicable
at
a
certain
time
in
the
engineering
design
work.
Hence,
information
systems
will
need
to
support
the
rapid
identification
of
a
collection
of
situational
relevant
files
and
their
organization
into
a
customizable
design
case.
3.
File
sharing
for
knowledge
sharing.
More
than
half
60
of
all
engineers
frequently
exchange
and
share
files
with
colleagues
who
are
participating
in
the
same
project.
The
data
in
Figure
4
d
shows
that
the
engineers
most
frequently
share
their
electronic
files
with
others
in
the
same
department
40
and
less
often
with
those
working
on
the
same
project
30
or
in
collaborating
companies
30
.
File
exchange
and
sharing
may
also
take
place
with
customers,
suppliers
and
contractors,
but
this
was
not
extensively
observed
due
to
the
characteristics
of
the
sample
population.
More
importantly,
the
evidence
indicates
that
there
is
both
a
broad
and
wide
distribution
of
knowledge
as
data
contained
in
each
the
electronic
files.
That
is,
each
engineer
retains
a
deep
base
of
knowledge
in
personal
files
2.5Gb
,
but
shares
and
accesses
a
broad
range
of
knowledge
with
and
from
others.
This
pattern
of
electronic
file
sharing
is
consistent
with
engineering
design
practice.
Communication
of
knowledge
among
different
domain
specialists
is
a
fundamental
aspect
of
engineering.
The
maintenance
of
the
data
files
by
engineers
within
a
department
also
serves
a
secondary
purpose,
assisting
other
engineers
in
the
department
by
filtering
and
providing
information
from
past
projects
to
those
who
need
the
data
at
specific
junctions
of
the
project.
This
role
is
what
Sonnenwald
1996
called
intradisciplinary
stars,
those
who
transmit
knowledge
within
a
discipline.
Given
the
lack
of
negative
feedback
about
the
file
sharing,
it
is
plausible
to
infer
that
the
cost
of
maintaining
file
sharing
services
is
exceeded
by
the
payoff.
This
should
be
considered
not
only
in
terms
of
the
direct
value
of
the
knowledge
itself,
but
also
in
terms
of
being
able
to
control
the
circumstances
of
disclosure
and
to
maintain
personal
relationships
Grudin
2001
,
with
necessarily
demand
higher
levels
of
trust
and
less
misinterpretation
in
the
shared
knowledge
Tsoukas
1996
.
Additionally,
it
appears
that
the
engineers
prefer
to
maintain
a
local
copy
of
electronic
files
specifically
so
that
they
can
be
referred
to
at
a
future
date
and
reused
both
by
the
engineer
and
for
file
sharing,
which
makes
sense
where
the
reliability
and
permanence
of
formal
external
stores
are
not
always
trusted
Whittaker
and
Hirschberg,
2001
.
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
23,
Publication
date:
September
2008.
23:28
B.
J.
Hicks
et
al.
6.
IMPROVING
THE
ORGANIZATION
OF
PERSONAL
ELECTRONIC
FILES
This
section
is
concerned
with
the
development
of
a
remedy
for
the
naming
and
organization
of
electronic
files
that
balances
the
personal
nature
of
data
storage
with
the
need
for
personal
records
to
be
accessed
and
reused
by
colleagues,
possibly
far
into
the
future
where
the
original
owner
is
no
longer
available.
Firstly,
a
summary
of
key
approaches
for
improving
file
access
and
management
is
presented
and
critiqued
with
respect
to
the
previously
observed
practices
of
engineers.
The
most
apposite
approach
is
then
used
as
the
basis
for
the
development
of
a
strategy
for
improving
the
organization
and
management
of
personal
electronic
files
in
engineering
organizations.
The
possible
benefits
and
potential
barriers
to
the
introduction
of
the
proposed
strategy
within
the
context
of
engineering
are
then
discussed.
6.1
Improving
the
Access
and
Management
of
Electronic
Files
Improved
file
management
can
be
achieved
by
a
variety
of
means
and
in
particular:
conventions
for
file
naming,
file
management
systems,
search
tools
including
indexing
and
classification
and
visualization
techniques.
Each
of
these
are
discussed
in
turn
below.
1.
De
facto
conventions
and
industry
standards
for
naming
files.
These
standards
either
prescribe
the
name
space
or
restrict
the
allowable
character
sets,
reserved
words
and
maximum
length
of
file
names.
Examples
of
rule-driven
name
space
conventions
are
those
used
for
the
management
of
large
homogeneous
sets
of
data
files
such
as
climate
and
meteorological
data
NetCDF,
http:
cf-pcmdi.llnl.gov
,
and
satellite
images
TruEarth
2008
.
A
common
example
of
restricted
character
conventions
are
those
of
ISO
9660
ISO,
1988
,
which
define
a
file
system
for
CD-ROM
media
which
supports
Microsoft
Windows,
Mac
OS,
and
Unix
Unix-like,
and
restricts
both
the
length
of
file
names
and
the
allowed
character
set.
In
the
engineering
domain,
no
such
standards
exist
that
would
be
suitable
for
the
naming
of
electronic
files.
2.
Content
and
document
management
systems.
Document
management
systems
support
the
creation,
flow,
storage,
retrieval
and
archiving
of
electronic
documents
Sutton
1996
.
Development
began
in
the
late
1980s
and
early
1990s
with
the
creation
of
Laserfiche
Compulink
Management
Center
2008
and
PC
DOCS
Hummingbird
2008
and
there
are
now
a
large
number
of
commercial
systems
in
use
in
engineering
design
firms.
Systems
have
evolved
from
initially
providing
document
imaging
and
cataloguing
to
providing
repositories
Szykman
et
al.
2000
for
all
unstructured
documents
including
for
example
reports,
emails,
letters,
and
drawings.
The
repositories
employ
a
formal
design
modelling
language
and
standard
representations
of
data
to
support
interfaces
for
adding,
editing
and
browsing
repository
entries.
A
formal
design
modelling
language
leads
to
a
more
comprehensive
description
of
engineering
design
works,
which
in
turn
provides
for
meaningful
indexing
of
engineering
information.
The
repositories,
however,
deal
with
engineering
data
as
information
objects
which
are
independent
of
the
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
23,
Publication
date:
September
2008.
Organizing
and
Managing
Personal
Electronic
Files
23:29
actual
electronic
files
within
which
they
are
contained.
That
is,
an
information
object
within
the
repository
may
be
comprised
of
data
coming
from
multiple
electronic
files.
The
repositories
do
not
address
the
management
of
the
files
themselves.
3.
Search
tools.
Commercial
tools
such
as
Google
Desktop
http:
desktop.
google.com
automatically
traverse
a
prescribed
set
of
electronic
files
to
create
an
index
of
all
files
and
keywords
based
on
the
content
of
the
files.
This
index
can
then
be
used
to
perform
very
fast
keyword-based
searches
of
files.
Research
prototypes
such
as
Microsoft
Research
s
Stuff
I
ve
Seen
Dumais
et
al.
2003
support
both
full-text
search
like
Google
Desktop
and
metadata
based
search,
such
as
the
name
of
the
person
not
necessarily
the
user
who
created
an
information
item.
Implicit
Query
facilities
Dumais
et
al.
2004
allow
contextually-similar
information
to
be
presented
in
addition
to
the
most
likely
relevant
file.
The
adoption
of
full-text
search
engines
for
personal
computer
files
is
problematic
for
engineering
organizations
due
to
the
lack
of
indexable
and
searchable
text
in
many
types
of
files
used
Table
V
.
Finally,
while
these
tools
enable
search
based
on
content,
they
do
not
as
yet
support
task-based
or
context-based
search
Cuttrell
et
al.
2006
.
Whilst
such
full-text
search
tools
can
significantly
simplify
the
problem
of
finding
an
electronic
file,
a
prerequisite
for
their
effective
use
is
an
understanding
of
the
content
of
the
files.
To
overcome
the
need
for
a
relatively
in-depth
a
priori
knowledge
of
the
document
set,
commercial
classification
tools
have
been
developed
McMahon
et
al.
2004
.
These
tools
require
that
each
document
be
classified,
usually
by
an
expert
or
a
keyword
search,
with
respect
to
a
predetermined
ontology
or
taxonomy.
These
ontologies
generally
represent
domain
knowledge
or
a
particular
perspective
context
with
the
aim
of
providing
the
user
with
a
common
and
more
insightful
view
of
the
information
space
thereby
reducing
access
time.
The
classification
is
then
intended
to
assist
in
searching
for
information
items.
The
remedy
that
we
propose
includes
a
simple
taxonomy
and
ontology.
4.
Visualization
techniques.
Tools
such
as
Haystack
Karger
and
Quan,
2004
present
views
of
the
file
space
which
emphasize
certain
dimensions
or
factors
including
graphical
views
of
metadata
such
directory
sizes
and
file
sizes,
color-coded
views
of
file
types,
and
graphical
representations
of
modification
history,
and
summary
views
of
the
data
itself.
They
may
also
include
utilities
to
notify
the
user
s
of
the
files
currently
being
modified
or
the
most
frequently
accessed
files.
They
are
normally
incorporated
with
search
tools,
that
is,
present
the
results
of
a
search
for
electronic
files,
rather
than
present
a
visualization
of
an
entire
set
of
personal
electronic
files.
As
such,
they
generally
deal
with
the
problem
of
locating
a
file
rather
than
managing
the
personal
electronic
files
for
sharing.
These
approaches
all
offer
potential
benefits,
but
also
possess
various
limitations
with
respect
to
the
management
of
personal
files
and,
ultimately,
a
remedy
to
overcome
the
barriers
to
more
effective
reuse
of
personal
electronic
files
across
an
engineering
organization.
In
the
case
of
standards,
there
exist
no
guidelines
or
recommendations
in
the
engineering
design
community
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
23,
Publication
date:
September
2008.
23:30
B.
J.
Hicks
et
al.
for
the
creation
of
naming
conventions
for
personal
electronic
files.
Standards
such
as
STEP
ISO
10303
Product
Data
Representation
and
Exchange
provide
a
means
of
describing
engineering
product
data,
but
not
the
electronic
files
containing
the
product
data.
Where
document
management
systems
and
product
data
management
systems
are
available,
our
experience
in
consulting
for
engineering
design
firms
in
the
UK
is
that
their
application
tends
to
be
limited
to
project
files,
file
groups,
or
published
documents
and
particularly
those
documents
generated
through
collaborative
working
rather
than
personal
electronic
files.
For
these
reasons,
and
cost
considerations,
such
systems
not
generally
implemented
for
personal
or
user
file
spaces,
which,
as
the
empirical
results
from
this
study
show,
can
represent
a
significant
amount
of
electronic
information.
Given
the
limitations
of
these
approaches
and
the
fact
that
they
can
be
considered
to
address
the
consequences
of
poor
file
organization
rather
than
the
cause
i.e.,
inconsistent
and
unrepresentative
naming
and
organization
of
the
electronic
files
when
first
generated
,
it
is
proposed
to
develop
conventions
and
standards
for
naming
and
organizing
files
as
a
remedy
to
the
reuse
of
personal
information
within
the
context
of
engineering.
6.2
A
Shared
Scheme
for
Directory
Organization
and
File
Naming
As
discussed
previously,
the
results
of
the
study
suggest
that
the
reuse
of
personal
information
in
an
engineering
environment
and
hence
knowledge
sharing
can
be
facilitated
through
a
shared
scheme
for
directory
organization
and
file
naming.
However,
the
issues
surrounding
the
development
of
a
generic
scheme
are
complex
and
unclear.
Arguably,
the
primary
purpose
of
a
filename
is
to
represent
the
subject
matter
or
context
of
the
information
contained
within
the
file--to
better
support
recall
and
recognition
activities
associated
with
the
file
s
retrieval
Lansdale
1988
.
When
naming
files,
users
may
deliberate
at
length
to
decide
a
name
which
best
represents
the
information
content,
purpose
or
subject.
Filenames
which
fail
to
achieve
this
may
make
identifying
and
retrieving
particular
files
unnecessarily
time-consuming.
It
is
known
in
engineering
that
there
exist
normative
ways
of
reasoning
about
engineering
problems,
such
as
mapping
between
function,
structure,
and
behavior
of
the
design
work
Gero
1990
,
functional
reasoning
Far
and
Elamy
2005
,
and
as
hierarchical
systems.
These
aspects
however,
are
likely
to
be
dependent
on
the
particular
organization,
its
design
process
and
the
artifact
being
designed.
Consider
for
example,
the
design
of
a
commercial
aircraft
vs.
a
mobile
phone.
The
scale,
structure,
complexity,
and
life
cycle
of
the
two
artifacts
are
considerably
different
and
are
hence
likely
to
require
the
adoption
of
different
schemes.
For
these
reason,
this
section
discusses
the
key
baseline
features
that
a
shared
scheme
should
support,
including:
--
The
requirement
of
engineers
to
use
more
than
one
computer.
--
The
requirement
to
support
the
identification
and
recall
of
files
based
on
criteria
that
include:
specific
context,
keywords,
project
title,
author
and
project
team.
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
23,
Publication
date:
September
2008.
Organizing
and
Managing
Personal
Electronic
Files
23:31
Fig.
10.
The
five
level
directory
structure.
Fig.
11.
A
hierarchical
model
of
the
five
level
directory
structure.
--
The
practice
of
engineers
to
work
in
levels
three,
four
and
five
of
their
directory
structure.
--
The
need
to
support
sixteen
fundamental
classes
of
file
used
by
engineers.
--
The
ability
to
verify
self-check
the
location
of
a
file
within
the
directory
structure.
--
The
need
to
reduce
duplicate
filenames.
--
The
need
to
support
archiving,
backup
and
synchronization
of
files.
These
key
features
are
each
discussed
with
reference
to
one
proposed
scheme
for
directory
organization
and
file
naming.
This
scheme
is
shown
conceptually
in
Figure
10
and
hierarchically
in
Figure
11.
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
23,
Publication
date:
September
2008.
23:32
B.
J.
Hicks
et
al.
6.2.1
Multiple
Computers.
In
order
to
satisfy
the
requirement
for
using
multiple
computers
the
name
of
the
top
level
directory
could
be
specific
to
the
individual
engineer
and
a
computer
drive
or,
where
file
spaces
are
shared,
the
project
team
since
design
knowledge
reuse
occurs
largely
through
social
knowledge
networks
Demian
and
Fruchter
2006
.
For
example,
in
the
scheme
shown
in
Figure
10
the
uppermost
directory
of
a
user
s
file
space
might
is
labeled
name
computername.
In
this
manner,
files
and
complete
directory
structures
from
multiple
computers
can
be
managed
without
the
need
for
renaming
and
assimilation
of
directories.
6.2.2
Identification
and
Recall
of
Files.
The
aim
of
the
proposed
remedy
is
to
improve
knowledge
reuse
through
access
to
the
personal
files
of
others
and
facilitate
rudimentary
case-based
reasoning
i.e.,
determining
if
a
file
could
be
relevant
to
the
current
problem
simply
by
reading
the
name
of
the
file
and
the
directory
within
which
the
file
is
stored.
The
empirical
results
are
used
to
guide
our
understanding
of
the
electronic
files
as
mediators
of
coordinated
and
collaborative
activity
in
engineering
design
Suchman
2000
.
The
study
has
shown
that
for
engineers,
the
most
commonly
used
cues
for
identification
and
recall
of
files
are:
specific
context,
keywords,
project
title,
the
name
of
the
author
engineer
and
the
name
of
the
project
team.
As
a
consequence
it
is
desirable
for
these
elements,
or
part
therefore,
to
appear
explicitly
in
the
filename
and
or
the
directory
path.
6.2.3
The
Practice
of
Using
Five
Level
Directory
Structures.
As
stated
in
the
previous
section,
there
is
a
requirement
to
include
elements
such
as
specific
context,
keywords,
project
title,
author
and
project
team
in
the
directory
path
or
filename.
However,
the
study
has
also
shown
that
the
current
practice
of
engineers
is
to
work
in
the
third,
fourth
and
fifth
levels
of
their
directory
structure.
It
is
therefore
desirable
to
restrict
a
shared
scheme
to
five
levels.
Given
this
constraint
and
the
need
to
provide
a
means
for
organizing
sixteen
classes
of
file
there
remain
only
three
levels
of
the
directory
structure
available
for
organizing
files
with
respect
to
the
aforementioned
elements.
One
possible
scheme
would
be
to
label
the
second
and
third
levels
of
the
directory
structure
according
to
a
function-oriented
decomposition
of
the
design
work,
which
is
congruent
with
the
empirical
data
found
in
this
study
and
other
frameworks
proposed
for
the
capture
of
rationale
about
product
data
Anthony
at
al.
2001
.
The
second
level--shown
in
Figures
10
and
11--should
include
subdirectories
named
by
each
project
or
activity,
which
should
indicate
the
component
or
system
being
engineered
e.g.,
bushing
or
tolerance
ring
or
the
particular
activity
e.g.,
makefiles
or
risk
analysis
.
The
third
level
of
the
hierarchy
is
for
the
specific
functional
context,
for
example,
report
or
research.
In
addition,
where
appropriate,
the
name
should
include
what
the
system
or
component
does
e.g.,
research
dampening
.
The
combination
of
subject,
component,
and
functional
naming
has
been
shown
in
other
studies
to
be
of
most
value
in
improving
the
automated
information
retrieval
of
engineering
documents
Yang
et
al.
2005
.
This
leaves
one
remaining
level
of
the
directory
structure
level
4
,
which
should
be
used
to
provide
an
indication
of
currency.
Hence,
in
the
scheme
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
23,
Publication
date:
September
2008.
Organizing
and
Managing
Personal
Electronic
Files
23:33
shown
in
Figure
10
the
fourth
level
of
the
structure
is
labeled
according
to
year.
6.2.4
Support
for
the
Sixteen
Fundamental
Classes
of
File.
One
way
to
improve
the
organization
and
management
of
the
sixteen
classes
of
file
identified
in
this
study
is
to
logically
group
them
according
to
class.
Hence,
in
the
scheme
illustrated
in
Figures
10
and
11,
the
fifth
and
final
level
of
the
hierarchy
provides
containers
for
specific
file
classes:
Text,
Spreadsheet,
Presentation,
Database,
Project,
Graphics,
CAD,
Code,
Simulation,
Application,
Image,
Audio,
Video,
Internet,
Data,
and
Compressed
files.
Separating
the
files
by
type
facilitates
the
creation
of
Product
Data
Management
PDM
and
Product
Lifecycle
Management
PLM
systems
that
associate
CAD
models
to
related
documents,
thereby
embedding
significant
semantic
content
in
the
product
model
rather
than
specific
application
programs
which
access
these
files
Dong
and
Agogino
1998
.
It
is
suggested
that
given
almost
50
of
files
can
are
text
and
images
that
subdirectories
for
these
two
classes
of
files
be
created
as
standard
and
the
remaining
directories
be
created
as
and
when
required.
6.2.5
Verification
of
File
Location.
Within
an
engineering
environment
which
is
largely
team
based,
there
is
a
requirement
to
ensure
that
the
file
systems
provide
a
mechanism
by
which
files
can
be
organized
such
that
their
position
within
the
structure
can
be
unambiguously
determined
and
where
necessary
verified.
Such
a
task
might
normally
be
performed
by
a
moderator
or
librarian.
However,
for
the
purpose
of
organizing
electronic
files,
it
is
desirable
to
provide
an
inherent
mechanism
within
the
strategy
that
is
congruent
and
consistent
with
the
function-based
file
naming
preference
and
way
of
thinking
about
files
uncovered
by
the
empirical
data
reported
previously.
A
prescribed
organizational
structure
for
a
file
hierarchy
could
assist
in
providing
context
about
an
information
item
Karger
and
Jones
2006
,
which
would
then
facilitate
a
shared
understanding
of
the
context
within
which
a
project
was
undertaken
Coughlan
and
Johnson
2008
.
This
can
be
achieved
by
ensuring
that
the
directory
naming
convention,
file
naming
convention
and
organization
are
self-consistent
and
in
some
manner
are
auditable.
6.2.6
Reduction
of
Duplicate
Filenames.
As
stated
in
the
previous
section,
it
is
desirable
to
determine
a
file
naming
convention
and
scheme
for
directory
organization
that
are
self-consistent.
It
has
also
been
shown
that
engineers
frequently
use
shared
names
for
both
files
and
directories
which
frustrates
the
processes
of
recall
and
recognition.
One
way
to
address
these
two
dimensions
is
to
adopt
a
file
naming
convention
that
parallels
the
structure
of
the
directory.
For
the
proposed
scheme
in
Figure
10,
this
would
involve
the
construction
of
a
filename
that
included
the
engineer
or
project
teams
name,
the
project
title,
the
context,
working
year
and
a
description
of
the
file
separated
by,
for
example,
underscores
or
dashes.
6.2.7
Archiving
and
Backup.
In
addition
to
organizing
the
working
directory
structure,
it
is
necessary
to
support
archiving,
backup
and
synchronization
of
files
across
the
multiple
computers
used
by
an
engineer.
As
previously
stated
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
23,
Publication
date:
September
2008.
23:34
B.
J.
Hicks
et
al.
in
4.1.1,
within
the
context
of
engineering,
archiving
tends
to
be
undertaken
for
either
a
particular
project
or
a
specific
customer,
whilst
backup
is
performed
at
regular
time
intervals
for
a
given
file
system
s
.
In
general,
the
tasks
of
archiving
and
backup
involve
the
generation
of
an
instance
of
a
drive,
a
file
system
or
a
portion
thereof
at
a
given
time,
either
as
part
of
a
backup
procedure,
or
at
the
end
of
a
project
or
financial
period
in
the
case
of
archiving.
Effectively,
these
processes
can
be
considered
to
add
another
level
to
the
top
most
level
of
the
existing
directory
structure.
For
archiving
files
this
could
be
based
on
the
year
in
which
the
project
finished
or
the
end
of
a
financial
period,
whereas
for
backing
up
files
this
might
be
based
on
the
identifier
name
of
the
computer
drive.
6.3
Potential
Barriers
to
the
Introduction
of
a
Shared
Scheme
The
remedy
suggested
is
an
attempt
to
rationalize
the
namespace
and
directory
hierarchy
for
personal
collections
of
electronic
files
in
an
engineering
organization.
While
the
remedy
has
not
been
implemented
and
evaluated
to
ascertain
the
acceptability
of
a
prescribed
electronic
file
collection
organizing
structure,
our
aim
is
to
suggest
a
potential
solution
to
the
engineering
information
management
community
based
on
empirical
evidence
on
the
use
of
personal
electronic
files.
Further,
there
is
evidence
to
suggest
the
potential
for
the
uptake
of
such
a
remedy.
Evidence
from
a
study
by
Jones
et
al.
2005
reported
that
their
users
already
attempted
to
reuse
predefined
directory
structures
across
similar
projects,
suggesting
that
engineers
within
an
organization
working
on
similar
engineering
projects
would
likely
make
use
of
prescribed
directory
structures
when
those
structures
correspond
to
the
way
they
think
of
their
projects.
Second,
Boardman
and
Sasse
2004
reported
that
users
are
likely
to
devote
time
and
attention
to
organizing
files
that
they
authored,
since
they
feel
a
sense
of
ownership
over
the
files.
Thirdly,
where
engineers
are
considered,
many
working
practices--such
as
the
use
of
Bills
of
Material
BOM
,
and
supportive
tools,
such
as
PLM
systems
Siemens
Inc.
2008
--follow
hierarchical
structures
and
standardized
naming
conventions.
Thus,
whilst
it
is
not
a
priori
obvious
that
engineers
will
follow
a
prescribed
paradigm
for
naming
personal
electronic
files--
even
if
adherence
to
these
paradigms
would
facilitate
both
the
sharing
of
the
files
and
the
capability
to
reuse
them--a
remedy
which
uses
a
prescribed
naming
scheme
is
consistent
with
working
practices
that
are
widely
adopted
within
engineering.
The
remedy
is
intended
to
rationalize
the
storage
of
personal
electronic
files
under
the
consideration
that
these
files
might
later
on
be
of
use
for
others.
Given
that
more
than
60
of
the
engineers
reported
frequently
sharing
their
files,
the
issue
of
managing
personal
information
items
to
be
shared
with
others
is
a
relevant
one.
The
remedy
does
not
solve
the
problem
of
unifying
Karger
and
Jones
2006
and
contextualizing
this
information
by
others
once
this
information
is
found.
The
unifying
problem
is
being
addressed
by
projects
such
as
Haystack
Adar
et
al.
1999
whereas
the
contextualizing
issue
is
being
addressed
by
projects
such
as
Sonic
Sketchpad
Coughlan
and
Johnson
2008
for
creative
practitioners
such
as
design
engineers.
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
23,
Publication
date:
September
2008.
Organizing
and
Managing
Personal
Electronic
Files
23:35
In
addition
to
the
potential
barriers
to
the
uptake
of
the
proposed
remedy,
the
adoption
of
a
prescribed
scheme
will
impact
the
distribution
of
files
within
the
file
system.
In
particular,
all
files
will
be
contained
within
the
leaf
directories
level
5
and
will
require
navigation
through
four
directory
levels.
However,
the
additional
semantic
content
provided
by
the
directory
levels
and
associated
labels
names
should
assist
the
user
in
locating
relevant
files,
since
conventions
for
the
names
of
surrounding
files
and
directory
names
suggesting
the
content
of
files
provide
context
for
orienteering
Teevan
et
al.
2004
to
find
other
files.
Further,
the
additional
levels
project
activity,
context,
year
and
file
class
should
discourage
users
from
placing
large
numbers
of
similar
files
such
as
images,
in
a
single
flat
directory.
This
remedy
differs
from
Bergman
et
al.
2006
,
who
recommended
that
all
project-related
files
should
be
stored
in
a
single
directory.
Others
studies,
such
as
Boardman
and
Sasse
2004
,
find
that
users
may
have
different
organizational
needs
based
on
the
type
of
work
supported
by
different
tools,
and
the
concomitant
electronic
files
produced,
suggesting
that
neither
the
single
hierarchy
or
the
multiple
hierarchy
model
is
necessarily
right.
The
single
convention
for
the
file
directory
hierarchy
may
beneficially,
however,
reduce
the
duplicate
hierarchy
problem
Boardman
et
al.
2003
.
For
the
purpose
of
developing
the
proposed
remedy,
it
was
determined
that
there
was
a
requirement
to
ensure
that
the
directory
naming
convention,
file
naming
convention
and
organization
are
self-consistent
in
order
to
provide
a
means
for
self-checking
verification
of
a
file
s
location.
Whilst
such
functionality
is
desirable
within
large
collaborative
teams,
where
responsibility
for
managing
files
is
distributed,
the
resulting
file
names
are
lengthy.
For
example,
the
scheme
shown
in
Figure
10
gives
rise
to
a
maximum
path
length
of
91
characters
excluding
directory
switches,
drive
name,
and
filename.
If
the
filename
also
includes
these
various
elements,
then
the
filename
could
be
around
108
characters
and
generate
a
path
uniform
resource
identifier
excluding
computer
and
drive
information
of
200
characters.
Whilst
these
values
are
significantly
below
those
imposed
by
ISO
9660
and
the
operating
systems,
they
will
significantly
impact
on
speed
of
naming
and
file
manipulation.
These
lengthy
paths
and
filenames
could
make
recognition
and
recall
difficult
and
increase
the
cognitive
load
so
that
the
user
can
focus
on
the
engineering
work
rather
than
on
managing
electronic
files
associated
with
the
engineering
work
Oviatt
2006
.
However,
if
a
prescribed
paradigm
is
followed
and
the
files
are
listed
alphabetically,
then
recognition
and
recall
arguably
become
easier
through
habituation.
Further,
it
may
be
possible
to
embody
the
Name,
Project,
Context,
and
Year
elements
of
the
file
name
into
the
meta
data
of
the
file
itself,
thereby
reducing
the
cognitive
load
necessary
to
recognize
particular
file
names.
In
summary,
it
is
unlikely
that
a
prescribed
convention
for
naming
and
organizing
personal
electronic
files
will
provide
an
optimal
solution
by
itself
for
managing
files
in
personal
electronic
information
collections--that
is,
reducing
cognitive
load,
improving
file
access
and
universality.
Rather,
it
is
envisaged
that
the
adoption
of
good
practices
for
naming
and
organization
could
deliver
benefits
in
terms
of
storing
personal
information
to
assist
in
the
reuse
and
sharing
of
the
personal
electronic
files
with
others.
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
23,
Publication
date:
September
2008.
23:36
B.
J.
Hicks
et
al.
7.
CONCLUSIONS
This
article
deals
with
the
management
of
personal
electronic
files
and
in
particular
the
naming
and
organization
of
directories
and
the
computer-based
files
used
by
engineers
on
their
personal
computers.
The
issues
associated
with
managing
electronic
files
in
today
s
commercial
environments--where
the
volume
and
diversity
of
information
is
continually
increasing--are
discussed
and
the
need
to
improve
the
organization
and
access
of
personal
electronic
files
within
shared
and
individual
file
spaces
is
described.
In
addition
to
this,
the
lack
of
guidelines
or
conventions
for
naming
and
organizing
files
is
highlighted
and
the
limitations
of
a
variety
of
existing
approaches
and
tools
are
discussed.
It
follows
that
the
proposition
of
this
paper
is
that
many
of
the
issues
associated
with
the
management
of
engineers
personal
electronic
files
could
be
overcome
or
at
least
alleviated
through
the
development
of
shared
schemes
for
directory
organization
and
file
naming.
Central
to
the
development
of
any
scheme
is
the
need
to
understand
the
practices
and
requirements
of
engineers
for
managing
personal
electronic
files.
In
order
to
achieve
this,
a
detailed
study
and
audit
of
the
file
spaces
of
40
engineers
is
presented.
The
study
involved
a
survey
of
the
personal
strategies
of
engineers
and
an
analysis
of
their
file
spaces
with
a
software-based
audit
tool
developed
especially
for
this
study.
As
this
study
focused
on
the
management
of
personal
electronic
files,
the
research
method
did
not
explicitly
consider
why
the
engineers
stored
the
electronic
file,
or
the
task
s
that
necessitated
the
retrieval
of
personal
electronic
files
or
their
sharing
with
others.
Future
studies
employing
a
task-based
method
as
done
in
the
study
by
Elsweiler
and
Ruthven
2007
could
uncover
the
rationale
behind
the
behaviors.
The
findings
of
the
study
reveal
that:
--
Engineers
generally
use
two
computers;
a
desktop
and
a
laptop.
--
70
of
engineers
archive
older
electronic
files
and
around
half
55
back-up
files.
--
The
average
size
of
the
directory
structure
of
an
engineer
is
2.5
gigabytes,
including
406
directories
and
4,660
files.
--
Over
60
of
engineers
frequently
exchange
and
share
files
with
colleagues
involved
in
the
same
project.
--
Almost
95
of
all
files
used
by
engineers
can
be
classified
as
text,
spreadsheet,
presentation,
database,
project,
drawing,
cad,
code,
simulation,
application,
image,
audio,
video,
internet,
data
and
compressed
files.
--
20
of
all
files
are
text
files,
20
are
image
files,
and
between
3
5
are
code
files,
Internet
files,
data
files,
spreadsheet
files,
CAD
files,
presentation
files,
simulation
files
and
applications.
--
The
most
common
criteria
for
naming
directories
are
the
purpose
or
function
85
,
the
name
of
the
project
55
and
the
date
20
.
--
The
directory
structure
includes
an
average
of
nine
hierarchical
levels
with
42
subdirectories
and
495
files
per
level.
--
The
third,
fourth
and
fifth
levels
of
the
hierarchy
hold
the
majority
of
all
files
53.5
.
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
23,
Publication
date:
September
2008.
Organizing
and
Managing
Personal
Electronic
Files
23:37
--
The
most
common
criteria
used
to
name
files
are
document
title
75
,
purpose
or
function
60
,
project
title
50
and
date
45
.
--
Over
30
of
files
have
a
duplicate
name.
A
similar
number
of
directories
also
share
a
duplicate
name.
--
An
engineer
will
modify
17
files
from
8
different
directories
in
a
day,
50
files
from
22
directories
in
a
week
and
1500
files
from
295
directories
in
a
year.
--
The
most
useful
and
or
commonly
used
mechanisms
for
locating
files
are
context,
followed
by
filename
and
keywords,
then
project
team
and
author.
What
is
most
interesting
is
that
the
personal
file
naming
practices
of
the
engineers
followed
their
function-based
reasoning
work
practice
and
training,
as
well
as
the
personal
knowledge
reuse
and
sharing
required
by
engineering
design.
It
may
not
be
surprising
that
electronic
file
naming
practices
follow
the
way
that
engineers,
or
any
professional,
thinks
and
reasons
about
their
work.
Since
engineers
tend
to
think
functionally
about
components
and
systems,
it
is
not
surprising
that
the
file
names
follow
this
convention.
The
understanding
gained
from
the
findings
of
this
study
are
used
to
develop
the
key
features
of
a
shared
scheme
for
organizing
personal
directories
and
naming
files.
These
features
are
illustrated
through
a
five
level
directory
naming
convention
that
is
based
on
a
root
directory
highest
level
of
a
user
s
file
system
which
corresponds
to
a
particular
engineer.
This
is
followed
by
a
project
level,
a
context
level,
a
working
year
and
subdirectories
corresponding
to
the
sixteen
file
classes
identified
in
this
study.
A
file
naming
convention
is
also
proposed,
and
is
self-consistent
with
the
directory
naming
convention
and
enables
the
location
of
the
file
within
the
hierarchy
to
be
cross-referenced
verified
.
The
file
naming
convention
includes
the
name
of
the
engineer
or
project
team,
followed
by
the
project
name,
the
context,
the
year
and
a
description.
For
the
purpose
of
backing
up
and
archiving,
the
five
level
directory
structure
is
extended
to
six
to
include
an
additional
top
level
root
directory
corresponding
to
the
computer
name
or
the
year
respectively.
Various
limitations
on
the
lengths
of
these
elements
are
also
defined
in
order
that
the
convention
conforms
to
ISO
standards
and
hardware
restrictions.
The
key
features
of
a
shared
scheme
have
been
developed
from
an
understanding
of
the
fundamental
classes
of
file
used
by
engineers,
a
detailed
study
of
current
practices
for
organizing
files
and
the
most
commonly
used
criteria
for
searching
and
indexing
files.
It
is
intended
that
the
scheme
be
sufficiently
generic
for
engineering
tasks
per
se
and
also
flexible
enough
to
incorporate
context
specific
information
for
particular
users
and
domains.
It
is
argued
that
the
scheme
should
afford
meaningful
navigation
around
shared
file
spaces
and
the
file
spaces
of
other
users.
This
will
not
only
improve
the
access
and
retrieval
of
working
files
but
also
support
more
efficient
and
effective
management
and
in
particular,
archiving
and
disposal.
It
is
also
proposed
that
if
such
conventions
are
adopted
they
could
be
used
in
combination
with
indexing,
classification
and
visualization
tools
to
further
enhance
their
overall
benefits
to
engineering
work.
Finally,
although
the
work
reported
in
this
paper
is
concerned
with
mechanical
engineers,
the
issues
dealt
with
are
relevant
to
all
knowledge
based
industries
in
particular
those
which
involve
teams
of
knowledge
workers
and
the
scheme
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
23,
Publication
date:
September
2008.
23:38
B.
J.
Hicks
et
al.
could
be
readily
adapted
to
the
needs
of
these
other
domains
through
a
similar
process
to
the
one
reported
in
this
research.
REFERENCES
ADAR,
E.,
KARGER,
D.,
AND
STEIN,
L.
A.
1999.
Haystack:
Per-user
information
environments.
In
Proceedings
of
the
8th
International
Conference
on
Information
and
Knowledge
Management
CIKM
99
.
ACM
Press,
New
York,
NY,
413
422.
AGRAWAL,
N.,
BOLOSKY,
W.
J.,
DOUCEUR,
J.
R.,
AND
LORCH,
J.
R.
2007.
A
five-year
study
of
file-system
metadata.
ACM
Trans.
Stor.
3,
3,
32.
ANDERSON,
C.
J.,
GLASSMAN,
M.,
MCAFEE,
R.
B.,
AND
PINELLI,
T.
2001.
An
investigation
of
factors
affecting
how
engineers
and
scientists
seek
information.
J.
Engin.
Techn.
Manag.
18,
2,
131
155.
ANTHONY,
L.,
REGLI,
W.
C.,
AND
JOHN,
J.
E.
2001.
An
approach
to
capturing
structure,
behavior,
and
function
of
artifacts
in
computer-aided
design.
J.
Comput.
Inf.
Sci.
Engin.
1,
2,
186
192.
BARREAU,
D.
AND
NARDI,
B.
A.
1997.
Finding
and
reminding
revisited:
appropriate
metaphors
for
file
organization
at
the
desktop.
ACM
SIGCHI
Bull.
29,
1,
76
78.
BARREAU,
D.
AND
NARDI,
B.
A.
1995.
Finding
and
reminding:
File
organization
from
the
desktop.
ACM
SIGCHI
Bull.
27,
3,
39
43.
BERGMAN,
O.,
BEYTH-MAROM,
R.,
AND
NACHMIAS,
R.
2006.
The
project
fragmentation
problem
in
personal
information
management.
In
Proceedings
of
the
SIGCHI
Conference
on
Human
Factors
in
Computing
Systems
CHI
06
,
R.
Grinter,
T.
Rodden,
P.
Aoki,
E.
Cutrell,
R.
Jeffries,
and
G.
Olson,
Eds.
ACM,
New
York,
NY,
271
274.
BOARDMAN,
R.
AND
SASSE,
M.
A.
2004.
Stuff
goes
into
the
computer
and
doesn
t
come
out:
A
crosstool
study
of
personal
information
management.
In
Proceedings
of
the
SIGCHI
Conference
on
Human
Factors
in
Computing
Systems
CHI
04
.
ACM
Press,
New
York,
NY,
583
590.
BOARDMAN,
R.,
SPENCE,
R.,
AND
SASSE,
M.
A.
2003.
Too
many
hierarchies?
The
daily
struggle
for
control
of
the
workspace.
In
Proceedings
of
the
10th
International
Conference
on
HumanComputer
Interaction.
Lawrence
Erlbaum
Publishers,
New
York,
NY,
616
620.
CHAFFEY,
D.
AND
WOOD,
S.
2004.
Business
Information
Management,
Improving
Performance
Using
Information
Systems.
Addison-Wesley.
CLARKSON,
J.
AND
ECKERT,
C.
2005.
Design
Process
Improvement:
A
Review
of
Current
Practice.
Springer-Verlag.
COMPULINK
MANAGEMENT
CENTER
INC.
2008.
Laserfiche
document
management.
http:
www.
laserfiche.com
resources
basics
index.html.
COUGHLAN,
T.
AND
JOHNSON,
P.
2008.
Personal
information
management
for
creative
practitioners.
In
Proceedings
of
the
Workshop:
Personal
Information
Management.
ACM
Press,
New
York,
NY.
CRABTREE,
R.
A.,
FOX,
M.
S.,
AND
BAID,
N.
K.
1997.
Case
studies
of
coordination
activities
and
problems
in
collaborative
design.
Resear.
Engin.
Des.
9,
2,
70
84.
CULLEY,
S.
J.,
COURT,
A.
W.,
AND
MCMAHON,
C.
A.
1992.
The
information
requirements
of
engineering
designers.
Engin.
Des.
18,
3,
21
23.
CULLEY,
S.
J.
AND
MCMAHON,
C.
A.
2005.
Perspectives
on
knowledge
management
in
design,
engineering
design.
Theory
and
practice:
A
symposium
in
honour
of
Ken
Wallace.
Cambridge,
Engineering
Design
Centre,
University
of
Cambridge,
UK.
CURTIS,
G.
AND
COBHAM,
D.
2005.
Business
Information
Systems:
Analysis,
Design
and
Practice.
Harlow,
Financial
Times
Prentice
Hall.
CUTRELL,
E.,
DUMAIS,
S.
T.,
AND
TEEVAN,
J.
2006.
Searching
to
eliminate
personal
information
management.
Comm.
ACM
49,
1,
58
64.
DEMIAN,
P.
AND
FRUCHTER,
R.
2006.
An
ethnographic
study
of
design
knowledge
reuse
in
the
architecture,
engineering,
and
construction
industry.
Res.
Engin.
Des.
16,
4,
184
195.
DIETEL,
J.
E.
2000.
Improving
corporate
performance
through
records
audit.
Inf.
Manag.
J.
34,
2,
18
24.
DONG,
A.
AND
AGOGINO,
A.
M.
1998.
Managing
design
information
in
enterprise-wide
CAD
using
smart
drawings
.
Comput.
Aid.
Des.
30,
6,
425
435.
DUMAIS,
S.,
CUTRELL,
E.,
SARIN,
R.,
AND
HORVITZ,
E.
2004.
Implicit
queries
for
contextualized
search.
In
Proceedings
of
the
International
Conference
on
Research
and
Development
in
Information
Retrieval.
ACM
Press,
New
York,
594.
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
23,
Publication
date:
September
2008.
Organizing
and
Managing
Personal
Electronic
Files
23:39
DUMAIS,
S.,
CUTRELL,
E.,
CADIZ,
J.,
JANCKE,
G.,
SARIN,
R.,
AND
ROBBINS,
D.
2003.
Stuff
I
ve
Seen:
A
system
for
personal
information
retrieval
and
reuse.
In
Proceedings
of
the
International
Conference
on
Research
and
Development
in
Information
Retrieval.
ACM
Press,
New
York,
72
79.
EDMUNDS,
A.
AND
MORRIS,
A.
2000.
The
problem
of
information
overload
in
business
organizations:
A
review
of
literature.
Int.
J.
Inf.
Manag.
20,
17
28.
ELSWEILER,
D.
AND
RUTHVEN,
I.
2007.
Towards
task-based
personal
information
management
evaluations.
In
Proceedings
of
the
30th
Annual
international
ACM
SIGIR
Conference
on
Research
and
Development
in
information
Retrieval
SIGIR
07
.
ACM,
New
York,
NY,
23
30.
ERICKSON,
T.
2006.
From
PIM
to
GIM:
Personal
information
management
in
group
contexts.
Comm.
ACM
49,
1,
74
75.
FAR,
B.
H.
AND
ELAMY,
A.
H.
2005.
Functional
reasoning
theories:
Problems
and
perspectives.
AI
Engin.
Des.
Manu.
19,
75
88.
GERO,
J.
S.
1990.
Design
prototypes:
a
knowledge
representation
schema
for
design.
AI
Mag.
11,
4,
26
36.
GROTH,
K.
AND
EKLUNDH,
K.
S.
2006.
Combining
personal
and
organizational
information.
In
Proceedings
of
the
SIGIR
Workshop:
Personal
Information
Management.
ACM
Press,
New
York,
NY,
2006.
GRUDIN,
J.
2001.
Desituating
action:
Digital
representations
of
context.
Hum.-Comput.
Interac.
16,
269
286.
HICKS,
B.
J.,
CULLEY,
S.
J.,
ALLEN,
R.
D.,
AND
MULLINEUX,
G.
2002.
A
framework
for
the
requirements
of
capturing,
storing
and
reusing
information
and
knowledge
in
engineering
design.
Int.
J.
Inf.
Manag.
22,
4,
263
280.
ISO
9660.
1988.
International
Organization
for
Standardization
Information
processing
Volume
and
file
structure
of
CD-ROM
for
information
interchange.
JESSUP,
L.
M.
AND
VALACICH,
J.
S.
2008.
Information
Systems
Today:
Managing
in
the
Digital
World.
Pearson
Prentice
Hall,
Upper
Saddle
River,
NJ.
JONES,
W.
2007.
Personal
information
management.
In
B.
Cronin,
Annual
Review
of
Information
Science
and
Technology,
41.
JONES,
W.,
PHUWANARTNURAK,
A.
J.,
GILL,
R.,
AND
BRUCE,
H.
2005.
Don
t
take
my
folders
away!:
Organizing
personal
information
to
get
things
done.
In
Proceedings
of
the
Extended
Abstracts
on
Human
Factors
in
Computing
Systems
CHI
05
.
ACM
Press,
New
York,
NY,
1505
1508.
KARGER,
D.
R.
AND
JONES,
W.
2006.
Data
unification
in
personal
information
management.
Comm.
ACM
49,
1,
77
82.
KARGER,
D.
R.
AND
QUAN,
D.
2004.
Haystack:
a
user
interface
for
creating,
browsing,
and
organizing
arbitrary
semistructured
information.
In
Proceedings
of
the
Extended
Abstracts
on
Human
Factors
in
Computing
Systems
CHI
04
.
ACM,
New
York,
NY,
777
778.
KWASNIK,
B.
H.
1989.
How
a
personal
document
s
intended
use
or
purpose
affects
its
classification
in
an
office.
In
Proceedings
12th
Annual
ACM
SIGIR
Conference
on
Research
and
Development
in
Information
Retrieval
SIGIR
89
.
ACM.
LANSDALE,
M.
W.
1988.
The
psychology
of
personal
information
management.
Appl.
Ergonomics
19,
1,
55
66.
LYMAN,
P.
AND
VARIAN,
H.
L.
2003.
How
much
information.
http:
www.sims.berkeley.edu
howmuch-info-2003.
MAHER,
M.
L.
AND
GOMEZ
DE
SILVA
GARZA,
A.
1997.
Case-based
reasoning
in
design.
IEEE
Expert
12,
2,
34
41.
MCALPINE,
H.,
HICKS,
B.
J.,
HUET,
G.
AND
CULLEY,
S.
J.
2006,
An
investigation
into
the
use
and
content
of
the
engineer
s
logbook.
Design
Studies
27,
4,
481
504.
MCMAHON,
C.
A.,
PITT,
D.
J.,
YANG,
Y.,
AND
SIMS
WILLIAMS,
J.
H.
1993.
Review:
An
information
management
system
for
informal
design
data.
In
Proceedings
of
the
1993
ASME
Computer
in
Engineering
Conference
and
Exposition,
K.
H.
Law,
Ed.
San
Diego,
CA,
215
226.
MCMAHON,
C.
A.,
LOWE,
A.,
CULLEY,
S.
J.,
CORDEROY,
M.,
CROSSLAND,
R.,
SHAH,
T.,
AND
STEWART,
D.
2004.
Waypoint:
An
Integrated
Search
and
Retrieval
System
for
Engineering
Documents.
J.
Comput.
Inf.
Sci.
Engin.
4,
329
338.
OVIATT,
S.
2006.
Human-centered
design
meets
cognitive
load
theory:
designing
interfaces
that
help
people
think.
In
Proceedings
of
the
14th
Annual
ACM
international
Conference
on
Multimedia
MULTIMEDIA
06
.
ACM,
New
York,
NY,
871
880.
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
23,
Publication
date:
September
2008.
23:40
B.
J.
Hicks
et
al.
PAHL,
G.
AND
BEITZ,
W.
1999.
Engineering
Design:
A
Systematic
Approach.
Springer,
Berlin,
Germany.
RACHURI,
S.,
SUBRAHMANIAN,
E.,
BOURAS,
A.,
FENVES,
S.
J.,
AND
SEBTI
FOUFOU,
R.
D.
S.
2007.
Information
sharing
and
exchange
in
the
context
of
product
lifecycle
management:
Role
of
standards.
Comput.-Aid.
Des.
RAVASIO,
P.
R.,
SCH
R,
S.
G.,
AND
KRUEGER,
H.
2004.
In
pursuit
of
desktop
evolution:
User
Problems
A
and
Practices
with
modern
desktop
systems.
In
ACM
Trans.
Comput.-Hum.
Interac.
TOCHI
11,
2,
156
180.
REGLI,
W.
C.,
HU,
X.,
ATWOOD,
M.,
AND
SUN,
W.
2000.
A
survey
of
design
rationale
systems:
approaches,
representation,
capture
and
retrieval.
Engin.
Comput.,
16,
3-4,
209
235.
SCHMIDT,
K.
AND
WAGNER,
I.
2004.
Ordering
systems:
Coordinative
practices
and
artifacts
in
architectural
design
and
planning.
Computer
Supported
Cooperative
Work
CSCW
,
Vol.
13,
Springer,
349
408.
SIEMENS
INC.
2008.
Teamcenter
Classification
Facilitating
product
and
process
reuse
using
Teamcentre
s
classification
and
reuse
capabilities.
http:
www.siemens.com
plm
.
SONNENWALD,
D.
H.
1996.
Communication
roles
that
support
collaboration
during
the
design
process.
Des.
Studies
17,
3,
277
301.
STEWART,
T.
A.
1994.
Surviving
information
overload.
Information
Technology
Special
Report,
Fortune
July
.
STEWART,
T.
A.
1997.
Intellectual
capital:
The
new
wealth
of
organizations.
Knowl.
Manag.
67.
SUCHMAN,
L.
2000.
Embodied
practices
of
engineering
work.
mind.
Culture
Activity
7,
1
2,
4
18.
SUTTON,
M.
J.
D.
1996.
Document
Management
for
the
Enterprise:
Principles,
Techniques,
and
Applications.
John
Wiley
Sons,
New
York,
NY.
SZYKMAN,
S.,
SRIRAM,
R.
D.,
BOCHENEK,
C.,
RACZ,
J.
W.,
AND
SENFAUTE,
J.
2000.
Design
repositories:
Engineering
design
s
new
knowledge
base.
IEEE
Intell.
Syst.
Appl.
15,
3,
48
55.
TEEVAN,
J.,
ALVARADO,
C.,
ACKERMAN,
M.
S.,
AND
KARGER,
D.
R.
2006.
The
perfect
search
engine
is
not
enough:
A
study
of
orienteering
behavior
in
directed
search.
In
Proceedings
of
the
SIGCHI
Conference
on
Human
Factors
in
Computing
Systems.
ACM
Press,
New
York,
NY,
415
422.
TSOUKAS,
H.
1996.
The
firm
as
a
distributed
knowledge
system:
A
constructionist
approach.
Strategic
Manag.
J.
17,
11
26.
TRUEARTH.
2008.
TruEarth
R
1km
satellite
imagery
filename
convention.
http:
www.truearth.
com
prod
1km
filenames
content.htm.
WARD,
M.
2001.
A
survey
of
engineers
in
their
information
world.
J.
Librarianship
Inf.
Sci.
33,
4,
168
176.
WHITTAKER,
S.
AND
HIRSCHBERG,
J.
2001.
The
character,
value,
and
management
of
personal
paper
archives.
In
ACM
Trans.
Comput.-Hum.
Interac.
8,
2,
150
170.
YANG,
M.
C.,
WOOD
III,
W.
H.,
AND
CUTKOSKY,
M.
R.
2005.
Design
information
retrieval:
a
thesauribased
approach
for
reuse
of
informal
design
information.
Engin.
Comput.
21,
2,
177
192.
Received
March
2007;
revised
February
2008,
May
2008;
accepted
July
2008
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
23,
Publication
date:
September
2008.
Information
Scraps:
How
and
Why
Information
Eludes
Our
Personal
Information
Management
Tools
MICHAEL
BERNSTEIN,
MAX
VAN
KLEEK,
and
DAVID
KARGER
Massachusetts
Institute
of
Technology
and
MC
SCHRAEFEL
University
of
Southampton
In
this
article
we
investigate
information
scraps--personal
information
where
content
has
been
scribbled
on
Post-it
notes,
scrawled
on
the
corners
of
sheets
of
paper,
stuck
in
our
pockets,
sent
in
email
messages
to
ourselves,
and
stashed
in
miscellaneous
digital
text
files.
Information
scraps
encode
information
ranging
from
ideas
and
sketches
to
notes,
reminders,
shipment
tracking
numbers,
driving
directions,
and
even
poetry.
Although
information
scraps
are
ubiquitous,
we
have
much
still
to
learn
about
these
loose
forms
of
information
practice.
Why
do
we
keep
information
scraps
outside
of
our
traditional
PIM
applications?
What
role
do
information
scraps
play
in
our
overall
information
practice?
How
might
PIM
applications
be
better
designed
to
accommodate
and
support
information
scraps
creation,
manipulation
and
retrieval?
We
pursued
these
questions
by
studying
the
information
scrap
practices
of
27
knowledge
workers
at
five
organizations.
Our
observations
shed
light
on
information
scraps
content,
form,
media,
and
location.
From
this
data,
we
elaborate
on
the
typical
information
scrap
lifecycle,
and
identify
common
roles
that
information
scraps
play:
temporary
storage,
archiving,
work-in-progress,
reminding,
and
management
of
unusual
data.
These
roles
suggest
a
set
of
unmet
design
needs
in
current
PIM
tools:
lightweight
entry,
unconstrained
content,
flexible
use
and
adaptability,
visibility,
and
mobility.
Categories
and
Subject
Descriptors:
H.5.2
Information
interfaces
and
presentation
:
User
Interfaces--Graphical
user
interfaces
This
work
has
been
partially
funded
by
Nokia
Research
Center
Cambridge
VIRPI
Project,
a
National
Science
Foundation
Graduate
Research
Fellowship,
the
EPSRC
UK
Overseas
Travel
Grant
EP1E035930,
the
Royal
Academy
of
Engineering
Global
Research
Award,
and
by
the
Web
Science
Research
Initiative.
Authors
addresses:
M.
Bernstein,
M.
Van
Kleek,
and
D.
Karger,
Computer
Science
and
Artificial
Intelligence
Laboratory,
Massachusetts
Institute
of
Technology,
32
Vassar
Street,
Cambridge
MA
02139;
email:
msbernst,emax,
karger
csail.mit.edu;
M.
C.
Schraefel,
School
of
Electronics
and
Computer
Science,
University
of
Southampton,
Southampton,
UK,
S017
IBJ;
email:
mc
ecs.soton.ac.uk.
Permission
to
make
digital
or
hard
copies
of
part
or
all
of
this
work
for
personal
or
classroom
use
is
granted
without
fee
provided
that
copies
are
not
made
or
distributed
for
profit
or
direct
commercial
advantage
and
that
copies
show
this
notice
on
the
first
page
or
initial
screen
of
a
display
along
with
the
full
citation.
Copyrights
for
components
of
this
work
owned
by
others
than
ACM
must
be
honored.
Abstracting
with
credit
is
permitted.
To
copy
otherwise,
to
republish,
to
post
on
servers,
to
redistribute
to
lists,
or
to
use
any
component
of
this
work
in
other
works
requires
prior
specific
permission
and
or
a
fee.
Permissions
may
be
requested
from
Publications
Dept.,
ACM,
Inc.,
2
Penn
Plaza,
Suite
701,
New
York,
NY
10121-0701,
USA,
fax
1
212
869-0481,
or
permissions
acm.org.
C
2008
ACM
1046-8188
2008
09-ART24
5.00
DOI
10.1145
1402256.1402263
http:
doi.acm.org
10.1145
1402256.1402263
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
24,
Publication
date:
September
2008.
24
24:2
M.
Bernstein
et
al.
General
Terms:
Design,
Experimentation,
Human
Factors
Additional
Key
Words
and
Phrases:
Personal
information
management,
note
taking,
information
scraps
ACM
Reference
Format:
Bernstein,
M.,
Van
Kleek,
M.,
Karger,
D.,
and
Schraefel,
M.
C.
2008.
Information
scraps:
How
and
why
information
eludes
our
personal
information
management
tools.
ACM
Trans.
Inform.
Syst.,
26,
4,
Article
24
September
2008
,
46
pages.
DOI
10.1145
1402256.1402263
http:
doi.acm.org
10.1145
1402256.1402263
1.
INTRODUCTION
Despite
the
number
of
personal
information
management
tools
available
today,
a
significant
amount
of
our
information
remains
out
of
their
reach:
the
content
is
instead
scribbled
on
Post-it
notes,
scrawled
on
the
corners
of
sheets
of
paper,
stuck
in
our
pockets,
sent
in
email
messages
to
ourselves,
and
stashed
into
miscellaneous
digital
text
files.
This
scattered
information
ranges
from
ideas
and
sketches
to
notes,
reminders,
shipment
tracking
numbers,
driving
directions,
and
even
poetry.
The
information
may
never
make
its
way
into
our
usual
PIM
applications--yet
we
carry
it
around
with
us,
decorate
our
desks
with
it,
and
often
even
make
sure
to
archive
it.
For
a
category
of
personal
information
with
so
little
traditional
support,
it
is
all
but
ubiquitous
in
our
lives.
We
refer
to
these
pieces
of
personal
information
as
information
scraps.
The
term
suggests
several
images:
notes
that
are
written
on
a
scrap
of
paper,
that
are
incomplete,
or
that
have
been
separated
from
our
primary
personal
information
tools.
As
a
class
of
personal
information,
much
remains
unknown
about
information
scraps.
What
similarities
exist
among
scrap
features
and
management
practices?
Why
are
information
scraps
so
often
held
outside
of
our
traditional
PIM
locations
and
instead
on
scraps
of
paper
or
in
text
files?
Why
do
we
manage
other
scraps
by
coopting
our
traditional
PIM
applications
against
their
intended
modes
of
use,
such
as
by
composing
emails
addressed
to
ourselves?
If
these
unorganized
bits
truly
indicate
the
limits
of
our
PIM
tools,
how
might
we
begin
to
build
better
tools?
Our
goal
in
this
research
is
to
open
an
investigation
of
information
scraps,
so
that
we
might
begin
answering
these
questions.
In
this
paper,
we
investigate
the
nature
and
use
of
information
scraps.
We
contribute
a
cross-tool
methodology
for
studying
existing
information
scraps,
and
apply
this
methodology
to
an
investigation
of
information
scrap
practice.
In
our
study,
we
investigate
the
information
types
Jones
2007a
stored
in
information
scraps,
scraps
layout
and
language,
the
tools
used
in
support
of
information
scrap
work,
and
the
information
scrap
life
cycle.
Our
artifact
investigation
reveals
a
large
diversity
of
information
types
captured
in
scraps;
these
uncommon
types
cumulatively
account
for
a
sizable
percentage
of
all
information
scraps.
Through
analysis
of
our
results,
we
derive
a
characterization
of
the
typical
roles
that
information
scraps
serve
in
personal
information
practice:
temporary
storage,
cognitive
support,
reminding,
information
archiving,
and
recording
of
unusual
information
types.
These
roles
suggest
a
set
of
unmet
design
needs
in
current
PIM
tools:
lightweight
entry,
unconstrained
content,
flexible
use
and
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
24,
Publication
date:
September
2008.
Information
Scraps
24:3
adaptability,
visibility,
and
mobility.
Finally,
we
describe
approaches
that
we
believe
will
be
the
most
successful
in
the
information
scrap
management
tools
of
tomorrow.
1.1
Information
Scrap
Definition
Though
our
investigation
began
without
a
firm
definition
for
the
term
information
scrap,
we
have
continued
to
refine
our
ideas.
Firm
boundaries
around
what
items
constitute
and
don
t
constitute
information
scraps
allow
us
to
relate
our
efforts
to
prior
work,
communicate
our
ideas
to
a
general
audience,
and
scope
our
research
program.
An
information
scrap
is
an
information
item
that
falls
outside
all
PIM
tools
designed
to
manage
it.
This
definition
suggests
that
information
scraps
include
items
such
as
address
information
not
in
the
address
book,
electronic
communication
not
in
the
email
client,
and
to-dos
not
in
a
to-do
manager.
It
intentionally
includes
information
items
for
which
no
PIM
tools
currently
exist,
as
well
as
information
items
stored
and
managed
in
general-purpose
e.g.,
non-PIM
information
tools.
For
the
purposes
of
our
work,
we
choose
to
include
tools
such
as
notebooks,
spreadsheets,
and
text
editors
word
processors
in
the
set
of
general
purpose
tools
because
they
tend
to
be
catch-alls
for
PIM
data.
Our
definition
also
intentionally
makes
no
distinction
between
paper
and
digital
PIM
tools.
To
illustrate,
here
are
some
examples
of
information
scraps
from
our
research:
--
Note
of
how
to
make
a
call
abroad
saved
as
a
text
file
in
a
Miscellaneous
folder
--
To-do
on
a
Post-it
note
--
Photo
of
a
whiteboard
from
a
discussion
kept
on
the
computer
desktop
--
Meeting
notes
in
a
general-purpose
paper
notebook
--
Serial
number
for
an
application
saved
in
an
email
to
yourself
--
A
friend
s
phone
number
written
on
a
piece
of
scratch
paper
--
Cooking
recipes
kept
in
a
personal
wiki
--
Song
lyrics
and
guitar
tabs
taped
on
the
wall
--
A
copy
of
an
academic
transcript
saved
in
a
text
file
By
our
definition,
information
scraps
are
the
personal
information
items
that
have
fallen
between
the
cracks
of
our
PIM
tools.
An
information
scrap
is
evidence
that
there
is
no
appropriate
tool
at
a
time
of
need;
the
user
deliberately
chooses
a
tool
with
affordances
designed
for
other
forms
of
information.
In
our
analysis,
we
treat
the
existence
of
such
items
as
evidence
of
PIM
design
failures
and
thus
suggestive
of
unfulfilled
design
opportunities.
We
might
also
examine
whether
information
scraps
constitute
a
single,
welldefined
information
type.
Jones
defines
an
information
type
or
information
form
by
the
constellation
of
tools
and
applications
that
make
it
possible
to
manipulate
a
set
of
items
Jones
2007a,
p.
7
.
Information
scraps
might
be
defined
in
terms
of
the
characteristically
large
and
varied
set
of
tools
that
people
use
to
typically
create,
manage
and
hold
their
scraps,
including
Post-its,
email
clients,
word
processors,
and
even
the
backs
of
hands.
This
approach
fails
to
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
24,
Publication
date:
September
2008.
24:4
M.
Bernstein
et
al.
yield
a
useful
definition,
however,
because
these
tools
are
sufficiently
general
that
they
could
be
used
to
hold,
create,
and
manipulate
nearly
any
other
kind
of
information.
Also,
our
informal
notion
of
information
scraps
includes
items
that
theoretically
can
be
created
and
held
in
any
application
or
tool.
Thus,
the
set
of
tools
supporting
information
scrap
creation
and
manipulation
is
the
set
of
all
applications,
which
is
not
a
useful
definition
for
us
either.
The
definition
given
in
this
section
was
developed
over
time
and
engagement
with
the
participants
of
our
study
via
a
grounded
theory
approach.
As
the
study
described
in
this
paper
was
mainly
exploratory,
we
developed
an
interim
definition
described
in
Section
4.1
as
a
theory-building
placeholder.
1.2
Results
Overview
Our
study
consisted
of
27
semi-structured
interviews
and
artifact
examinations
of
knowledge
workers
across
five
organizations.
Among
the
artifacts
we
collected,
we
found
information
scraps
to
encode
a
small
set
of
popular
information
types,
including
to-dos,
meeting
notes
and
contact
information,
but
also
a
wide
variety
of
uncommon
types,
including
fantasy
football
lineups,
passwords
and
guitar
tabs.
Information
scraps
often
exhibited
abbreviated
language
and
underspecified
data,
sometimes
complemented
by
sketches
and
freehand
drawings.
Tools
such
as
email
and
paper
notebooks
were
most
popular,
although
more
structured
tools
such
as
calendars
were
often
adapted
or
coopted
as
well.
Synthesizing
these
results,
we
consolidated
a
set
of
roles
that
information
scraps
commonly
play
in
personal
information
practice:
--
Temporary
storage:
created
with
a
short
life
expectancy
and
retained
as
memory
prostheses
Lamming
et
al.
1994
until
such
time
as
their
usefulness
has
expired.
--
Cognitive
support:
works
in
progress,
brainstorms,
and
other
instances
of
thinking
it
through
on
paper.
--
Archiving:
intended
to
hold
on
to
important
information
reliably
for
long
periods
of
time--for
example,
web
site
passwords
or
descriptions
of
how
to
perform
a
complicated
task.
--
Reminding:
placed
in
the
way
of
our
future
movements
and
activities,
thereby
leaving
reminders
for
ourselves.
--
Unusual
information
types:
captured
because
the
information
did
not
fit
in
the
participant
s
existing
PIM
tools.
By
examining
the
reasons
why
information
scraps
were
used
in
the
preceding
roles,
we
derived
a
set
of
needs
and
design
affordances
for
information
scrap
management:
--
Lightweight
capture:
lowering
the
time
and
effort
barriers
associated
with
capturing
personal
information.
--
Flexible
contents
and
representation:
allowing
multiple
capture
modalities,
and
enabling
the
user
to
capture
whatever
information
is
important.
--
Flexible
use
and
organization:
enabling
users
to
devise
their
own
organizational
systems
or
to
adapt
tools
and
information
in
novel
ways.
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
24,
Publication
date:
September
2008.
Information
Scraps
24:5
--
Visibility
and
reminding:
priming
information
so
that
it
is
likely
to
be
encountered
at
a
desired
point
in
the
future.
--
Mobility
and
availability:
migrating
personal
information
to
and
from
mobile
scenarios.
Finally,
we
applied
these
insights
to
motivate
PIM
tool
designs
that
provide
these
affordances
and
are
thus
better
prepared
to
handle
the
exigencies
of
messy
personal
information
practices.
2.
RELATED
WORK
In
this
section
we
situate
our
study
of
information
scraps
among
the
rich
body
of
research
already
surrounding
PIM-related
activities.
We
begin
by
reviewing
related
research
on
the
psychological
underpinnings
behind
information
scrap
practice.
Then,
we
canvas
specific
practices
related
to
individual
information
scrap
types
such
as
to-dos
and
general
personal
information
management
practices
as
they
bear
on
information
scraps.
2.1
Psychological
Foundations
There
exists
extensive
psychological
literature
surrounding
our
motivations
for
creating
and
manipulating
information
scraps.
Perhaps
the
simplest
framing
of
the
problem
was
formulated
by
Ross
and
Nisbett
1991
in
channel
factors,
the
small
but
critical
facilitators
or
barriers
to
an
action.
Ross
and
Nisbett
demonstrated
the
amplified
effects
that
small
difficulties
or
facilitators
will
have
on
human
action,
just
as
a
pebble
placed
at
the
fork
of
a
stream
can
dramatically
divert
the
course
of
water.
Seemingly
small
time
and
effort
requirements
such
as
booting
up
a
laptop
might
thus
be
perceived
as
enough
of
a
burden
to
cause
us
to
use
other
means
of
capture
such
as
writing
on
our
hands.
There
are
many
such
channel
factors
encouraging
us
to
create
information
scraps.
Lansdale
1988
was
the
first
to
relate
psychology
to
the
study
of
personal
information
management;
in
his
work,
he
noted
classification,
or
filing,
as
a
cognitively
difficult
activity
of
special
note.
This
result
suggests
that
information
scraps
may
be
created
when
the
cost
of
filing
a
piece
of
information
is
perceived
to
be
too
high--whether
choosing
a
point
in
a
folder
hierarchy
or
deciding
which
of
several
related
applications
to
use.
Csikszentmihalyi
1991
identified
humans
desire
to
maintain
a
state
of
flow
where
uninterrupted
concentration
is
highest;
Bederson
2004
translated
this
concept
into
interaction
design
principles
in
support
of
the
flow
state.
When
the
user
is
in
a
flow
state,
Bederson
and
Csikszentmihalyi
argue
that
unrelated
thoughts
and
interruptions
may
be
undesirable,
causing
us
to
write
them
down
as
quickly
as
possible
before
our
original
thought
was
lost.
Information
scraps
often
serve
as
a
memory
prosthesis
Lamming
et
al.
1994
or
exosomatic
memory,
later
used
to
remind
us
of
the
original
thought.
Scraps
can
help
us
index
into
our
memory
via
a
variety
of
cues.
Location
is
a
very
powerful
memory
primer
Darken
and
Sibert
1993;
Jones
and
Dumais
1986;
Robertson
et
al.
1998;
Yates
1966
;
a
combination
of
knowing
what
and
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
24,
Publication
date:
September
2008.
24:6
M.
Bernstein
et
al.
when
can
also
effectively
aid
recall
of
the
rest
of
a
memory
Wagenaar
1986
.
We
are
also
able
to
recall
a
variety
of
contextual
information
about
our
documents
to
potentially
aid
in
refinding,
such
as
textual
content,
visual
elements,
file
type,
or
implicit
narratives
around
file
creation
Barreau
1995;
Blanc-Brude
and
Scapin
2007;
Goncalves
and
Jorge
2004
.
However,
many
in
formation
scraps
do
not
include
such
metadata;
it
is
unknown
whether
the
highly
abbreviated
contents
of
many
information
scraps
e.g.,
Joe
the
attorney
Bellotti
et
al.
2004
are
more
powerful
memory
cues
than
those
previously
mentioned.
Often,
we
fail
to
create
memory
prostheses
such
as
information
scraps
even
when
they
might
later
be
useful.
We
are
habitually
overconfident
in
our
own
knowledge
and
memory
Lichtenstein
et
al.
1982
,
leading
to
conscious
choices
not
to
remember
an
item
that
later
becomes
unexpectedly
valuable.
Further,
even
if
we
chose
to
write
down
or
make
an
effort
to
remember,
we
do
not
always
utilize
this
information
when
recall
is
needed.
Our
memory
s
faulty
yet
quick
access
is
often
preferred
over
accurate
but
slower
external
memory
aids
Gray
and
Boehm-Davis
2000,
Gray
and
Fu
2001,
Kalnikait
and
Whittaker
2007
.
e
Such
a
preference
suggests
that
information
scraps
may
only
be
deliberately
re-accessed
when
our
own
memory
has
failed.
2.2
Information
Scraps
in
Studies
of
Specific
Data
Types
Information
scraps
take
many
forms,
and
researchers
have
noted
their
existence
across
a
number
of
type-specific
studies.
Here
we
review
the
relevant
work
by
type.
Perhaps
the
most
typical
information
scrap
is
the
note-to-self.
Through
a
series
of
semi-structured
interviews,
Lin
et
al.
2004
arrived
at
a
model
of
such
notes
life
cycle:
trigger,
record,
transfer
or
maintain
and
refer,
complete,
discard
or
archive.
Campbell
and
Maglio
2003
identified
salient
characteristics
of
what
they
termed
notable
information,
including
transience,
visibility,
mobility,
ability
to
post,
transferability,
short
length,
and
ease
of
both
creation
and
destruction.
The
authors
further
observed
a
strong
preference
for
paper-based
media
over
digital
media.
Dai
et
al.
2005
investigated
this
preference
by
interviewing
expert
users
of
PDA
memo
applications
to
suggest
future
design
directions;
users
were
typically
most
hindered
by
a
lack
of
organizational
support
for
their
digital
notes.
Hayes
et
al.
2003
studied
the
phenomenon
of
short
important
thoughts,
uncovering
a
strong
need
for
ubiquity
and
mixed-initiative
systems
in
the
support
of
such
information.
Strikingly,
73
of
Hayes
et
al.
s
participants
reported
regularly
transcribing
such
notes
onto
another
medium,
suggesting
transfer
as
an
especially
important
stage
of
the
information
scrap
life
cycle.
Professional
fields
often
encourage
similar
notetaking
practices
in
the
workplace.
Paper
engineering
logbooks,
long
a
common
practice
for
professional
engineers
to
use
for
recording
notes
and
ideas,
were
found
to
commonly
serve
as
reminders
of
work
in
progress
and
as
a
personal
work
record
for
future
reference
McAlpine
et
al.
2006
.
Meeting
notes
created
by
professional
information
workers
contain
a
large
number
of
facts
e.g.,
names,
phone
numbers,
technical
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
24,
Publication
date:
September
2008.
Information
Scraps
24:7
details,
and
procedures
and
action
items
Khan
1993
.
The
degree
to
which
these
practices
translate
across
other
logbook-intense
professions
such
as
the
sciences
Schraefel
et
al.
2004
is
not
yet
clear.
Studies
of
email
use
have
revealed
a
wide
range
of
behaviors
from
information
keeping
to
collaboration
and
coordination,
leading
to
characterizations
of
email
as
a
habitat
in
which
we
embed
much
of
our
personal
information
Ducheneaut
and
Bellotti
2001
.
As
a
result
of
this
embedding,
we
see
e-mail
used
for
a
variety
of
purposes
in
common
with
information
scraps.
E-mails
are
deliberately
marked
as
unread
or
left
unorganized
in
the
inbox
to
serve
as
reminders
or
to-dos,
and
half-completed
messages
are
saved
along
with
notes
for
what
to
include
Bellotti
et
al.
2005
.
Venolia
et
al.
2001
suggest
that
such
coping
strategies
are
due
to
the
sheer
volume
of
incoming
messages.
Whittaker
and
Sidner
s
1996
early
study
found
that
35
of
folders
contained
only
one
or
two
emails,
suggesting
that
many
of
these
emails
may
have
had
no
natural
application
and
required
small,
artificial
homes
to
be
created.
More
recently,
we
have
learned
that
nearly
a
third
of
all
archived
e-mail
is
actually
sent
by
the
owner
to
him-
or
herself
Fisher
et
al.
2006,
Jones
et
al.
2005a
--another
common
information
scrap
pattern.
The
ubiquity
of
to-dos,
scattered
in
unorganized
locations
across
the
physical
and
virtual
workspace,
suggests
that
they
might
constitute
a
particularly
common
form
of
information
scrap.
Bellotti
et
al.
2004
undertook
the
most
rigorous
investigation
of
to-do
practice
to
date.
Their
findings
suggest
that
todos
are
created
by
expending
as
little
effort
as
necessary,
and
only
elaborated
enough
to
provide
a
salient
clue
to
the
original
author
e.g.,
a
to-do
containing
only
the
text
Joe
the
attorney
.
To-dos
are
often
integrated
as
resources
into
ongoing
work,
incorporating
state
or
links
to
other
artifacts.
Bellotti
et
al.
s
investigation
uncovered
a
large
number
of
separate
tools
average
11.25
per
person
being
used
to
manage
to-dos,
noting
that
often
to-dos
are
intentionally
placed
in
the
way
of
a
typical
routine
to
promote
visibility,
rather
than
in
usual
filing
schemes.
These
fragments
or
notes
are
very
much
in
keeping
with
our
definition
of
information
scraps:
they
are
deliberately
not
kept
in
an
application
like
a
to-do
list;
they
are
in
a
specific
elsewhere.
These
locations
might
include
the
backs
of
hands,
scraps
of
paper,
unstructured
text
files,
and
Post-its
Blandford
and
Green
2001
.
Calendaring
tools
also
display
many
of
the
characteristics
of
information
scrap
work.
Users
keep
a
plethora
of
nonappointment
but
still
time-based
information
scraps
in
their
calendaring
tools:
notes
of
which
week
of
the
semester
it
is,
pointers
from
a
diary
entry
to
supporting
materials,
reminders,
reports
of
how
time
was
actually
spent,
and
notes
of
prospective
but
not
finalized
events,
among
others
Blandford
and
Green
2001
.
In
their
studies
of
email
and
task
management,
Bellotti
et
al.
2005
also
noted
that
participants
would
create
calendar
events
as
reminders.
The
consideration
of
web
and
Internet
material
raises
further
issues.
The
Keeping
Found
Things
Found
project
has
investigated
the
means
by
which
users
keep
web
information
Bruce
et
al.
2004;
Jones
et
al.
2001;
2005a
.
Participants
in
these
studies
often
captured
information
on
the
web
using
tools
outside
of
browsers
native
bookmarking
and
history
facilities,
instead
emailing
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
24,
Publication
date:
September
2008.
24:8
M.
Bernstein
et
al.
themselves
URLs
with
comments,
saving
web
pages
to
disk,
or
printing
out
information
and
filing
away
the
hard
copy.
In
their
ubiquity,
camera
phones
have
recently
become
a
popular
mechanism
for
information
scrap
collection.
Ito
describes
the
camera
phone
s
ability
to
add
meaning
to
the
mundane
objects
in
our
lives,
for
example
with
photographs
of
the
seashell
we
found
on
the
beach,
the
street
sign
that
will
allow
us
to
re-find
a
restaurant,
and
other
objects
that
are
simply
interesting
in
some
way
Ito
and
Okabe
2003
.
Such
one-off
photos
may
fall
into
the
domain
of
information
scraps
as
they
exist
outside
the
domain
of
typical
personal
information
categories,
may
serve
unclear
purposes,
and
may
be
difficult
to
categorize.
These
scrap
pictures
may
be
numerous,
as
well:
images
captured
for
personal
reflection
or
reminiscence
were
the
most
numerous
of
those
indexed
by
Kindberg
et
al.
2004
.
2.3
Organizational
Practice
Studies
of
physical
document
organization
and
information
workers
offices
have
revealed
several
behaviors
common
to
information
scraps,
including
an
aversion
to
filing
and
an
affinity
for
paper
media.
Malone
s
1983
seminal
paper
on
office
organization
examined
the
existence
of
unorganized
piles
in
office
work.
He
found
that
piles
served
as
reminders
of
unfinished
tasks
and
lessened
the
cognitive
effort
associated
with
filing
documents.
Whittaker
and
Hirschberg
2001
discovered
that
working
notes
for
current
projects
constituted
17
of
the
paper
archives
maintained
across
an
office
move--many
of
these
working
notes
were
handwritten
and
irreplaceable,
likely
containing
many
information
scraps
e.g.,
meeting
notes
and
brainstorms
.
In
The
Myth
of
the
Paperless
Office,
Sellen
and
Harper
2003
detail
numerous
reasons
for
the
continued
prevalence
of
paper
in
the
workplace,
including
ease
of
annotation,
flexible
navigation,
spatial
reorientability
and
support
for
collaboration.
These
affordances
align
with
many
common
information
scrap
needs.
Other
studies
have
revealed
similar
organizational
tendencies
for
information
stored
in
digital
tools.
In
parallel
to
Whittaker
and
Hirschberg
s
description
of
working
papers,
Barreau
and
Nardi
detail
what
they
term
ephemeral
information--that
which
has
a
short
shelf
life.
Many
information
scraps
exhibit
the
characteristics
of
ephemeral
information:
they
are
loosely
filed
or
not
filed
at
all,
and
difficult
to
manage
in
large
quantities
Barreau
and
Nardi
1995
.
Boardman
and
Sasse
2004
noted
that
their
participants
tended
to
combine
filing
and
piling
strategies
based
on
item
priority,
regularly
filing
items
of
high
perceived
value
but
otherwise
leaving
their
collections
to
spring
cleaning
or
no
organization
at
all.
Boardman
and
Sasse
further
reported
that
3
of
files,
41.6
of
email,
and
38.8
of
bookmarks
remained
unfiled
over
their
longitudinal
study--again,
the
forces
driving
these
artifacts
to
remain
unfiled
will
likely
also
exist
for
information
scraps.
Digital
folder
structures
often
remain
ad-hoc
and
relatively
flat.
Jones
investigated
folders
in
the
service
of
ongoing
projects
Jones
et
al.
2005c
,
and
found
that
folders
semantics
were
continually
adapted
to
reflect
each
participant
s
evolving
understanding
of
a
project
and
its
components.
This
result
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
24,
Publication
date:
September
2008.
Information
Scraps
24:9
may
hold
for
information
scraps
as
well,
whose
boundaries
are
even
less
clearly
delineated
than
project
folders.
Similarly,
Barreau
and
Nardi
1995
investigated
digital
file
hierarchies
and
discovered
that
most
participants
hierarchies
were
unexpectedly
shallow
due
to
low
perceived
future
usefulness
of
complex
archives.
Rather,
digital
hierarchies
were
structured
in
the
service
of
what
they
referred
to
as
location-based
finding:
navigating
to
a
directory
of
interest
and
proceeding
to
browse.
This
result
suggests
that
users
preferred
to
depend
on
their
own
cognitive
capacities
for
recognition
of
documents
rather
than
ensure
that
everything
was
elaborately
filed.
We
suspect
that
this
preference
may
also
hold
when
filing
information
scraps.
Difficulty
categorizing
information
at
the
time
it
is
captured
has
been
repeatedly
examined
as
a
modulator
of
how
information
is
written
down.
Malone
s
participants
complained
of
the
difficulty
of
accurately
filing
paper
information
Malone
1983
;
Bowker
and
Star
expand
this
point
to
the
digital
realm:
A
quick
scan
of
one
of
the
author
s
desktops
reveals
eight
residual
categories
represented
in
the
various
folders
of
email
and
papers:
fun,
take
back
to
office,
remember
to
look
up,
misc.,
misc.
correspondence,
general
web
information,
teaching
stuff
to
do,
and
to
do.
We
doubt
if
this
is
an
unusual
degree
of
disarray
or
an
overly
prolific
use
of
the
none
of
the
above
category
so
common
to
standardized
tests
and
surveys.
Bowker
and
Star
2000
p.
2
Information
fragmentation
Jones
2004
of
information
scraps
occurs
across
devices,
applications,
and
media.
In
a
cross-tool
study,
Boardman
et
al.
2003
reported
on
three
consequences
of
fragmentation:
1
file
compartmentalization
across
tools,
2
lack
of
ability
to
coordinate
work
activity
between
tools,
and
3
inconsistent
design
vocabularies.
Fragmentation
leads
to
undesirable
effects
such
as
an
inability
to
gather
all
information
about
a
single
person
or
topic
or
to
effectively
link
such
data
Karger
2007
.
While
fragmentation
mainly
occurs
between
applications,
mobile
situations
lead
to
fragmentation
instead
across
devices
such
as
cell
phones,
laptops,
notebooks
and
other
mobile
devices
Oulasvirta
and
Sumari
2007
.
We
hypothesize
that
the
low-effort,
spontaneous
capture
needs
of
information
scraps
lead
them
to
be
particularly
susceptible
to
both
application
and
device
fragmentation.
Data
unification
approaches
have
been
proposed
as
potential
solutions
to
these
problems
Bergman
et
al.
2003;
Boardman
et
al.
2003;
Jones
et
al.
2005b;
Karger
2007;
Karger
and
Quan
2004
.
3.
GOALS
In
the
research
presented
here,
we
have
targeted
a
type
of
personal
information
with
many
unanswered
questions--one
that
highlights
the
ad-hoc,
unorganized
underbelly
of
personal
information.
Our
focus
is
on
understanding
why
information
scraps
exist,
what
kinds
of
information
they
hold,
why
they
end
up
in
the
tool
or
medium
they
do,
and
how
they
evolve
through
their
lifetime.
We
entered
our
study
hoping
to
understand
to
following:
--
Characterization
of
the
phenomenon.
What
is,
and
is
not,
an
information
scrap?
Can
we
improve
our
intuitive
understanding
into
a
more
precise
characterization
of
the
phenomenon?
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
24,
Publication
date:
September
2008.
24:10
M.
Bernstein
et
al.
--
Type
variety.
What
kind
of
data
will
be
encoded
in
information
scraps?
How
much
variety
will
there
be,
and
which
information
types
will
be
the
most
popular?
--
Structure
and
expression.
How
does
a
calendar
item
as
an
information
scrap
compare
to
a
similar
item
in
a
digital
calendar
such
as
Outlook?
Will
the
information
scrap
carry
less
information,
or
express
it
in
a
different
way?
--
Tools.
Information
scraps
are
by
definition
held
in
inappropriate
or
generalpurpose
tools.
What
tools
are
these,
and
why
do
we
use
them?
How
do
we
adapt
the
tools
to
hold
information
they
may
not
have
been
designed
to
carry?
To
what
extent
does
fragmentation
take
place
across
tools,
and
does
this
fragmentation
inhibit
later
refinding
or
re-use?
--
User
needs.
What
needs
do
information
scraps
serve?
Why
are
they
used
in
preference
to
other
PIM
techniques?
4.
METHOD
We
conducted
a
study
consisting
of
27
semi-structured
interviews
and
artifact
examinations
of
participants
physical
and
digital
information
scraps.
From
our
previous
work
Bernstein
et
al.
2007;
Van
Kleek
et
al.
2007
it
had
become
clear
that
information
scraps
are
a
fundamentally
cross-tool
phenomenon,
so
we
chose
a
cross-tool
study
inspired
by
Boardman
and
Sasse
2004
.
This
study
design
allowed
us
to
examine
the
many
locations
and
tools
in
which
we
believed
scraps
might
appear.
As
our
questions
primarily
surrounded
information
scrap
content,
organization,
location,
and
life
cycle,
we
chose
to
focus
on
examining
the
scraps
themselves
rather
than
the
capture
or
retrieval
process.
Diary
studies
and
experience
sampling
studies
would
have
also
allowed
us
to
record
information
scraps
as
they
were
generated,
but
we
were
concerned
that
the
additional
time
and
energy
burden
on
participants
would
have
conflicted
with
the
overriding
importance
participants
place
on
ease
and
speed
when
capturing
scraps--participants
may
have
simply
chosen
not
to
record
the
scrap
to
avoid
the
effort
associated
with
writing
in
their
diaries.
We
carried
out
the
interviews
with
participants
from
five
different
organizations,
following
a
five-person
pilot
study
in
our
lab.
Three
of
the
organizations
we
visited
were
information
technology
firms,
focusing
on
mobile
communication,
interactive
information
retrieval,
and
wireless
communication.
One
was
small
start-up
,
another
medium
sized,
and
the
third
a
large
international
corporation.
The
fourth
organization
was
an
Internet
consortium,
working
internationally
in
a
highly
distributed
fashion.
The
fifth
was
an
academic
research
lab.
We
interviewed
7
managers
MAN
,
7
engineers
ENG
,
6
administrative
or
executive
assistants
ADMN
,
2
finance
workers
FIN
,
2
usability
professionals
UI
,
1
technical
writer
WR
,
1
campus
recruiting
officer
REC
and
1
industrial
researcher
RES
.
There
were
13
males
and
14
females;
the
median
and
mode
age
range
was
30
35.
Educational
level
ranged
between
some
college
4
,
college
degree
11
,
and
graduate
degree
12
.
This
population
was
a
diverse
group
of
professional
knowledge
workers
with
a
skew
toward
those
working
in
information
technology.
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
24,
Publication
date:
September
2008.
Information
Scraps
24:11
Interviews
were
performed
at
each
participant
s
main
computer
at
his
or
her
typical
place
of
work.
For
privacy
reasons,
participants
were
free
to
refrain
from
sharing
any
particular
piece
of
personal
information.
During
the
interviews,
we
did
not
use
the
term
information
scrap;
rather,
we
asked
participants
to
tell
us
about
information
that
they
had
that
was
not
formally
recorded
in
a
proper
place,
like
a
calendar
or
project
folder.
Participants
then
provided
us
with
a
stream
of
examples
that
we
noted
as
they
discussed
these
exemplar
artifacts.
Our
questions
focused
on
revealing
the
purpose
of
the
item,
the
reason
it
was
recorded
the
way
it
was,
as
well
as
where
it
fit
in
any
context
or
process
of
use.
4.1
Information
Scrap
Operationalization
As
discussed
in
the
introduction,
the
term
information
scrap
can
be
difficult
to
define.
However,
for
purposes
of
internal
validity,
and
lacking
a
rigorous
definition
at
the
time
of
our
study,
we
required
an
operationalization
of
the
term
that
would
allow
us
to
identify
which
artifacts
to
record.
We
used
this
operationalization
to
decide
which
artifacts
were
in
scope;
we
did
not
generally
attempt
to
convey
this
definition
to
the
subjects
of
the
study.
Throughout
the
discussion
of
the
study,
we
refer
to
an
information
scrap
as
a
piece
of
personal
information
that:
--
is
in
a
tool
with
no
explicit
support
for
that
information
s
schema,
e.g.,
a
phone
number
on
a
Post-it
note
or
--
has
no
tool
specifically
designed
to
handle
that
kind
of
information,
e.g.,
an
application
serial
number
or
--
is
in
a
tool
that
does
not
seem
particularly
well
suited
to
the
information
type.
e.g.,
a
to-do
with
where
to
remind
me
information
shoehorned
into
the
details
field
.
The
following
are
examples
of
artifacts
that
we
excluded
from
our
capture:
email
serving
a
communicative
purpose,
word
processor
documents
with
papers
or
full
essays,
and
contact
information
in
the
computer
address
book.
This
definition
carries
a
connotation
that
what
is
and
is
not
an
information
scrap
depends
on
each
participant
s
tools,
needs,
and
practices.
Based
on
the
results
of
this
study,
we
refined
our
definition
to
that
presented
in
Section
1.1.
4.2
Triangulation
Method
Information
scraps
are
distributed
among
tools
and
locations,
and
strategies
vary
from
person
to
person.
We
faced
a
challenge
in
our
artifact
examination--
specifically,
that
we
might
not
uncover
some
classes
of
each
participant
s
information
scraps.
We
could
canvas
the
space
by
asking
about
all
known
tools--but
what
if
the
participant
used
a
tool
we
didn
t
know
about,
or
used
a
common
application
in
a
way
we
did
not
think
to
investigate?
We
could
instead
query
by
location,
such
as
Desktop
or
Miscellaneous
folders--but
what
if
the
data
lived
in
an
application
rather
than
a
folder?
We
could
ask
how
participants
dealt
with
common
scrap
types
such
as
how-to
guides
and
URLs--but
certainly
there
would
be
types
we
might
leave
out.
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
24,
Publication
date:
September
2008.
24:12
M.
Bernstein
et
al.
Table
I.
The
three
categories
of
Tools,
Locations
and
Types
characterized
the
main
starting
points
for
our
artifact
study.
Triangulation
Perspective
Tools
Examples
Email
What
was
targeted
Messages
that
do
not
serve
communication
purposes:
emails
sent
to
oneself,
in
the
Drafts
folder,
or
archived
in
the
inbox.
Calendar
entries
that
did
not
correspond
to
actual
events;
use
of
the
details
field.
Bookmarks
carrying
information
beyond
just
a
pointer
to
a
web
page--for
example,
todo
or
toread
bookmark
folders.
All
available
data
this
location
commonly
holds
information
scraps
.
All
available
data
this
location
commonly
holds
information
scraps
.
All
available
data
this
location
commonly
holds
information
scraps
.
todo.txt
or
todo.doc
files
containing
personal
notes,
to-dos,
and
other
data.
Documents
of
short-term
interest
and
notes
to
self.
Freeform
notes
and
documents
of
short-term
interest.
Data
that
was
difficult
to
categorize.
Participant-authored
decorations
or
annotations.
To-dos
not
in
the
to-do
manager
or
that
did
not
fit
the
to-do
manager
s
schema.
All
examples
no
known
application
to
organize
this
information
.
Examples
not
held
in
a
bookmarking
utility.
Examples
not
held
in
a
contact
utility.
All
examples
common
information
scrap
.
All
examples
common
information
scrap
.
Calendar
Bookmarks
Physical
Notebooks
Physical
Post-it
Notes
Notetaking
Applications
Freeform
text
files
Locations
Computer
Desktop
Physical
Desktop
Miscellaneous
Folder
Office
wall
and
whiteboard
Reminders
and
To-dos
How-to
guides
URLs
of
interest
or
quotes
from
web
sites
Contact
information
Notes
Short
pieces
of
data
e.g.,
phone
numbers,
passwords,
serial
numbers,
thank-you
note
lists
Types
Our
solution
was
to
fashion
a
methodology
by
which
we
would
look
for
information
scraps
along
all
three
axes:
tool,
location,
and
type.
By
exploring
along
each
axis
with
participants,
we
would
be
able
to
zero
in
on,
or
triangulate,
the
location
of
an
appropriate
artifact.
We
first
examined
their
personal
information
by
tool
which
yielded
the
greatest
number
of
information
scraps
,
followed
by
location,
and
finally
by
type.
To
our
knowledge,
this
methodology
is
novel.
We
began
by
running
a
pilot
study
5
participants
to
generate
a
broad
set
of
tools,
locations,
and
types
that
we
used
as
a
seed
list
for
our
final
study.
Participants
were
free
to
generalize
to
other
tools
as
they
desired.
We
recorded
information
scraps
that
were
digital,
physical,
and
mobile.
Table
I
lists
a
sample
of
the
script
we
followed.
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
24,
Publication
date:
September
2008.
Information
Scraps
24:13
As
the
participant
or
interviewers
pointed
out
information
scraps
in
each
tool,
location
or
information
type,
the
interviewers
performed
an
artifact
analysis
and
a
semi-structured
interview
focused
on
techniques
surrounding
the
items
of
interest.
One
of
the
interviewers,
performing
the
artifact
analysis,
probed
for
as
many
specific
instances
of
the
class
of
information
scrap
as
feasible
given
the
time
constraints.
For
each
artifact,
the
following
were
recorded:
--
Information
type:
the
information
type
as
described
by
the
participant
e.g.,
to-do,
URL,
shopping
list
.1
--
Tool:
the
tool
used
to
author
and
edit
the
information
scrap.
--
Whether
the
scrap
was
completely
authored
by
the
participant
or
contained
copy
pasted
material.
--
Whether
the
scrap
contained
each
of
the
following
types
of
content:
--
Text:
written
or
typed
text
or
words
--
Photographs
--
Pictorial
Drawings:
representational
illustrations
of
real-world
objects
--
Abstract
Drawings:
non-representational
drawings,
doodles,
symbols,
arrows,
annotations,
graphs
representing
relationships
or
quantitative
data
Participants
were
responsible
for
identifying
items
and
distinguishing
multiple
items
from
each
other.
The
interviewers
guided
the
interview
so
as
to
try
and
get
a
representative
sample
of
information
scrap
from
a
variety
of
tools,
skewing
for
breadth
rather
than
depth,
though
we
did
spend
extra
time
investigating
tools
with
large
numbers
of
scraps.
At
the
conclusion
of
the
study,
we
consolidated
similar
information
type
categories.
To
consolidate,
we
began
with
the
specific
types
recorded
by
one
of
the
interviewers
and
verified
by
the
other.
The
two
researchers
then
acted
as
coder
aggregators,
consolidating
types
as
aggressively
as
possible
without
sacrificing
the
participant
s
original
intent
with
the
scrap.
At
the
conclusion
of
the
artifact
examination
the
interviewers
continued
the
semi-structured
interview,
following
up
on
topics
of
interest.
Interviews
typically
lasted
sixty
minutes.
5.
RESULTS
5.1
What
do
Information
Scraps
Contain?
In
our
artifact
analysis,
we
coded
each
of
the
533
information
scraps
for
its
information
type,
and
then
consolidated
similar
categories.
The
results
can
be
seen
in
Table
II
and
Figure
1.
5.1.1
Common
Information
Scrap
Types.
The
four
most
common
information
types
we
found
in
scraps
were
to-dos
92
instances
,
meeting
notes
1
The
data
type
was
recorded
as
per
the
participant
s
primary
classification
of
the
artifact,
even
if
it
contained
multiple
data
types.
Thus,
even
though
many
to-dos
included
names
of
people,
places
to
be,
times
of
events,
and
so
on,
they
were
nonetheless
coded
as
to-do
if
the
participant
viewed
the
overall
artifacts
as
to-dos.
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
24,
Publication
date:
September
2008.
24:14
M.
Bernstein
et
al.
Table
II.
The
number
of
occurrences
of
each
information
type
observed
in
our
study.
Distinct
types
on
the
right
are
separated
by
commas;
e.g.,
8--Progress
Report,
Receipts
Confirmations
represents
two
types
each
occurring
eight
times.
Occurrences
92
44
38
25
16
14
13
12
9
8
7
6
5
4
Types
with
the
Given
Number
of
Occurrences
To-Do
Meeting
Notes
Name
Contact
Information
How-Tos
Work-In-Progress
File
Path
Directory
Path
URL
Desired
Items
Login
Password
Brainstorm,
Calendar
Event
Details,
Event
Notes
Progress
Report,
Receipts
Confirmations
Computer
Repair
Status,
Conversation
Artifact,
Correspondence
Chat
,
Financial
Data,
Products
Of
interest,
Reminder
Calendar
or
Event
List,
Correspondence,
Debugging
Notes
Archived
E-Mail,
Computer
Address,
Ideas,
Pre-Emptive
Calendar
Scheduling
Account
Number,
Airplane
Flight
Information,
Annotations,
Design
Layout,
People
of
Interest,
Plans
Goals,
Timeline,
Airplane
Flight
Information,
Archived
Document
Agenda,
Configuration
Settings,
Jobs
Classifieds,
Math
Scratchwork,
Project
Notes,
Shipping
Information,
Template
E-Mail
Response,
To
Read,
Whiteboard
Capture
Mixed
type
,
Academic
Record,
Bug
List,
Change
log,
Company
Organization
Chart,
Debugging
Program
Output,
Favorite
Quote,
File
Backup,
Frequent
Flier
Information,
Hotel
Information,
Performance
Tracking,
Room
Setup
Diagram,
Tax
Information,
Template
Text,
Time
Log,
To
Share
No
Memory
of
Meaning
,
Announcement,
Application
Instructions,
Architecture,
Archived
Document,
ASCII
Art,
Baseball
Schedule,
Blue
Chip
Stocks,
Book
Margin
Comments,
Book
Outline,
Calculation
Chart,
Car
Supply
Shops,
Citation,
Class
Assignments,
Client
ID
Number,
Concert
Tickets,
Correspondence
E-Mail
,
Credit
Card
Information,
Deadlines,
Decorative
Drawing,
Definition,
Demographic
Breakdown,
Documentation,
E-Mail
Lists
of
Interest,
Employee
Desires
Goals,
Event
Planning,
Expense
Report,
Fantasy
Football
Lineup,
File
Transfer,
Flow
Diagram,
Funding
Options,
Gift
Certificate,
Guitar
Chords,
Gym
To
Join,
Insurance
Claim,
Kayaking
Resources,
Library
Number,
Moving
Plans,
Network
Diagram,
Newsletter
Outline,
Notes
From
Old
Job,
Parking
Location,
Part
Number,
Patent
Information,
Phone
Payment
Statistics,
Picture
of
Car,
Picture
of
Poster,
Pictures
of
Team
Members,
Planned
Trip
Map
,
Presentation,
Price
List,
Project
Overview,
Public
Notice,
Puzzle
Answers,
README
File,
Rebate
UPCs,
Recipe,
Resume,
Room
Location,
Salary
Calculation,
Serial
Number,
Sign
Out
Sheet,
Song
Lyrics,
Talks
Given,
Travel
Agent,
Word
To
Spell-check
3
2
1
44
instances
,
name
and
contact
information
38
instances
,
and
how-to
guides
25
instances
:
--
To-dos:
Information
scraps
containing
lists
of
items
participants
wanted
to
accomplish.
Action
items,
traditional
to-do
lists,
and
other
information
interpreted
as
a
to-do
fell
into
this
category.
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
24,
Publication
date:
September
2008.
Information
Scraps
24:15
Fig.
1.
This
visualization
of
Table
II
demonstrates
the
prevalence
of
uncommon
types
in
information
scraps.
The
ordering
is
identical
to
that
in
Table
II.
--
Meeting
notes:
Notes
taken
down
while
the
participant
was
in
a
meeting
or
discussion.
These
ranged
from
notes
taken
at
formal
meetings
to
hallway
conversations.
--
Name
and
contact
information:
Typical
contact
information
such
as
name,
address,
phone
number
or
email
address.
--How-to
guides:
Notes
containing
instructions
on
how
to
perform
certain
tasks,
kept
for
future
reference.
Examples
included
UNIX
shell
command
sequences
incantations
,
login
procedures
for
remote
servers,
instructions
on
ordering
food
for
meetings
seminars,
filing
for
reimbursements,
and
common
office
tasks
such
as
shipping,
calling,
or
faxing
internationally.
We
note
that
two
of
these
top
four
categories
to-dos
and
contact
information
are
easily
managed
by
many
PIM
applications.
Section
5.5.1,
discussing
the
capture
stage,
will
give
possible
reasons
why
this
information
still
may
end
up
in
scrap
form.
5.1.2
Diversity
of
Data
Types
and
the
Long
Tail.
Another
compelling
view
of
information
scrap
forms
appears
when
one
focuses
on
the
least
frequently
occurring
items.
Figure
1
illustrates
the
frequency
of
each
information
type
we
found
within
all
participants
scraps,
ordered
from
most
to
least
frequent.
Immediately
noticeable
is
the
fast
drop
in
the
histogram
after
the
most
common
types
described
above,
and
the
large
mass
of
types
with
few
occurrences.
Furthermore,
as
can
be
seen
in
Table
III,
the
least
frequently
occurring
types
the
tail
of
the
distribution
comprised
a
significant
percentage
of
the
information
stored
in
all
the
scraps;
in
particular,
forms
that
occurred
only
once
comprised
13
of
all
scraps;
twice
or
less,
18
of
all
scraps.
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
24,
Publication
date:
September
2008.
24:16
M.
Bernstein
et
al.
Table
III.
Information
types
that
appeared
only
once
make
up
approximately
one
eighth
of
all
information
scraps.
The
50
threshold
is
crossed
at
nine
occurrences
out
of
533
recorded
information
scraps.
Upper
Bound
on
Number
of
Occurrences
of
an
Information
Type
1
2
3
4
5
6
7
8
9
of
all
Information
Scraps
12.8
18.4
24.0
29.3
33.0
36.4
44.3
47.3
52.4
From
our
limited
sample,
the
distribution
of
information
types
appears
to
follow
a
discrete
power
law
probability
distribution,
containing
a
long
tail
of
unpopular
types.
This
correspondence
between
naturally
occurring
events
and
power
law
distributions
is
referred
to
as
Zipf
s
law,
and
has
been
noted
in
several
other
domains
such
as
the
Long
Tail
of
Internet
sales
Anderson
2006
,
the
Pareto
Principle
80
20
rule
in
business
economics,
and
Bradford
s
Law
in
scientific
citation
patterns.
In
common
with
Anderson
s
conception
of
power
laws
in
internet
sales,
we
found
that
the
scraps
containing
rarer
forms
in
our
study
cumulatively
rivaled
the
number
of
occurrences
of
commonly
occurring
forms.
We
uncovered
a
large
number
of
rarely
occurring
information
types,
including
book
wish
lists,
application
serial
numbers,
expense
reports,
resumes,
guitar
chords,
and
information
about
kayaking.
All
of
these
types
could
benefit
from
an
application
tailored
to
their
particular
intended
uses
e.g.,
comparing
kayaking
in
Cambridge
and
Palo
Alto,
sorting
resumes
by
years
of
experience,
or
finding
the
most
similar
expense
reports
;
however,
unlike
information
types
like
to-dos,
for
the
majority
of
these
rarely
occurring
types,
such
applications
are
either
unavailable
or
unpopular.
In
addition
to
looking
at
the
distribution
of
types
for
all
scraps
as
a
single
group,
we
also
looked
at
each
individual
s
scrap
distributions
to
gain
a
sense
for
comparison.
In
doing
so,
we
observe
a
similar
distribution
for
individuals
information
scraps,
which
implies
that
individuals
also
use
information
scraps
to
keep
a
large
number
of
infrequently
occurring
information
types;
see
Figure
2
a
and
Figure
2
b
.
Figure
2
b
provides
an
interesting
link
between
the
global
i.e.,
interparticipant
and
individual
scrap
type
distributions:
ENG3
s
most
popular
item
is
notes
on
the
repair
status
of
the
computers
he
managed;
however,
he
was
the
only
participant
to
record
such
information.
Thus,
even
though
computer
status
notes
are
in
the
head
of
ENG3
s
individual
distribution,
they
fall
to
the
tail
of
the
global
distribution.
We
observed
similar
patterns
in
several
other
participants:
ADMN1
maintained
a
set
of
artifacts
she
needed
to
share
with
her
superior,
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
24,
Publication
date:
September
2008.
Information
Scraps
24:17
Fig.
2.
a
FIN1
s
information
form
distribution,
which
also
follows
a
long
tail.
Here
the
histogram
closely
mirrors
the
accumulated
distribution
across
all
participants.
b
ENG3
s
information
form
distribution,
evidencing
a
large
number
7
of
computer
status
note
scraps.
Though
these
notes
are
in
the
head
of
his
distribution,
he
was
the
only
participant
to
collect
such
data,
so
when
accumulated
the
computer
status
notes
fall
into
the
long
tail.
ENG2
kept
an
extensive
set
of
personal
progress
reports,
and
MAN6
had
a
sizable
folder
of
documents
to
read
in
his
free
time.
Therefore
despite
overall
distributional
similarities,
individual
differences
are
clearly
visible
among
the
frequencies
of
types
kept
by
different
individuals.
5.2
Scrap
Encoding,
Composition
and
Layout
The
predominant
method
of
encoding
information
in
scraps
was
text,
typed
or
handwritten.
Information
scraps
occasionally
contained
abstract
drawings
and
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
24,
Publication
date:
September
2008.
24:18
M.
Bernstein
et
al.
Fig.
3.
All
three
of
these
information
scraps
contain
abstract
drawings,
including
arrows,
boxes,
and
so
on.
annotations
Figure
3
.
Such
drawings
included
arrows,
graphs
or
timelines,
stars,
organizational
lines
and
boxes,
and
markings
indicating
emphasis.
We
coded
our
data
to
record
the
number
of
information
scraps
that
contained
text,
abstract
drawings,
pictorial
drawings,
or
photographs.
We
found
96
of
our
information
scraps
to
contain
some
sort
of
text
95
of
the
digital
scraps,
96
of
the
physical
scraps
,
5
to
contain
abstract
drawings
1
digital,
10
physical
,
2
to
contain
pictorial
drawings
no
digital
examples,
4
physical
,
and
2
to
contain
actual
pictures
4
digital,
1
physical
.
In
two-proportion
z-tests,
the
differences
between
digital
and
physical
media
for
abstract
drawings,
pictorial
drawings,
and
actual
pictures
are
significant
p
0.01
;
the
difference
for
text
was
not
significant.
A
small
number
of
information
scraps
contained
other
kinds
of
media,
digital
or
physical
attachments
such
as
laundry
receipts.
Due
to
the
varied
nature
of
the
types
of
information
scraps,
there
was
a
corresponding
variation
in
scraps
elements,
such
as
phone
numbers,
email
addresses,
or
URLs.
It
was
also
common
to
see
several
elements
intermixed
in
a
single
information
scrap
or
several
unlike
information
scraps
together
in
one
location--Figure
5b
contains
a
URL,
a
PIN
number,
two
UNIX
commands,
and
a
phone
number
sequence.
The
information
types
were
rarely
labeled,
which
occasionally
made
it
difficult
for
the
interviewers
to
understand
the
content
of
some
notes
Figure
4
.
Many
examples
of
artifacts
collected
contained
both
incomplete
and
vague
information.
Furthermore,
the
need
to
capture
information
in
such
an
incomplete
or
vague
form
occasionally
impacted
how
and
where
something
was
written.
For
example,
MAN3
explained
the
calendar
event
in
his
to-do
manager:
I
don
t
know
exactly
when
my
visitor
will
come
today.
.
.If
we
ll
agree
on
the
details
later,
I
prefer
to
use
a
to-do.
Information
scraps
could
also
capture
data
with
more
fields
than
applications
knew
how
to
handle;
for
example,
MAN
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
24,
Publication
date:
September
2008.
Information
Scraps
24:19
Fig.
4.
Information
scraps
containing
unusual
data.
Counterclockwise
from
upper
left:
guitar
chords,
an
unknown
string
of
numbers,
and
answers
to
an
online
riddle.
maintained
his
own
contact
list
where
he
could
record
previous
deals
and
other
personal
notes
on
each
person
he
worked
with.
While
most
information
scraps
were
very
short
a
few
words
or
lines
long
,
we
observed
several
instances
of
scraps
of
approximately
a
handwritten
page
in
length,
particularly
meeting
notes.
This
result
indicates
more
variation
in
length
than
we
were
originally
expecting
to
see.
Several
participants
who
kept
free
text
files
on
their
computer
utilized
the
ability
to
mix
types
or
lay
out
thoughts
as
they
desired--even
creating
ASCII
art
in
the
case
of
ADMN6.
Paper
and
physical
tools
were
particularly
preferred
for
their
encoding
flexibility,
allowing
participants
freedom
over
visual
structure
and
sketching
10
of
physical
scraps
involved
some
sort
of
drawing
annotation
.
5.3
Use
of
Language
in
Scrap
Text
We
found
that
text
written
in
information
scraps
used
extremely
terse
language;
many
scraps
consisted
exclusively
of
key
words,
such
as
lists
of
names
of
people,
places
or
objects,
and
raw
bits
of
data,
such
as
phone
numbers,
addresses,
passwords,
and
other
strings.
Figure
5
gives
examples
of
text
used
in
scraps.
Information
scraps
used
as
temporary
storage
locations
in
particular
exhibited
short
language,
listing
single
words
or
pairs
of
noun-object
or
noun-data
value,
often
omitting
the
verb
or
relevant
predicate,
as
well
as
articles
and
particles.
For
notetaking--in
a
meeting,
class
or
brainstorming--phrase
structure
was
more
common.
We
also
noticed
a
tendency
to
omit
the
subject
title
or
description
of
what
the
data
actually
represented.
Several
participants,
when
sending
emails
to
themselves,
intentionally
left
the
subject
field
blank
or
wrote
something
general
such
as
note
to
self.
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
24,
Publication
date:
September
2008.
24:20
M.
Bernstein
et
al.
Fig.
5.
Several
of
the
information
scraps
we
noted,
focusing
on
typical
examples
of
minimal
use
of
language
in
scraps:
a
an
envelope
with
several
scraps:
guitar
chords,
stock
ticker
symbols,
e-mail
addresses,
and
an
unknown
number,
b
a
web
site
address,
password
and
helpful
SSH
commands
in
a
rolodex,
c
use
of
the
Outlook
notes
facility
to
maintain
links
of
interest
and
the
outline
of
a
blog
entry,
d
CFP
Meeting,
ERCIM,
didn
t
use
database
s,
up
in
incomplete
,
meeting
notes,
e
a
brainstorm
on
a
programming
decision,
f
email
to
Leone
w.r.t.
600.000
euros,
a
reminder,
g
several
post-its
on
the
laptop
palm
rest,
reading
XIA
HUA,
ANDREW
talk,
and
Gopal
s
cell
,
h
a
reminder
written
on
the
participant
s
hand,
a
single
word,
i
text
at
the
top
of
an
e-mail
the
participant
received,
condensing
it
into
a
few
memorable
words,
j
an
annotated,
copy
pasted
chat
transcript
detailing
a
slightly
arcane
UNIX
command.
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
24,
Publication
date:
September
2008.
Information
Scraps
24:21
Fig.
6.
There
are
a
small
number
of
tools
and
locations
used
frequently,
such
as
notebooks,
email,
and
post-its,
and
a
large
number
of
locations
used
a
small
number
of
times.
5.4
Tools
and
Locations
We
noted
51
different
tools
in
use
across
our
investigation,
33
digital
and
18
physical.
Figure
6
shows
the
distribution
of
the
number
of
information
scraps
we
located
in
each
tool
or
location.
Again
there
is
a
power
law
pattern,
beginning
with
a
set
of
extremely
popular
tools
and
trailing
off
to
a
large
number
of
less
popular
ones.
Participants
maintained
a
small
set
of
main
tools
for
capturing
information
scraps,
supplemented
with
a
large
number
of
less-used
auxiliary
tools.
Among
digital
tools,
email
was
the
most
often
used
for
recording
information
scraps
74
instances,
26.4
of
digital
scraps
,
followed
by
a
text
editor
47
instances,
16.8
;
e.g.,
TextEdit
or
Notepad
and
word
processor
27
instances,
6.4
;
e.g.,
Microsoft
Word
.
Text
editors
were
preferred
over
word
processors
for
being
less
complete
or
formal,
particularly
in
early
drafts
of
work.
Several
participants
used
text
files
to
keep
separate
collections
of
contact
information:
for
example,
for
business
clients
versus
family
and
friends.
In
addition,
some
participants
used
text
files
to
keep
contacts
because
it
was
easier
to
add
notes
about
particular
contacts,
such
as
the
history
of
business
negotiation
with
a
particular
contact,
or
the
names
of
the
contact
member
s
spouses
and
family
members.
In
the
physical
world,
paper
notebooks
94
instances,
37.2
of
physical
scraps
and
Post-its
60
instances,
23.7
were
the
most
popular
choices.
Participants
reported
that
paper
notebooks
were
often
an
appropriate
choice
because
they
were
portable
and
more
socially
acceptable
in
face-to-face
meeting
settings.
Thus,
paper
notebooks
were
the
most
popular
tool
for
meeting
notekeeping,
and
physical
meeting
notes
were
three
times
as
common
as
digital
meeting
notes.
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
24,
Publication
date:
September
2008.
24:22
M.
Bernstein
et
al.
Fig.
7.
Examining
the
number
of
participants
at
each
level
of
digital
data,
we
see
two
groups:
one
centered
around
half
digital,
half
physical,
and
the
other
almost
completely
digital.
5.4.1
Physical
Digital
Divide.
Overall,
there
was
an
approximate
parity
in
the
number
of
physical
253,
47.47
and
digital
280,
52.53
information
scraps
we
gathered.
However,
this
statistic
is
slightly
misleading,
as
participants
adopted
widely
different
strategies.
Examining
the
relationship
between
participant
and
percentage
of
scraps
kept
in
digital
vs.
physical
form,
a
chisquared
test
rejects
the
null
hypothesis
p
0.01
.
Figure
7
indicates
how
this
dependence
might
have
arisen:
there
is
a
bimodal
distribution
with
most
participants
centered
at
the
50
mark
and
a
smaller
group
being
almost
entirely
digital.
These
digital
participants
tended
to
be
technophiles
or
mobile
workers,
including
two
managers,
an
engineer,
an
administrative
assistant
and
a
research
scientist.
The
existence
of
almost-digital
practice
is
a
somewhat
surprising
conclusion
given
previous
research
s
claims
that
paper
is
still
an
overriding
favorite
for
many
information
scraps
Campbell
and
Maglio
2003,
Lin
et
al.
2004,
Sellen
and
Harper
2003
.
5.4.2
Mobility.
When
the
scenario
called
for
information
workers
to
go
mobile,
participants
often
generated
information
scraps
to
carry
important
data
around
or
to
capture
information
as
events
occurred.
A
small
number
of
information
scraps
22
instances
were
in
mobile
digital
form:
primarily
smartphones,
but
also
PDAs,
SMS
messages
and
camera
phone
pictures.
ENG7
and
MAN7
in
particular
used
smartphones
heavily
for
capture,
and
relied
on
synchronizing
functionality
with
their
desktops.
Though
we
were
unable
to
note
which
physical
information
scraps
were
used
in
mobile
scenarios
and
which
were
not,
our
interviews
suggested
that
paper
information
scraps
were
particularly
useful
when
mobile.
UI2
is
an
illustrative
example
of
a
digital
smartphone
user:
she
described
how
she
would
reference
a
note
file
on
her
smartphone
with
relevant
phone
numbers,
and
was
likely
to
send
herself
a
voice
mail
or
add
a
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
24,
Publication
date:
September
2008.
Information
Scraps
24:23
Fig.
8.
ENG3
wrote
notes
on
masking
tape,
then
affixed
the
information
directly
to
the
computers
of
interest.
smartphone
note
file
if
the
situation
required.
In
addition
to
mobile
scenarios,
social
constraints
came
into
play:
when
laptops
were
not
socially
appropriate
at
meetings,
paper
notebooks
were
used
instead.
5.4.3
Tool
Adaptation.
Many
information
scraps
revealed
ways
in
which
participants
adapted
tools
to
better
serve
their
purposes.
Post-Its
provided
the
best
examples
of
this
behavior.
We
observed
Post-its
adapted
to
deliver
contextually-relevant
information
by
being
stuck
in
the
places
or
to
the
physical
objects
to
which
they
referred:
in
Figure
8,
ENG3
affixed
tape
labels
directly
to
the
computers
he
was
attempting
to
annotate.
Post-its
were
also
used
as
bookmarks
in
aid
of
refinding
and
placed
on
the
back
of
a
cell
phone
to
act
as
an
extension
of
the
device
s
note-taking
facilities.
Interviews
revealed
that
Postits
were
well-loved
information
tools;
the
wide
variety
of
creative
adaptation
behaviors
support
this
perception.
We
also
observed
adaptation
and
reappropriation
in
digital
tools.
For
example,
ENG4
used
a
popular
Web-based
software
bug
tracking
tool
as
his
personal
to-do
list
because
it
afforded
an
organizational
principle
that
he
liked
individual
issues
as
commitments
with
deadlines
,
he
could
access
it
from
anywhere,
and
because
he
could
easily
update
his
list
of
commitments
by
emailing
the
system.
Email
was
also
often
reappropriated,
most
commonly
by
participants
seeking
to
archive
information
by
sending
themselves
messages.
Annotation
and
revision
were
also
quite
common--documents
were
annotated
with
comments
on
what
the
recipient
should
do,
calendar
events
contained
explanatory
notes,
and
last-minute
amendments
were
appended
to
agendas.
For
example,
ADMN5
printed
out
the
day
s
schedule
for
her
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
24,
Publication
date:
September
2008.
24:24
M.
Bernstein
et
al.
supervisor
produced
using
a
calendaring
tool
and
marked
it
up
with
physical
notes.
5.5
The
Information
Scrap
Lifecycle
In
this
section
we
build
on
Lin
et
al.
s
micronote
life
cycle
Lin
et
al.
2004
,
revisiting
our
results
using
the
lens
of
the
information
scrap
life
cycle.
Here,
we
discuss
Capture,
Transfer,
Organization,
and
Reuse:
--
Capture:
the
process
of
translating
a
thought
into
a
physical
or
digital
information
scrap.
--
Transfer:
optionally
translating
an
information
scrap
from
one
form
into
another,
either
to
put
it
in
a
more
permanent
form
or
enable
mobility.
--
Organization:
the
addition
of
structures
and
metadata
to
aid
refinding
of
scraps
later.
--
Reuse--reference,
retrieval
and
recall:
the
need
to
refind
scraps
reference
,
the
process
of
refinding
those
scraps
retrieval
,
and
memory
for
scrap
contents
recall
.
5.5.1
Capture.
We
observed
three
major
sources
of
information
scraps:
directly
authored
material,
automatically
archived
material,
and
copy
pasted
material.
Directly
authored
material
the
most
common
was
intentionally
written
in
an
effort
to
record
information.
Indirectly
authored
material
consisted
of
scraps
that
were
created
as
the
result
of
some
external
action
not
initiated
by
the
participant,
such
as
receiving
an
email
or
paper
correspondence
from
someone
else,
and
then
explicitly
kept
by
the
participant.
Thus,
emails
received
and
then
saved
in
a
Miscellaneous
directory
constituted
indirectly
authored
material.
In
our
interviews,
copy
pasted
information
included
examples
such
as
photocopies
of
a
credit
card
in
a
notebook,
pieces
of
an
online
FAQ
pasted
into
a
text
file,
and
Internet
chat
transcripts
manually
saved.
We
coded
our
data
to
examine
how
often
participants
included
any
material
they
did
not
directly
author
in
their
information
scraps.
We
found
that
113
of
the
533
scraps
21.20
overall
contained
portions
copied
from
other
sources,
breaking
down
as
28.93
of
the
digital
scraps
and
14.48
of
the
physical
scraps.
In
a
two-proportion
z-test,
this
difference
between
physical
and
digital
was
significant
p
0.01
.
Among
the
directly
authored
material,
the
most
commonly
cited
situation
prompting
information
scrap
creation
was
the
need
to
write
something
down
quickly
before
it
was
forgotten
Hayes
et
al.
2003
.
To
MAN6,
writing
information
down
quickly
was
essential
to
keeping:
Mind
like
water,
he
explained,
is
a
critical
component
of
the
Getting
Things
Done
approach
to
workflow
organization
Allen
2001
.
Others
also
reported
how
offloading
information
from
the
mind
and
into
an
information
scrap
freed
them
to
focus
on
their
primary
task,
such
as
holding
a
conversation,
paying
attention
to
a
meeting,
or
even
driving
the
car
as
with
MAN7
.
Capture
speed
was
one
of
the
most
important
determiners
of
the
tool
participants
chose
to
use.
Even
seemingly
minor
difficulties
or
annoyances
with
tools
could
deter
use
of
a
tool.
If
it
takes
three
clicks
to
get
it
down,
it
s
easier
to
email,
FIN1
explained.
MAN3
would
write
notes
on
Post-its
and
stick
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
24,
Publication
date:
September
2008.
Information
Scraps
24:25
them
to
his
cellular
phone
to
transfer
into
Outlook
later
rather
than
enter
the
data
directly
into
his
smartphone,
even
though
the
phone
supported
note
synchronization.
When
asked
why
not
enter
the
note
digitally
in
the
first
pass,
he
responded,
Starting
in
Outlook
forces
me
to
make
a
type
assignment,
assign
a
category,
set
a
deadline,
and
more;
that
takes
too
much
work!
Similarly,
paper
notebooks
were
often
chosen
instead
of
laptops
because
they
required
no
time
to
boot
up.
The
effect
is
similar
to
the
one
described
with
mobile
applications
by
Oulasvirta
and
Sumari
2007
and
with
organic
digital
memory
tradeoffs
by
Kalnikait
and
Whittaker
2007
.
e
Even
when
data
was
implicitly
structured,
such
as
with
calendar
events,
participants
chose
the
faster,
structureless
route
of
recording
a
scrap:
for
example,
plain
text
such
as
mtg
5pm
in
cafe.
In
interviews,
participants
explained
that
entering
the
data
into
a
structured
form
or
application
could
often
double
or
triple
the
time
it
would
take
to
simply
type
the
information.
Thus,
there
was
a
tension
between
the
desire
to
capture
the
information
quickly
and
the
desire
for
rich
representation
and
structure,
often
later
achieved
via
the
transfer
process.
5.5.2
Transfer.
Transfer,
the
process
of
moving
an
information
scrap
from
one
medium
to
another
after
it
has
been
captured,
only
occurred
for
a
small
proportion
of
the
information
scraps
we
observed.
Participants
explained
that
the
scraps
that
were
transferred
often
held
some
particular
importance.
In
particular,
we
discovered
three
major
reasons
for
initiating
transfer.
The
first
was
to
transform
and
re-interpret
the
information
to
fill
in
incomplete
details,
making
the
notes
appropriate
for
consumption
by
others
or
for
permanent
archiving.
For
example,
MAN4
religiously
transferred
all
of
his
handwritten
meeting
notes
into
emails
to
fill
in
the
gaps
and
make
the
notes
sixty-day
proof
ensuring
they
would
be
understandable
sixty
days
later
.
Second,
transfer
occurred
when
information
was
ready
to
migrate
from
a
work-in-progress
state
to
a
more
complete
representation
and
needed
a
tool
that
offered
additional
functionality.
Third
was
mobility:
scraps
were
sometimes
transferred
onto
other
media
to
carry
to
another
room,
or
sent
in
e-mail
so
that
it
would
be
retrievable
from
home.
5.5.3
Organization.
Among
information
scraps
that
were
archived,
techniques
varied;
some
consolidated
information
scraps
of
similar
types
purposes,
while
others
situated
scraps
with
others
that
were
created
at
the
same
time
and
thus
created
a
chronological
ordering.
ENG1
in
particular
maintained
three
text
files
corresponding
to
three
different
types
of
how-tos
accumulated
over
several
years.
Several
participants
expressed
difficulty
filing
information
scraps
accurately.
This
effect
was
especially
powerful
for
single,
unique
thoughts:
as
ENG3
jokingly
complained,
where
would
you
put
the
last
two
octets
of
a
MAC
address?
REC
concurred:
It
s
too
much
work
to
decide
which
section
it
should
go
in--
because
sometimes
things
don
t
fit
in
just
one,
or
fit
in
multiple
places.
It
s
hard
to
decide
what
to
do.
When
such
difficulties
occurred,
participants
reported
dedicating
areas
to
unorganized
information
scrap
collection,
ranging
from
a
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
24,
Publication
date:
September
2008.
24:26
M.
Bernstein
et
al.
special
email
folder,
Miscellaneous
file
folders,
misc.txt
text
files,
or
a
catch-all
notebook.
We
also
noted
that
participants
often
copied
information
into
multiple
places
to
circumvent
application
limitations
or
fragmentation,
resulting
in
replication
across
digital
tools.
For
example,
several
participants
copied
contents
from
emails,
wikis,
bug
tracking
tools,
and
groupware
into
their
to-do
list
management
tool
or
calendar.
Participants
reported
this
behavior
was
an
effective
coping
mechanism
for
linking
information
from
one
tool
into
another
which
better
fit
their
workflow.
For
example,
ENG4
pasted
emails
into
job
tickets
and
summarized
them
in
one
line
at
the
top
Figure
5i
,
because
he
wanted
to
keep
all
of
the
information
relevant
to
an
outstanding
ticket
in
one
location.
Similarly,
ADMN5
copied
relevant
email
threads
into
calendar
note
fields
when
reminding
her
superior
of
a
meeting
so
that
the
superior
would
be
able
to
reconstruct
the
context
and
purpose
of
the
meeting.
5.5.4
Reuse:
Reference,
Retrieval,
and
Recall.
Participants
reported
that
few
of
their
scraps
were
actually
referenced
regularly.
One
group
of
information
scraps,
typified
by
the
to-do
list
on
a
Post-it,
was
referenced
actively
until
its
usefulness
was
exhausted,
and
then
was
either
archived
or
thrown
away.
A
second
group,
including
meeting
notes,
was
archived
immediately
without
a
period
of
active
reference.
After
archiving
the
scraps,
participants
reported
not
needing
them
except
for
special
occasions.
Because
our
study
methodology
directly
located
the
information
scraps,
we
did
not
rigorously
examine
refinding
techniques;
however,
participants
often
spontaneously
recalled
the
existence
of
a
particular
scrap
and
we
could
observe
as
they
located
it
for
us.
When
refinding,
participants
used
a
technique
similar
to
the
orienteering
behavior
described
by
O
Day
and
Jeffries
1993
and
Teevan
et
al.
2004
:
direct
navigation
to
a
folder
location
thought
to
be
relatively
close
to
the
desired
information
scrap,
then
small
local
steps
to
explore
the
results
and
their
neighbors.
We
found
that
participants
exhibited
good
memory
for
the
meaning
of
almost
all
of
the
scraps
we
uncovered.
Out
of
the
533
information
scraps
indexed,
only
one
was
ultimately
left
with
the
participant
unable
to
identify
its
meaning.
We
did
not
investigate
memory
for
the
meaning
of
specific
details
in
each
information
scrap,
only
focusing
on
the
gist;
specific
details
would
likely
have
fared
more
poorly,
due
to
human
memory
for
meaning
outperforming
memory
for
details.
5.6
The
Psychopathology
of
Information
Scraps
During
our
interviews,
we
encountered
a
series
of
affective
and
psychological
dimensions
surrounding
participants
perceptions
of
their
own
information
scrap
practice.
There
was
often
perceived
social
pressure
to
be
an
organized
individual;
admitting
to
the
existence
of
information
scraps
ran
directly
counter
to
this
perception.
Similarly
to
as
recounted
by
Boardman
and
Sasse
2004
,
participants
often
began
the
interview
proud
to
demonstrate
their
complex
personal
information
solutions.
However,
when
the
interviewers
began
to
inquire
after
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
24,
Publication
date:
September
2008.
Information
Scraps
24:27
those
pieces
of
the
workspace
that
were
unorganized,
the
same
participants
would
often
become
uncomfortable
and
embarrassed,
much
like
Bellotti
and
Smith
2000
recount.
Our
participants
reported:
--
I
would
like
to
have
time
to
organize
what
I
ve
captured,
but
this
never
happens.
DOC
adds
that
she
wishes
she
were
an
adherent
to
the
Getting
Things
Done
methodology
Allen
2001
.
--
By
Friday,
no
stickies
and
no
papers
on
the
desk,
UI2
preemptively
excused
the
existence
of
information
scraps
on
her
desk.
--
Ideally,
I
wouldn
t
need
this
anymore,
ADMN2
says
of
the
disorganized
notebook
in
which
she
keeps
all
of
her
important
information.
Clearly,
it
s
not
going
anywhere.
Several
participants
apologized
for
the
state
of
their
office
or
computer
desktop.
Exceptions
to
the
embarrassment
trend
were
found
both
in
participants
who
put
extra
time
into
organizing
their
lives,
colloquially
known
as
lifehackers
Trapani
2007
,
and
those
who
simply
embraced
the
mess.
MAN6,
as
a
Getting
Things
Done
devotee
Allen
2001
,
was
quite
proud
to
demonstrate
his
array
of
tools.
Several
participants
expressed
pride
in
keeping
a
tight
reign
over
their
information
scraps.
In
contrast
were
those
who
had
simply
accepted
that
their
lives
would
be
messy;
ENG1
repeated
to
the
researchers
what
had
become
an
affirmation
of
her
love
for
a
messy
notebook:
It
s
OK
to
have
a
notebook!
Often
participants
were
forced
into
a
cognitive
dissonance
between
their
perceptions
of
themselves
as
organized
individuals
and
the
messy
reality
of
their
lives
at
the
time
of
the
interview.
UI2
described
her
regimen
of
always
transferring
all
her
Post-it
notes
into
Outlook
tasks,
but
when
we
noted
that
there
were
several
such
notes
that
had
remained
untransferred
for
some
time,
the
response
was
defensive:
I
ve
been
too
busy
lately.
Believing
oneself
to
be
an
expert
refinder
of
information
scraps
also
seemed
de
rigueur.
When
asked
about
problems
participants
might
have
refinding
information
scraps
later,
responses
were
curt:
--
I
just
remember.
ADMN3
--
Generally,
I
remember
where
things
are.
RES
--
I
remember
things.
MAN5
In
contrast
to
these
reports,
participants
usually
spent
considerable
time
while
we
observed
them
trying
to
refind
scraps
they
wanted
to
share
with
us.
Many
participants
thus
overestimated
their
memory
for
scrap
locations.
6.
ANALYSIS
6.1
Common
Information
Scrap
Roles
We
have
consolidated
a
list
of
common
information
scrap
roles
Figure
9
:
Temporary
Storage.
Information
scraps
small,
discardable
presence
enabled
their
common
use
as
temporary
storage
or
exosomatic
short-term
memory.
ADMN2
kept
Post-it
notes
on
her
laptop
palm
rest
for
just
this
purpose,
recording
visitors
names
and
contact
information
later
to
be
disposed
of.
Mobility
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
24,
Publication
date:
September
2008.
24:28
M.
Bernstein
et
al.
Fig.
9.
The
five
main
roles
information
scraps
played
were
archiving,
temporary
storage,
workin-progress,
reminding,
and
a
place
to
put
information
that
wouldn
t
fit
elsewhere.
was
often
coincident
with
temporary
storage,
for
example
directions
written
on
a
Post-it
prior
to
a
drive.
Temporary
storage
information
scraps
were
selfregulating
in
number,
as
they
tended
to
be
thrown
away
or
to
disappear
in
piles
quickly
after
creation.
Participants
often
expected
this
graceful
degradation
from
temporary
storage
scraps.
Cognitive
Support.
Our
participants
shared
with
us
many
incomplete
works
in
progress
such
as
half-written
emails,
ideas
for
business
plans,
brainstorms,
and
interface
designs--scraps
used
to
aid
the
process
of
thought.
Before
I
put
anything
in
the
computer,
I
like
to
put
it
on
the
whiteboard
first,
ADMN4
explained
of
her
newsletter
layout
design
process.
Information
scraps
served
cognitive
support
roles
because
scrap
creation
tools
supported,
and
even
encouraged,
messy
information
work.
Two
such
support
functions
were
epistemic
action
Kirsh
and
Maglio
1994
and
external
cognition.
Participants
used
information
scraps
in
support
of
epistemic
action
to
manipulate
and
reflect
on
external
manifestations
of
their
ideas,
generating
alternatives,
refinements
and
elaborations.
Information
scraps
also
served
as
external
cognition,
enabling
our
participants
to
offload
difficult
thought
processes
onto
the
scrap.
We
observed
a
wide
variety
of
information
scraps
being
utilized
in
external
cognition
roles--for
example,
ENG2
and
ENG5
s
in-progress
notes
taken
down
while
debugging.
Archiving.
In
contrast
to
temporary
storage
scraps,
which
were
intended
to
have
short
lifetimes,
many
information
scraps
were
intended
to
hold
on
to
important
information
reliably
for
long
periods
of
time.
For
example,
many
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
24,
Publication
date:
September
2008.
Information
Scraps
24:29
participants
used
information
scraps
to
archive
notes
from
meetings--as
well
as
information
they
could
not
rely
on
themselves
to
remember
such
as
passwords.
Several
participants
emphasized
the
importance
of
knowing
that
the
information
had
been
safely
saved.
Reminding.
Preemptive
reminding
was
an
important
element
of
several
types
of
information
scraps
collected,
most
commonly
to-dos
and
calendaring
event
information.
Participants
preferred
simple,
reliable
approaches
to
such
reminding:
they
took
advantage
of
information
scraps
visibility
and
mobility
by
placing
them
in
the
way
of
future
movements
to
create
reminders.
Colored
Postits,
unread
or
unfiled
emails
and
files
left
on
the
desktop
reminded
participants
to
take
action
or
to
return
to
a
piece
of
information
at
a
later
date.
Unusual
Information
Types.
This
role
was
a
catch-all
for
personal
information
that
did
not
quite
fit
into
existing
tools.
Taking
advantage
of
information
scraps
freeform
nature,
participants
corralled
unique
information
types
that
might
have
otherwise
remained
unmanaged.
For
example,
ENG3
created
an
information
scrap
system
to
manage
a
library-style
checkout
for
his
privately
owned
construction
tools,
and
MAN4
maintained
a
complex
document
of
contact
information
annotated
with
private
notes
on
clients.
This
role
was
particularly
prominent
in
situations
where
the
information
did
not
match
existing
tools
schemas,
such
as
calendar
items
with
a
date
but
no
start
time
chosen.
6.2
Organization
and
Fragmentation
We
may
characterize
information
scraps
of
each
type
by
the
amount
of
effort
that
participants
have
invested
in
organizing
them.
We
use
Web
site
passwords
as
illustrative
examples.
--
Low
invested
effort:
scraps
that
are
fragmented,
unique,
or
separate
from
similar
data.
This
includes
information
of
a
type
that
has
not
recurred
often
enough
to
warrant
collection
in
a
specialized
repository,
or
temporary
information
such
as
might
be
encoded
on
a
post-it.
Example:
Web
site
passwords
are
archived
using
whatever
is
handy:
emails,
post-its,
or
text
files.
--
Medium
invested
effort:
information
types
with
many
instances
archived
together,
but
that
remain
unorganized.
Example:
The
user
has
made
sure
that
all
of
his
or
her
passwords
are
archived
somewhere
in
the
e-mail
inbox,
though
they
are
not
tagged
or
filed
in
any
consistent
way.
--
High
invested
effort:
information
types
with
many
instances
archived
and
organized.
Example:
Passwords
are
all
kept
in
the
e-mail
inbox
in
a
special
folder
called
passwords.
Of
the
scraps
we
collected,
a
large
proportion
exhibited
low
effort--once
captured,
they
were
allowed
to
remain
where
placed.
This
placement
was
usually
dictated
by
convenience
of
capture.
In
notebooks
or
text
files,
this
pattern
resulted
in
a
chronological
stream
of
scraps
as
new
scraps
were
simply
added
to
the
beginning
or
end.
For
email,
participants
seemed
to
leave
scrap
emails
e.g.,
emails
to
self
in
their
inbox
rather
than
filing
them
away
in
a
subfolder.
Our
results
support
earlier
observations
regarding
engineers
lack
of
organization
in
their
logbooks
McAlpine
et
al.
2006
.
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
24,
Publication
date:
September
2008.
24:30
M.
Bernstein
et
al.
Fragmentation
arose
from
participants
voluntary
placement
of
scrap
information
in
different
places.
The
primary
reason
participants
cited
for
writing
information
of
the
same
type
at
different
locations
was
convenience
at
time
of
capture.
For
instance,
ADMN2
kept
contact
information
names,
phone
numbers,
addresses
in
her
main
notebook,
a
paper
desk
calendar,
and
a
mini
address
book;
and
reported
that
where
any
piece
of
contact
information
ended
up
was
determined
by
the
location
of
the
closest
notepad.
When
asked
about
retrieval,
she
reported
having
to
rummage
around
when
she
didn
t
remember
where
something
was
placed,
that
this
often
took
time,
and
that
she
recopied
contact
information
between
locations
so
that
it
would
later
be
more
easily
found.
6.3
Constraints
Information
scrap
practice
depends
on
particular
physical,
temporal,
social,
or
structural
conditions
of
a
situation
that
may
necessitate
tool
use.
We
suggest
that
these
conditions
play
a
particular
role
in
information
scrap
generation
that
may
not
be
as
evident
in
tools
that
have
a
stronger
correlation
between
task
and
tool.
One
such
strong
correlation
is
a
personal
finance
task:
we
have
an
application
for
balancing
checkbooks,
and
are
fairly
indifferent
to
the
physical
environment,
time
constraints,
and
social
conditions
while
we
use
it.
There
are
also
certain
agreed
social
conventions
around
these
tasks.
Paying
bills
is
usually
a
primary
task
in
terms
of
one
s
attention--it
would
be
unusual
to
pay
bills
while
meeting
with
a
colleague.
As
such,
with
such
strong
conventions
and
tool
support
around
a
particular
practice,
we
do
not
see
information
scrap
challenges
in
capture,
storage
and
retrieval.
If
we
were
to
instead
jot
down
thoughts
while
meeting
with
a
colleague
about
a
paper,
the
temporal,
physical,
and
social
constraints
would
have
more
of
an
effect
on
our
choice
of
tool.
For
instance,
it
may
be
socially
taboo
to
be
seen
either
using
a
computer
in
this
context,
or
to
take
lengthy
notes
during
the
meeting.
Either
condition
may
predicate
quick
gestures
on
the
back
of
a
note
card.
The
social
conditions
of
an
exchange
may
be
such
that
it
would
break
the
flow
of
the
conversation
to
reach
for
a
more
formal
mechanism
than
a
scrap
of
paper.
Likewise,
when
mobile,
more
formal
mechanisms
for
recording
data
may
both
be
physically
awkward
and
require
too
much
engagement
and
time.
So,
currently,
physical,
temporal,
social
and
structural
factors
have
particular
bearing
on
the
devices
selected
and
the
kind
of
input
generated
with
information
scraps.
6.4
Caveats
in
Our
Findings
The
boundary
between
information
scraps
and
the
rest
of
our
personal
information
remains
fuzzy,
especially
in
the
case
of
high
organizational
effort.
Once
an
amateur
chef
decides
to
collect
all
of
her
favorite
recipes
on
a
blog
and
tag
them
by
cuisine
type,
are
the
individual
recipes
still
information
scraps?
In
one
sense,
no--she
has
devised
her
own
organizational
scheme
for
his
or
her
recipes
and
appropriated
a
generic
tool
to
support
this
scheme.
On
the
other
hand,
a
generic
tool
may
lack
some
of
the
capabilities
that
recipe-specific
tool
might
provide,
thereby
limiting
its
usefulness.
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
24,
Publication
date:
September
2008.
Information
Scraps
24:31
With
regard
to
the
variety
of
information
types
we
catalogued,
one
difficulty
with
analyzing
the
frequency
distribution
of
information
types
in
scraps
lies
in
drawing
distinctions
between
similar
information
types.
It
is
inevitable
to
question
whether
the
categories
we
list
could
have
been
combined
further,
and
thus
the
diversity
lessened
or
erased.
Our
approach
has
been
to
group
categories
as
aggressively
as
possible
without
losing
the
essence
of
the
scrap
s
composition,
attempting
to
find
a
rough
lower
bound
on
the
strength
of
this
long
tail
effect.
A
less
aggressive
grouping
strategy
produced
results
where
25
of
all
scraps
were
unique
types.
Though
individual
pairs
of
categories
might
be
further
merged,
we
believe
that
the
long
tail
effect
is
quite
strong
and
worth
noting.
Reflecting
on
our
methodology,
a
clear
limitation
of
our
study
stemmed
from
our
use
of
interview
and
artifact
analysis
instead
of
live
observation
via
shadowing.
We
were
thus
unable
to
study
how
information
scraps
were
used
and
created
in
situ,
but
instead
only
how
people
reported
they
used
their
scraps,
along
with
evidence
from
their
workspace
and
the
physical
and
digital
artifacts
we
collected.
We
were
unable
to
capture
the
number
of
information
scraps
participants
generated
each
day
or
significant
contextual
information
surrounding
capture
and
retrieval.
We
observed
that
participants
often
exhibited
a
kind
of
confirmation
bias
toward
their
own
organizational
skills
by
mainly
acknowledging
well-organized
work,
so
it
is
further
possible
that
this
also
affected
our
observations;
for
example,
our
participants
may
have
ignored
particularly
embarrassing
examples
of
disorganization.
The
methodology
s
strength
was
that
it
allowed
us
to
observe
a
broad
number
of
information
scraps,
perhaps
more
so
than
would
have
been
possible
in
situ.
We
believe
that
an
ethnographic
shadowing
methodology
would
complement
ours
well;
it
could
objectively
investigate
many
of
the
questions
we
could
not.
Our
triangulation
method
for
locating
information
scraps
was
also
not
a
perfect
lens.
We
found
that
it
was
successful
in
unearthing
a
large
number
of
information
scraps
in
a
variety
of
locations.
On
several
occasions,
only
the
last
of
the
three
dimensions
we
attempted
tool,
location,
and
then
type
successfully
located
a
particular
scrap.
The
triangulation
method
s
strength
lies
in
unearthing
a
wide
variety
of
information;
its
weakness
is
that
the
number
of
information
scraps
found
can
be
too
numerous
to
examine
thoroughly
within
the
allotted
time.
In
the
future,
we
suggest
the
triangulation
methodology
might
be
modified
to
serve
as
a
fast
tour
through
a
participant
s
information
scrap
landscape,
allowing
the
investigators
to
then
choose
a
small
number
of
tools
or
locations
to
focus
on.
7.
IMPLICATIONS
FOR
DESIGN
In
this
section,
we
ask:
what
design
affordances
would
enable
personal
information
tools
to
better
serve
these
important
and
underserved
roles?
How
realistically
can
we
expect
tools,
built
using
current
technology,
to
support
such
affordances?
We
relate
this
discussion
to
our
exploration
of
interface
design
ideas
that
we
built
into
Jourknow
Van
Kleek
et
al.
2007
,
a
prototype
notetaking
tool
in
which
we
sought
to
better
support
information
scrap
activity
Figure
10
.
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
24,
Publication
date:
September
2008.
24:32
M.
Bernstein
et
al.
Table
IV
outlines
the
set
of
affordances
we
have
identified,
including
lightweight
capture,
flexible
content,
flexible
organization,
visibility
and
reminding,
and
mobility
and
availability.
In
Table
IV
we
have
contextualized
each
affordance
in
terms
of
the
activities
and
constraints
that
influence
scrap
generation.
7.1
Lightweight
Capture
As
described
in
Section
5.5.2,
we
found
that
participants
often
generated
information
scraps
in
response
to
a
need
to
capture
data
quickly.
This
need
occurred
most
commonly
while
the
individual
was
performing
some
other
attentionally,
socially,
or
physically
engaging
primary
task.
For
this
reason,
lowering
both
the
actual
and
perceived
cost
of
cognitive,
social
and
physical
effort
may
improve
our
tools.
We
see
the
following
opportunities
for
reducing
effort
required
during
capture:
Avoiding
upfront
decisions
and
postponing
disambiguation.
Since
information
scraps
are
often
captured
at
a
moment
when
time,
attention
and
cognitive
resources
are
scarce,
requiring
individuals
to
make
significant
upfront
decisions
might
incur
sufficient
cost
to
impede
capture.
Such
decisions
may
include
forcing
the
categorization
of
a
new
piece
of
information,
choosing
a
reminder
time,
or
setting
parameters
ultimately
unimportant
to
the
captured
information.
Thus,
tools
might
aim
to
immediately
handle
information
in
whatever
form
provided
to
them
by
the
user
and
postpone
forcing
user
choices
until
a
more
appropriate
time.
Avoiding
task-switching,
cognitive
and
navigational
burdens.
Navigational
and
cognitive
costs
associated
with
launching
or
switching
applications
contribute
significantly
to
the
time
elapsed
between
the
moment
an
individual
forms
an
intention
of
writing
something
down
and
the
moment
they
can
actually
start
doing
so.
Since
perceived
time
and
effort
during
this
critical
interval
have
been
found
to
dictate
which
tool
will
be
chosen
Gray
and
Boehm-Davis
2000;
Gray
and
Fu
2001;
Kalnikait
and
Whittaker
2007
,
we
believe
that
minimize
ing
navigational
effort
will
improve
a
tool
s
capture
rate
and
therefore
overall
usefulness
to
the
user.
Supporting
abbreviated
expression
of
information.
As
described
in
Section
5.3,
we
found
idiosyncrasies
in
participants
language--notes
often
represented
very
little
explicitly
and
instead
served
as
memory
primers.
Our
finding
contrasts
considerably
with
most
PIM
applications
requirements
that
users
complete
forms
with
formal
expressions
for
properties
such
as
who,
when
and
where.
Our
belief
is
that
tools
can
lessen
the
time
and
effort
associated
with
entering
information
by
supporting
incomplete,
informal
capture
methods.
Supporting
diversity.
If
a
tool
is
restrictive
about
the
information
forms
it
will
accept,
individuals
will
inevitably
resort
to
a
coping
strategy--either
imperfectly
fitting
the
information
into
the
tool,
or
fragmenting
information
by
encoding
it
in
another
tool.
Since
coping
strategies
incur
non-zero
costs
to
devise
and
implement,
and
further
lead
to
decreased
effectiveness
of
future
retrieval,
we
believe
it
worthwhile
to
accommodate
whatever
information
the
user
wishes
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
24,
Publication
date:
September
2008.
Information
Scraps
24:33
Table
IV.
The
major
design
affordances
necessary
for
management
of
information
scraps
derive
from
access,
scraps
contents,
and
organization.
Category
Access
Observed
Behaviors
and
Constraints
Derived
Design
Needs
Finding
the
easiest
and
fastest
tool
to
use.
Con-
Lightweight
Capture:
Record
strained
by
effort
required,
limited
time
and
at-
information
with
minimal
eftentional
resources.
fort
or
distraction.
If
it
takes
three
clicks
to
get
it
down,
it
s
easier
to
e-mail.
--FIN2
Adapting
tool
use
to
physical
locations
and
social
situations.
Constrained
by
available
tools
and
social
norms.
At
off-site
meetings,
I
don
t
have
my
infrastructure
there
and
keeping
notes
on
paper
is
less
rude.
--MAN1
Information
scraps
kept
always
in
peripheral
view,
or
in
a
location
to
be
tripped
over
the
next
time
the
information
would
be
relevant.
Constrained
by
tools
ability
to
be
placed
in
the
way.
If
it
s
not
in
my
face,
I
ll
forget
it.
Like
if
it
s
on
the
wiki--I
have
no
idea
it
s
there.
--REC
Scribbling,
sketching,
and
annotation;
inprogress,
vague
and
underspecified
information.
Constrained
by
tools
expressiveness.
Section
7.1
Mobility
and
Availability:
different
capture
methods
may
be
necessary
in
different
situations.
Section
7.5
Visibility
and
Reminding:
information
appears
in
the
right
place,
at
the
right
time.
Section
7.4
Flexible
Content:
record
any
kind
of
data,
at
any
level
of
completeness.
Section
7.2
Flexible
Schema:
information
may
not
fit
existing
molds.
Section
7.2
Content
Drawing
is
the
way
you
really
see
it!
--ADMN4
Coping
strategies
when
information
does
not
fit
other
applications
models,
and
collections
of
unusual
information
types.
There
s
this
problem:
I
wanted
to
assign
dates
to
notes,
but
Outlook
would
only
allow
dates
on
tasks.
--MAN3
Organization
Organizational
strategies
varying
in
degrees
from
completely
disorganized
to
carefully
filed,
and
avoidance
of
cognitively
difficult
filing
decisions.
Flexible
Categories:
Support
for
a
variety
of
organizational
strategies,
as
well
as
for
transforming
unfiled
items
into
more
structured,
organized
forms.
It
s
too
much
work
to
decide
which
section
it
Section
7.3
should
go
in--because
sometimes
things
don
t
fit
in
just
one,
or
fit
in
multiple
places.
It
s
hard
to
decide
what
to
do.
--REC
Information
scraps
attached
to
or
placed
near
re-
Flexible
Linkage:
enable
inlated
items.
formation
to
be
linked
to
and
in
view
with
arbitrary
other
information
I
can
never
remember
which
computer
is
Section
7.3
which.
So
I
grabbed
the
gaffer
s
tape
and
marked
them!
--ENG3
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
24,
Publication
date:
September
2008.
24:34
M.
Bernstein
et
al.
to
express.
We
discuss
further
issues
with
supporting
diverse
information
forms
in
Section
7.2.
Capture
from
other
tools.
Given
that
a
significant
portion
of
the
artifacts
we
examined
originated
from
other
applications
and
devices,
e.g.,
mobile
phone,
emails,
web
pages,
IM
conversations
,
we
may
reduce
the
need
to
create
scraps
by
making
it
easy
to
select
and
pipe
the
relevant
information
from
any
application
into
an
appropriate
place.
This
desire
also
pertains
to
the
need
for
ubiquitous
availability
of
capture
tools,
discussed
in
Section
7.6.
Tablet-based
notetaking
tools
such
as
Microsoft
OneNote
http:
office.
microsoft.com
onenote
have
granted
digital
note-taking
some
of
the
expressive
freedom
of
paper
and
pen.
OneNote
also
allows
users
to
categorize
their
notes
post-hoc,
such
as
by
tagging
to-do
and
contact
items,
thereby
reducing
the
upfront
time
to
capture.
However,
the
OneNote
user
interface
is
large,
consuming
all
available
screen
real
estate,
making
it
difficult
to
have
on
the
side
or
to
switch
to
while
in
the
middle
of
another
task.
Natural
language
expression
interfaces
for
intuitive
expressions
of
PIM
information
are
another
promising
approach
to
reduce
capture
effort,
because
they
incur
little
or
no
cognitive
overhead
for
encoding
information
into
an
appropriate
form
for
capture.
Natural
language
interfaces
allowing
users
to
easily
add
information
such
as
events,
contact
information
and
to-do
reminders
to
their
calendars
have
become
increasingly
available,
including
Google
s
Quick
Add
http:
calendar.google.com
,
Presdo
http:
www.presdo.com
,
and
I
Want
Sandy
http:
www.iwantsandy.com
.
These
tools
have
found
some
popularity
and
thus
struck
a
niche;
users
prefer
them
to
form-based
GUI
equivalents
in
some
situations.
Another
promising
direction
towards
accelerating
and
reducing
interaction
effort
with
GUI
applications
is
the
use
of
keyboard
accelerators
that
map
repetitive
GUI
actions
to
reflexive
key
combinations.
A
number
of
application
launchers
that
employ
this
technique
have
recently
gained
popularity:
Quicksilver
http
:www.blacktree.com
,
Enso
http:
www.humanized.com
enso
,
and
GNOME
Do
http:
do.davesbd.com
,
for
example,
simplify
simple
tasks
such
as
application
launches
via
through
the
use
of
hotkey
trigger,
combined
with
small,
unobtrusive
pop-up
windows
that
that
provide
feedback
and
support
keystroke
disambiguation.
However,
these
tools
have
focused
primarily
on
application
launching
rather
than
information
capture
and
thus
do
not
yet
address
the
needs
of
information
scrap
capture.
Finally,
with
respect
to
tool
integration,
snippet-keeper
application
Yojimbo
http:
www.barebones.com
products
yojimbo
facilitates
cross-application
content
grabbing
by
letting
the
user
simply
select
the
content
they
want
grabbed
and
pressing
a
hotkey.
Users
can
immediately
tag
their
grabbed
items
or
choose
to
defer
organization;
the
items
are
then
added
to
collections
in
the
person
s
own
tag-based
Yojimbo
repository.
While
general-purpose
drawing
tools
and
word
processors
are
potential
candidates
for
information
scrap
management
because
they
afford
fast,
unconstrained
input
of
text
or
drawings,
they
are
not
ideal
for
several
reasons.
First,
the
design
needs
for
creating
published
documents
and
free
drawings
differ
significantly
from
those
of
scraps,
especially
in
creation,
use
and
semantic
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
24,
Publication
date:
September
2008.
Information
Scraps
24:35
structural
characteristics.
As
discussed
in
Section
5.2,
information
scraps
are
often
implicitly
structured
but
sketchy
and
rough,
whereas
word
processors
and
drawing
tools
are
designed
to
create
published
or
shared
documents
and
illustrations.
These
tools
thus
foreground
design
affordances
that
are
important
to
publishing
but
relatively
useless
for
scrap
creation.
Also,
these
tools
are
not
optimized
for
handling
a
large
number
of
small
data
items:
we
observed
a
number
of
participants
compiling
large
collections
of
scraps
into
a
single
text
or
word
processor
document
in
order
to
circumvent
the
overhead
of
creating
and
managing
many
documents
Section
5.5.1
.
7.2
Flexible
Contents
and
Representation
Our
artifact
analysis
in
Section
5.5.1
revealed
that
information
scraps
were
considerably
more
diverse
and
irregular
than
the
commonly
considered
set
of
PIM
information
types.
Information
often
did
not
match
expected
schemas:
some
properties
were
missing,
and
others
were
introduced.
Participants
also
commonly
combined
information
types
inside
of
a
single
scrap.
These
behaviors
resulted
in
scraps
such
as
a
person
s
first
name
and
phone
number,
a
time
indicating
when
to
contact
the
individual,
and
driving
directions
to
that
person
s
house--but
omitting
the
contact
s
last
family
name.
Furthermore,
as
the
distribution
of
types
discussed
in
Section
5.5.1
suggests,
there
is
a
very
large
potential
set
of
truly
personal
data
types
that
collectively
make
up
a
significant
portion
of
all
information
scraps
but
that
do
not
fit
at
all
in
PIM
applications
today.
The
widely
heterogeneous
fragments
of
information
contrast
significantly
with
the
limited
set
of
fixed
schemas
that
constitute
data
types
in
PIM
applications
today.
While
the
PIM
tools
such
as
Microsoft
Outlook
have
started
to
blur
distinctions
in
PIM
types
e.g.,
to-do
items
with
calendar
entries
,
research
tools
such
as
Haystack
Karger
and
Quan
2004
have
taken
a
more
radical
approach
in
which
general
relational
models
such
as
RDF
http:
www.w3.org
RDF
are
used
as
a
basis
of
representation.
In
such
a
representation,
rather
than
having
disparate
collections
of
data
records
of
particular
fixed
schemas,
instances
are
defined
in
terms
of
how
they
link
their
atomic
data
components
e.g.,
dates,
times
or
names
,
and
linking
is
possible
among
arbitrary
data
components.
Thus,
under
such
a
model
it
becomes
possible
to
create
and
represent
the
often
implicit
meanings
implied
by
the
freeform
scraps
we
found
in
the
study.
A
remaining
challenge
surrounds
developing
a
means
by
which
the
user
can
utilize
this
expressiveness.
Current
interfaces
for
directly
specifying
instances
using
similarly
rich
vocabularies
e.g.,
Protege
at
http:
protege.stanford.edu
carry
high
comprehension
and
execution
overheads.
Another
option
would
be
to
automatically
extract
semantics;
however,
in
practice
this
is
a
challenging
problem
because
the
language
used
in
scraps
Section
5.5.3
is
often
incomplete,
ungrammatical,
and
highly
personalized.
The
difficult
nature
of
the
problem
is
reflected
in
the
fact
that
personal
notes
are
often
ambiguous
and
unintelligible
to
people
other
than
the
author.
Controlled
naturalistic
languages
such
as
those
first
proposed
for
databases
Popescu
et
al.
2003
provide
a
possible
solution.
In
a
simplified
natural
language,
user
are
informed
that
the
system
can
only
interpret
a
restricted
set
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
24,
Publication
date:
September
2008.
24:36
M.
Bernstein
et
al.
of
simple,
common
phrasings
for
information
or
using
some
fixed
syntactic
convention
but
that
they
are
free
to
express
anything
they
wish
to
using
this
language.
This
technique
trades
off
expressiveness
for
perceived
naturalness
of
expression.
7.3
Flexible
Usage
and
Organization
The
tool
adaptations
described
in
Section
5.4.3
are
particularly
interesting
because
they
reveal
individuals
needs:
participants
devised
new
custom
organizational
systems
out
of
existing
tools
to
better
fit
their
needs,
for
example
ENG4
s
re-appropriation
of
a
bug-tracking
tool
as
a
personal
to-do
list
and
MAN3
s
use
of
Post-it
notes
on
the
back
of
his
cell
phone
as
a
capture
solution.
Our
study
also
revealed
several
instances
where
tools
had
to
be
adapted
in
order
to
accommodate
information
that
didn
t
fit,
such
as
when
an
application
failed
to
provide
free-text
annotation
capabilities.
Re-appropriation
and
adaptation
by
the
end-user
requires
tools
to
be
sufficiently
flexible
to
accommodate
novel
use
of
their
affordances
or
data,
and
for
this
flexibility
to
be
simple
enough
for
the
end
user
to
manage.
Many
highfunctionality
information
tools
e.g.,
emacs
at
http:
www.gnu.org
software
emacs
and
Eclipse
at
http:
www.eclipse.org
have
long
allowed
scripting
and
extension
facilities
for
customization.
The
practical
difficulty
associated
with
developing
application-specific
plug-ins
was
such
a
high
barrier
to
entry
that
few
users
attempted
it.
However,
the
past
few
years
have
seen
new
trends
in
open
Web-based
APIs
and
data
services
that
allow
users
to
combine
data
sources
and
user
interfaces
to
suit
their
information
needs.
Tools
such
as
Intel
s
MashMaker
Ennals
and
Garofalakis
2007
and
Dontcheva
et
al.
s
card
metaphor
2007
have
attempted
to
make
this
process
even
more
accessible
to
end
users
through
drag-and-drop
visual
interfaces.
Participants
also
devised
a
rich
set
of
organizational
techniques
and
strategies,
as
summarized
in
Section
5.5.3.
Some
of
these
behaviors
are
already
well
supported
in
digital
information
tools,
while
others
are
poorly
supported.
For
example,
unlike
notes
written
on
physical
media,
digital
PIM
tools
are
good
at
automatically
organizing
collections
of
information
records.
However,
these
tools
are
generally
less
capable
of
adapting
to
novel
organizational
strategies,
so
our
participants
tended
to
use
spreadsheet
software
if
they
needed
to
manage
or
sort
novel
fields.
We
propose
three
potential
solutions:
letting
users
manually
specify
organizational
rules,
specify
organizational
rules
by
example
Halbert
1993
,
or
dynamically
construct
faceted
views
by
combining
of
a
simple
set
of
operators
Huynh
et
al.
2007,
Yee
et
al.
2003
.
Supporting
physical
organizational
strategies
in
the
digital
realm
has
been
more
challenging.
Several
systems
have
demonstrated
methods
of
simulating
aspects
of
the
physical
environment
in
the
digital
realm,
including
the
creation
of
digital
stacks
and
piles
Mander
et
al.
1992,
Robertson
et
al.
1998b
and
visual
wear
and
tear
on
heavily
used
items
Hill
et
al.
1992
.
Others
have
demonstrated
completely
new
ways
of
envisioning
our
organizational
scheme,
such
as
time-based
metaphors
Fertig
et
al.
1996
.
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
24,
Publication
date:
September
2008.
Information
Scraps
24:37
7.4
Visibility
and
Reminding
Nearly
all
participants
employed
a
strategy
of
physically
situating
scraps
in
places
where
they
could
serve
as
references
or
reminders.
As
described
in
Section
5.4.3,
the
desire
to
be
able
to
reference
useful
information
frequently
easily
inspired
participants
to
place
items
in
prominent,
always-visible
locations,
for
example
sticking
Post-Its
to
their
workstation
monitor
or
writing
information
in
the
corner
of
whiteboards.
For
prospective
reminding,
several
participants
reported
strategically
placing
notes
in
locations
where
they
knew
they
would
later
serendipitously
trip
over
them
at
the
right
time.
It
is
difficult
to
support
these
behaviors
in
digital
tools
because
we
cannot
easily
situate
pieces
of
information
in
particular
locations
of
easy
access
or
strategic
significance.
The
problem
of
physically
situating
digital
information
is
still
solved
most
straightforwardly
by
first
converting
the
information
to
physical
form
i.e.,
printing
it
and
then
sticking
the
physical
version
in
the
appropriate
place.
Display
technologies
could
contribute
to
making
it
easier
to
physically
situate
digital
information,
including
low-cost
electronic
displays
such
as
e-ink
http:
www.eink.com
,
multisensory
ambient
displays
Butz
and
Jung
2005,
Dahley
et
al.
1998
,
and
pervasive
displays
such
as
the
EveryWhere
Display
Pinhanez
2001
.
An
alternative
approach
is
to
build
location
sensors
into
portable
displays
and
to
display
information
grounded
at
the
user
s
location;
efforts
in
this
vein
include
the
Remembrance
Agent
Rhodes
and
Crabtree
1998
and
augmented
reality
research
e.g.,
AR
Toolkit
at
http:
www.hitl.washington.edu
artoolkit
.
Still
other
work
Hsieh
et
al.
2006
has
sought
to
reproduce
the
kind
of
passive
reinforcement
that
occurs
when
we
shuffle
through
our
notes
or
flip
through
the
pages
of
our
physical
notebooks
while
looking
for
something
else.
Reminding
individuals
of
what
their
notes
mean
is
another
issue.
As
discussed
in
Section
5.5.2,
the
brief,
incomplete
nature
of
scraps
meant
that
they
were
not
necessarily
future-proof,
although
memory
for
the
general
gist
was
strong.
One
participant,
MAN4,
strongly
expressed
a
desire
for
helping
him
remember
the
significance
of
notes:
The
thing
I
want
the
most
in
a
note-taking
tool
would
be
to
be
able
to
ask
it--Who?
What?
Why?--
Why
is
absolutely
crucial.
If
my
sticky-note
could
answer
this
for
me
I
d
be
golden.
Research
leveraging
associative
memory
the
ability
to
recall
information
when
it
is
contextualized
with
other
relevant
events
may
be
promising
in
this
direction.
In
particular,
the
Stuff
I
ve
Seen
project
Dumais
et
al.
2003
has
focused
on
leveraging
memorable
events
and
past
encounters
with
documents
for
re-finding.
An
extension
to
this
approach
has
been
lifelogging--the
extended,
automatic
capture
and
retrieval
of
personal
experiences
Hodges
2006,
Gemmel
2002
.
In
particular,
work
using
images
taken
at
random
from
a
wearable
camera
to
prime
recall
has
demonstrated
substantial
gains
in
duration
and
fidelity
of
recall
of
routine
workplace
situations
and
events
Sellen
et
al.
2007
.
We
believe
that
similar
recall
effects
would
occur
for
the
meaning
of
notes,
potentially
also
aiding
in
the
refinding
of
lost
notes.
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
24,
Publication
date:
September
2008.
24:38
M.
Bernstein
et
al.
7.5
Mobility
and
Availability
Information
scraps
are
often
closely
tied
to
mobile
scenarios
Section
5.4.2
.
Social
constraints
may
dictate
the
availability
or
appropriateness
of
tools
in
certain
settings
Section
6.3
.
In
response,
many
of
our
participants
resorted
to
carrying
legal
pads,
day
planners
or
pocket
sketchbooks
whenever
they
were
away
from
their
desks.
To
support
capture
in
a
mobile
context,
nearly
every
mobile
smartphone
and
personal
digital
assistant
PDA
provides
some
basic
PIM
and
freeform
notetaking
functionality.
However,
adoption
varies
widely
among
users
Dai
et
al.
2005
.
Our
study
elicited
two
major
impediments
to
the
use
of
such
devices
for
notetaking
and
personal
information
management:
1
a
choice
between
fragmenting
information
across
devices
and
synchronizing
the
mobile
device,
and
2
the
difficulty,
time
and
attention
costs
associated
with
mobile
information
entry.
Fragmentation
can
be
ameliorated
by
synchronization,
and
the
synchronization
problem
is
mainly
an
engineering
one.
One
option
is
for
web-
or
desktop-based
tools
to
offer
mobile-accessible
capture
and
access
interfaces,
as
is
the
case
with
Google
Calendar
and
Microsoft
OneNote
Mobile.
Increasingly
high-bandwidth
wireless
networks
with
unlimited
data
pricing
models
have
started
to
make
feasible
constant,
transparent
over-the-air
synchronization
of
PIM
data
between
mobile
phones
and
desktop
software
Intellisync
http:
www.Intellisync.com
.
Another
choice
to
let
mobile
devices
serve
as
remote
capture
terminals
directly
to
PIM
software
running
on
the
desktop:
for
example,
Jott
http:
www.jott.com
translates
phone
voice
commands
into
calendar
appointments,
to-dos,
etc.
via
an
API
agreement
with
web
services.
With
respect
to
the
barrier
of
data
entry,
significant
progress
towards
improved
text
entry
methods
and
new
mobile
capture
modalities
may
improve
their
suitability
for
use
in
scrap
capture.
Both
on-screen
and
physical
thumbboards
are
improving
text
entry
speeds
by
incorporating
better
tactile
feedback,
predictive
input
and
error
correction
facilities.
Approaches
that
extend
the
reach
of
digital
capture
to
handwriting
on
real
paper
are
also
gaining
traction
in
new
digital
pen
products
http:
www.logitech.com
index.cfm
mice
pointers
digital
pen
devices
408
cl
us,en
systems
that
integrate
handwritten
notes
with
digital
information
are
becoming
visible
in
research
Stifelman
et
al.
2001,
Yeh
et
al.
2006
.
7.6
Jourknow
In
parallel
with
our
ethnographic
study,
we
have
designed
an
information
scrap
management
tool
called
Jourknow
Figure
10
to
explore
the
large
design
space
unearthed
by
the
previous
sections
Van
Kleek
et
al.
2007
.
Instead
of
proposing
to
replace
existing
PIM
applications
as
Yet
Another
Personal
Information
Manager,
our
intention
for
Jourknow
is
to
serve
as
an
exploratory
platform
for
studying
new
input,
retrieval
and
organizational
affordances,
for
informing
the
design
of
the
next-generation
of
PIM
tools,
to
better
support
user
needs.
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
24,
Publication
date:
September
2008.
Information
Scraps
24:39
Fig.
10.
The
user
interface
of
Jourknow,
our
prototype
information
scrap
capture
and
manipulation
tool.
Jourknow
has
attempted
to
address
the
preceding
design
needs
as
follows:
--Lightweight
Capture.
To
facilitate
unstructured
note
capture
into
Jourknow,
Jourknow
provides
cross-application
hotkeys
and
a
heads-up
display
for
grabbing
contents
out
of
other
applications
or
quickly
writing
down
a
piece
of
information
while
in
the
middle
of
something
else.
--Flexible
Contents
and
Representation.
We
intend
Jourknow
to
be
a
single
point
of
capture
for
all
manner
of
personal
information.
As
a
simplifying
maneuver,
we
have
currently
limited
the
system
to
text,
cameraphone
photos
and
voice
memos
as
input
techniques.
To
capture
more
structured
data
such
as
calendar
items
or
bookmarks
using
text,
the
system
incorporates
a
variety
of
different
approaches
to
artificial
natural
languages
which
we
call
pidgin
languages
Van
Kleek
et
al.
2008
designed
to
explore
variations
in
flexibility,
extensibility
and
naturalness.
We
have
also
begun
to
examine
interpretation
techniques
that
try
to
leverage
contextual
information
to
aid
disambiguation
of
short
information
scraps.
--
Flexible
Usage
and
Organization.
Jourknow
s
data
model
is
based
on
an
open
standard
RDF
and
thus
can
be
accessed
and
reinterpreted
by
other
applications.
In
support
of
flexible
organization,
Jourknow
bases
many
of
its
refinding
capabilities
on
flexible
user-initiated
or
automatically
captured
organizational
metadata.
The
user
may
manually
tag
notes
to
create
named
sets.
Leveraging
the
PLUM
framework
Van
Kleek
and
Shrobe
2007
,
Jourknow
also
automatically
associates
notes
with
a
wide
variety
of
contextual
metadata
such
as
location,
music
and
chat
activity,
as
well
as
open
documents
and
programs.
This
metadata
can
be
used
to
refind
notes
later
via
a
faceted
browsing
Yee
et
al.
2003
interface.
--
Visibility
and
Reminding.
Jourknow
contains
two
simple
awareness
mechanisms:
a
desktop
widget
allowing
users
to
keep
notes
always
visible,
and
an
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
24,
Publication
date:
September
2008.
24:40
M.
Bernstein
et
al.
alarm
mechanism
that
raises
a
later
reminder
of
a
particular
note.
We
are
also
interested
in
exploring
an
interface
to
opportunistically
display
notes
related
to
the
ones
currently
being
authored
or
viewed.
--
Mobility
and
Availability.
Our
goal
is
to
allow
users
to
capture
and
access
Jourknow
data
in
a
variety
of
different
situations.
The
main
Jourknow
client
is
desktop
based;
we
have
augmented
the
system
with
a
mobile
companion
client
called
JourMini.
JourMini
can
access
Jourknow
s
notes
as
well
as
capture
its
own
using
the
phone
s
keypad,
built-in
camera
or
voice
memo
features.
Notes
are
remain
synchronized
between
all
of
the
clients.
Jourknow
s
most
recent
longitudinal
evaluation
Bernstein
et
al.
2008
has
demonstrated
a
variety
of
unsolved
design
and
evaluation
problems
for
information
scrap
management.
We
touch
on
these
open
problems
in
Future
Work.
8.
FUTURE
WORK
An
important
next
step
for
this
work
is
to
extend
our
artifact
and
interview
study
by
observing
scrap
creation
and
refinding
in
situ.
We
have
focused
thus
far
on
understanding
scraps
contents,
tools
and
organization,
examining
artifacts
after
they
have
been
created
and
before
refinding
was
needed.
Our
research
does
not
paint
a
complete
picture
of
creation
and
refinding
as
they
occur.
What
exactly
triggers
the
need
to
record
or
reference
an
information
scrap?
What
kind
of
information
is
recalled
about
each
scrap
at
intervals
after
its
capture?
What
are
the
most
typical
refinding
procedures,
and
how
might
we
support
them?
A
shadowing
study
would
likely
elicit
many
interesting
and
likely
unexpected
answers
to
these
questions.
Just
as
we
found
it
necessary
to
innovate
methodology
for
the
study
information
scraps,
our
experience
suggests
that
it
may
be
necessary
to
innovate
again
with
the
design
process
for
scrap
managers.
Our
ongoing
investigation
Bernstein
et
al.
2008
has
led
us
to
the
following
set
of
open
questions
concerning
the
design
and
evaluation
of
systems
like
Jourknow:
--Scope
of
the
Design.
When
a
PIM
tool
such
as
Jourknow
bases
its
usefulness
on
Gestalt
integration
of
a
wide
variety
of
needs
and
uses,
how
can
we
identify
subsets
of
these
needs
to
prototype
and
test
independently?
The
complete
picture
may
be
the
only
compelling
one,
but
user-centered
design
suggests
that
monolithic
development
and
evaluation
is
likely
to
fail.
How
do
we
isolate
pieces
of
the
system
to
design
and
iterate?
--
Prototyping.
Information
scrap
tools
put
themselves
at
the
mercy
of
a
wide
variety
of
situational
factors
and
constraints,
as
discussed
in
Section
6.3.
How
can
we
then
craft
effective
experience
prototypes
Buchenau
and
Suri
2000
to
garner
feedback
on
the
rich
context
surrounding
notes
capture
and
reuse?
Experience
sampling
studies
may
be
successful
here,
as
they
can
force
users
to
record
information
at
unpredictable
times
and
across
a
variety
of
scenarios.
--
Study
type.
We
have
used
longitudinal
evaluations
to
give
Jourknow
a
chance
to
ingratiate
itself
into
our
participants
practice,
and
to
reflect
on
how
that
practice,
once
engaged,
was
or
wasn
t
successful.
However,
such
studies
are
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
24,
Publication
date:
September
2008.
Information
Scraps
24:41
high-risk,
and
it
is
difficult
to
force
a
change
in
practice
in
just
a
week.
How
can
we
combine
evaluation
methodologies
to
more
effectively
test
our
ideas?
--
Study
population.
The
quest
for
external
validity
dictates
that
researchers
and
practitioners
randomly
choose
participants
from
the
target
population,
rather
than
from
a
hand-picked
subset.
In
the
domain
of
information
scraps,
ironically,
there
are
reasons
why
testing
outside
a
friendly
community
might
hurt
a
study.
Due
to
mission
critical
aspects
of
PIM,
there
is
little
room
for
error--for
example,
while
business
students
were
excellent
critics
of
our
system,
they
were
also
unable
or
unwilling
to
overlook
entry
barriers
to
using
the
system
such
as
outstanding
bugs
and
performance
issues.
Many
of
these
issues
are
also
raised
by
Kelley
and
Teevan
2007
in
the
more
general
context
of
personal
information
management.
9.
CONCLUSION
In
this
article
we
examined
the
phenomenon
of
information
scraps:
personal
information
that
does
not
find
its
way
into
our
current
personal
information
management
tools.
Information
scraps
are
pervasive--our
participants
shared
with
us
an
impressive
number
of
scraps,
both
physical
and
digital,
scattered
over
diverse
parts
of
their
information
environments.
We
examined
the
wide
variety
of
information
held
in
scraps,
the
forms
this
information
took,
and
the
information
scrap
life
cycle--how
scraps
were
captured,
used
and
then
stored
or
disposed.
We
identified
a
set
of
typical
roles
information
scraps
play
in
our
lives:
temporary
storage,
cognitive
support,
reminding,
archiving,
and
capture
of
unusual
information
types.
We
then
examined
the
needs
that
these
roles
demanded
of
our
tools,
and
how
participants
information
scrap
solutions
addressed
the
needs.
These
needs
included
lightweight
fast,
low-effort
capture,
free-form
expression
of
data,
versatile
representation
of
data,
flexible
organizational
and
usage
capacities,
visibility,
proactive
reminding,
and
mobility.
Our
investigation
uncovered
evidence
of
unmet
design
needs
in
today
s
personal
information
management
tools
and
thus
suggests
opportunities
for
improving
those
tools.
By
carefully
considered
redesigns,
we
can
help
users
capture
information
into
applications.
We
hypothesize
that
improved
designs
may
thus
reduce
the
number
of
unmanaged
information
scraps
in
our
lives,
though
systematic
study
has
yet
to
verify
this
hypothesis.
In
particular,
the
wide
variety
of
information
contained
in
information
scraps
is
galvanizing,
as
it
suggests
an
opportunity
for
PIM
to
engage
new
types
of
information.
The
data
indicates
that
a
significant
percentage
of
our
personal
information
is
beyond
the
reach
of
our
current
generation
of
tools,
and
furthermore
will
likely
remain
so
without
a
significant
recalibration
of
our
goals.
We
identified
over
125
information
types
from
our
sample
of
participants,
and
surely
there
are
others.
The
diversity
suggests
that
it
may
not
be
tractable
for
each
of
these
information
types
to
be
managed
by
its
own
tool.
Instead,
we
suggest
that
the
future
of
personal
information
management
may
lie
in
finding
a
flexible
approach
that
encompasses
both
traditional
information
and
the
unique
data
types
that
are
currently
underserved
Karger
2007
.
First
steps
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
24,
Publication
date:
September
2008.
24:42
M.
Bernstein
et
al.
have
been
made
for
example,
Van
Kleek
et
al.
2007,
Jones
et
al.
2005b,
Karger
and
Quan
2004
,
but
it
remains
to
investigate
how
these
approaches
influence
practice.
Taken
in
sum,
these
conclusions
specify
a
set
of
problems
that
will
be
challenging
at
best.
Yet
the
challenge
is
necessary,
even
revolutionary.
For
PIM
to
move
beyond
its
current
limitations,
it
must
venture
beyond
its
established
boundaries--and
into
the
world
of
the
information
scrap.
ACKNOWLEDGMENTS
The
authors
would
like
to
thank
the
participants
and
pilot
testers
for
their
time,
energy,
and
willingness
to
share
their
personal
information.
We
are
indebted
to
Nokia,
Vanu
Inc.,
Endeca
Technologies
Inc.,
the
World
Wide
Web
Consortium
W3C
and
MIT
Computer
Science
and
Artificial
Intelligence
Laboratory
for
granting
access
to
interview
their
employees,
and
Jamey
Hicks,
Kate
Guarino,
and
Daniel
Tunkelang
for
their
help
recruiting
participants.
REFERENCES
ALLEN,
D.
2001.
Getting
Things
Done:
The
Art
of
Stress-Free
Productivity.
Penguin
Books,
New
York,
NY.
ANDERSON,
C.
2006.
The
Long
Tail:
Why
the
Future
of
Business
Is
Selling
Less
of
More.
Hyperion,
New
York,
NY.
BARREAU,
D.
K.
1995.
Context
as
a
factor
in
personal
information
management
systems.
J.
Amer.
Soc.
Inform.
Sci.
46,
327
339.
BARREAU,
D.
AND
NARDI,
B.
A.
1995.
Finding
and
reminding:
file
organization
from
the
desktop.
ACM
SIGCHI
Bull.
27,
39
43.
BEDERSON,
B.
B.
2004.
Interfaces
for
staying
in
the
flow.
Ubiquity
5,
1
1.
BELLOTTI,
V.,
DUCHENEAUT,
N.,
HOWARD,
M.,
SMITH,
I.,
AND
GRINTER,
R.
E.
2005.
Quality
versus
quantity:
E-mail-centric
task
management
and
its
relation
with
overload.
Hum.-Comput.
Interac.
20,
89
138.
BELLOTTI,
V.,
DALAL,
B.,
GOOD,
N.,
FLYNN,
P.,
BOBROW,
D.
G.,
AND
DUCHENEAUT,
N.
2004.
What
a
to-do:
studies
of
task
management
towards
the
design
of
a
personal
task
list
manager.
In
Proceedings
of
the
SIGCHI
Conference
on
Human
Factors
in
Computing
Systems
CHI
04
.
ACM
Press,
735
742.
BELLOTTI,
V.
AND
SMITH,
I.
2000.
Informing
the
design
of
an
information
management
system
with
iterative
fieldwork.
In
Proceedings
of
the
Conference
on
Designing
Interactive
Systems
DIS
00
.
ACM
Press,
227
237.
BERGMAN,
O.,
BEYTH-MAROM,
R.,
AND
NACHMIAS,
R.
2003.
The
user-subjective
approach
to
personal
information
management
systems.
J.
Amer.
Soci.
Inform.
Sci.
Techn.
54,
872
878.
BERNSTEIN,
M.,
VAN
KLEEK,
M.,
KHUSHRAJ,
D.,
NAYAK,
R.,
LIU,
C.,
SCHRAEFEL,
M.,
AND
KARGER,
D.
R.
2008.
Wicked
problems
and
gnarly
results:
Reflecting
on
design
and
evaluation
methods
for
idiosyncratic
personal
information
management
tasks.
Tech.
rep.
MIT.
BERNSTEIN,
M.,
VAN
KLEEK,
M.,
SCHRAEFEL,
M.,
AND
KARGER,
D.
R.
2007.
Management
of
personal
information
scraps.
In
Proceedings
of
the
Extended
Abstracts
on
Human
Factors
in
Computing
Systems
CHI
07
.
ACM
Press,
2285
2290.
BLANC-BRUDE,
T.
AND
SCAPIN,
D.
L.
2007.
What
do
people
recall
about
their
documents?:
Implications
for
desktop
search
tools.
In
Proceedings
of
the
12th
International
Conference
on
Intelligent
User
Interfaces
IUI
07
.
ACM
Press,
102
111.
BLANDFORD,
A.
E.
AND
GREEN,
T.
R.
G.
2001.
Group
and
individual
time
management
tools:
What
you
get
is
not
what
you
need.
Person.
Ubiquitous
Comput.
5,
213
230.
BOARDMAN,
R.
AND
SASSE,
M.
A.
2004.
Stuff
goes
into
the
computer
and
doesn
t
come
out:
A
crosstool
study
of
personal
information
management.
In
Proceedings
of
the
SIGCHI
Conference
on
Human
Factors
in
Computing
Systems
CHI
04
.
ACM
Press,
583
590.
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
24,
Publication
date:
September
2008.
Information
Scraps
24:43
BOARDMAN,
R.,
SPENCE,
R.,
AND
SASSE,
M.
A.
2003.
Too
many
hierarchies?
The
daily
struggle
for
control
of
the
workspace.
In
Proceedings
of
HCI
International,
Vol.
1,
616
620.
BOWKER,
G.
C.
AND
STAR,
S.
L.
2000.
Sorting
Things
Out:
Classification
and
Its
Consequence.
The
MIT
Press,
Cambridge,
MA.
BRUCE,
H.,
JONES,
W.,
AND
DUMAIS,
S.
2004.
Keeping
and
re-finding
information
on
the
Web:
What
do
people
do
and
what
do
they
need.
In
Proceedings
of
ASIST.
BUCHENAU,
M.
AND
SURI,
J.
F.
2000.
Experience
prototyping.
In
Proceedings
of
the
3rd
Conference
on
Designing
Interactive
Systems
DIS
00
.
ACM
Press,
424
433.
BUTZ,
A.
AND
JUNG,
R.
2005.
Seamless
user
notification
in
ambient
soundscapes.
In
Proceedings
of
the
10th
International
Conference
on
Intelligent
User
Interfaces
IUI
05
.
ACM
Press,
320
322.
CAMPBELL,
C.
AND
MAGLIO,
P.
2003.
Supporting
notable
information
in
office
work.
In
Proceedings
of
the
SIGCHI
Conference
Extended
Abstracts
on
Human
Factors
in
Computing
Systems
CHI
03
.
ACM
Press,
902
903.
CSIKSZENTMIHALYI,
M.
1991.
Flow:
The
Psychology
of
Optimal
Experience.
Harper
Perennial.
DAHLEY,
A.,
WISNESKI,
C.,
AND
ISHII,
H.
1998.
Water
lamp
and
pinwheels:
ambient
projection
of
digital
information
into
architectural
space.
In
Proceedings
of
the
SIGCHI
Conference
on
Human
Factors
in
Computing
Systems
CHI
98
.
ACM
Press,
269
270.
DAI,
L.,
LUTTERS,
W.
G.,
AND
BOWER,
C.
2005.
Why
use
memo
for
all?:
Restructuring
mobile
applications
to
support
informal
note
taking.
In
Proceedngs
of
the
SIGCHI
Conference
Extended
Abstracts
on
Human
Factors
in
Computing
Systems
CHI
98
.
ACM
Press,
1320
1323.
DARKEN,
R.
P.
AND
SIBERT,
J.
L.
1993.
A
toolset
for
navigation
in
virtual
environments.
In
Proceedings
of
the
6th
Annual
ACM
Symposium
on
User
Interface
Software
and
Technology
UIST
93
.
ACM
Press,
157
165.
DONTCHEVA,
M.,
DRUCKER,
S.
M.,
SALESIN,
D.,
AND
COHEN,
M.
F.
2007.
Relations,
cards,
and
search
templates:
user-guided
web
data
integration
and
layout.
In
Proceedings
of
the
20th
Annual
ACM
Symposium
on
User
Interface
Software
and
Technology
UIST
07
.
ACM
Press,
61
70.
DUCHENEAUT,
N.
AND
BELLOTTI,
V.
2001.
E-mail
as
habitat:
An
exploration
of
embedded
personal
information
management.
interactions
8,
30
38.
DUMAIS,
S.,
CUTRELL,
E.,
CADIZ,
J.,
JANCKE,
G.,
SARIN,
R.,
AND
ROBBINS,
D.
C.
2003.
Stuff
I
ve
seen:
A
system
for
personal
information
retrieval
and
re-use.
In
Proceedings
of
the
26th
Annual
International
ACM
SIGIR
Conference
on
Research
and
Development
in
Information
Retrieval
SIGIR
03
.
ACM
Press,
72
79.
ENNALS,
R.
J.
AND
GAROFALAKIS,
M.
N.
2007.
MashMaker:
Mashups
for
the
masses.
In
Proceedings
of
the
ACM
SIGMOD
International
Conference
on
Management
of
Data
SIGMOD
07
.
ACM
Press,
1116
1118.
FERTIG,
S.,
FREEMAN,
E.,
AND
GELERNTER,
D.
1996.
Lifestreams:
An
alternative
to
the
desktop
metaphor.
In
Proceedings
of
the
Conference
Companion
on
Human
Factors
in
Computing
Systems
CHI
96
.
ACM
Press,
410
411.
FISHER,
D.,
BRUSH,
A.
J.,
GLEAVE,
E.,
AND
SMITH,
M.
A.
2006.
Revisiting
Whittaker
Sidner
s
email
overload
ten
years
later.
In
Proceedings
of
the
20th
Anniversary
Conference
on
Computer
Supported
Cooperative
Work
CSCW
06
.
ACM
Press,
309
312.
GEMMEL,
J.,
BELL,
G.,
LUEDER,
R.,
DRUCKER,
S.,
AND
WONG,
C.
2002.
MyLifeBits:
Fulfilling
the
Memex
Vision.
In
Proceedings
of
the
10th
International
Conference
on
Multimedia
Multimedia
02
.
ACM
Press,
235
238.
GONCALVES,
D.
AND
JORGE,
J.
A.
2004.
Describing
documents:
what
can
users
tell
us?
In
Proceedings
of
the
9th
International
Conference
on
Intelligent
User
Interfaces
IUI
04
.
ACM
Press,
247
249.
GRAY,
W.
D.
AND
BOEHMDAVIS,
D.
A.
2000.
Milliseconds
matter:
An
introduction
to
microstrategies
and
to
their
use
in
describing
and
predicting
interactive
behavior.
J.
Experi.
Psych.
Appl.
6,
322
335.
GRAY,
W.
D.
AND
FU,
W.
2001.
Ignoring
perfect
knowledge
in-the-world
for
imperfect
knowledge
in-the-head.
In
Proceedings
of
the
SIGCHI
Conference
on
Human
Factors
in
Computing
Systems
CHI
01
,
ACM
Press,
112
119.
HALBERT,
D.
C.
1993.
SmallStar:
Programming
by
demonstration
in
the
desktop
metaphor.
In
Watch
What
I
Do:
Programming
by
Demonstration,
Cypher,
A.
Ed.
MIT
Press,
Cambridge
MA,
103
123.
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
24,
Publication
date:
September
2008.
24:44
M.
Bernstein
et
al.
HAYES,
G.,
PIERCE,
J.
S.,
AND
ABOWD,
G.
D.
2003.
Practices
for
capturing
short
important
thoughts.
In
Proceedings
of
the
SIGCHI
Conference
on
Extended
Abstracts
on
Human
Factors
in
Computing
Systems
CHI
03
.
ACM
Press,
904
905.
HODGES,
S.,
WILLIAMS,
L.,
BERRY,
E.,
IZADI,
S.,
SRINIVASAN,
J.,
BUTLER,
A.,
SMYTH,
G.,
KAPUR,
N.,
AND
WOOD,
K.
2006.
SenseCam:
A
retrospective
memory
aid.
In
Proceedings
of
the
8th
International
Conference
on
Ubiquitous
Computing
UbiComp
06
.
Springer,
177
193.
HILL,
W.
C.,
HOLLAN,
J.
D.,
WROBLEWSKI,
D.,
AND
MCCANDLESS,
T.
1992.
Edit
wear
and
read
wear.
In
Proceedings
of
the
SIGCHI
Conference
on
Human
Factors
in
Computing
Systems
CHI
92
.
ACM
Press,
3
9.
HSIEH,
G.,
WOOD,
K.
AND
SELLEN,
A.
2006.
Peripheral
display
of
digital
handwritten
notes.
In
Proceedings
of
the
SIGCHI
Conference
on
Human
Factors
in
Computing
Systems
CHI
06
.
ACM
Press,
285
288.
HUMANIZED.
Enso.
http:
www.humanized.com
enso
.
HUYNH,
D.
F.,
KARGER,
D.
R.,
AND
MILLER,
R.
C.
2007.
Exhibit:
Lightweight
structured
data
publishing.
In
Proceedings
of
the
16th
International
Conference
on
World
Wide
Web
WWW
07
.
ACM
Press,
737
746.
ITO,
M.
AND
OKABE,
D.
2003.
Camera
phones
changing
the
definition
of
picture-worthy.
Japan
Media
Rev.
29.
JONES,
W.
2004.
Finders,
keepers?
The
present
and
future
perfect
in
support
of
personal
information
management.
First
Monday
9.
JONES,
W.
P.
AND
DUMAIS,
S.
T.
1986.
The
spatial
metaphor
for
user
interfaces:
experimental
tests
of
reference
by
location
versus
name.
ACM
Trans.
Inform.
Syst.
4,
42
63.
JONES,
W.
2007a.
Introduction.
In
Personal
Information
Management,
W.
Jones
and
J.
Teevan,
Eds.
University
of
Washington
Press,
Seattle,
WA,
3
21.
JONES,
W.,
BRUCE,
H.,
AND
DUMAIS,
S.
2001.
Keeping
found
things
found
on
the
web.
In
Proceedings
of
the
10th
International
Conference
on
Information
and
Knowledge
Management
CIKM
01
.
ACM
Press,
119
126.
JONES,
W.,
DUMAIS,
S.,
AND
BRUCE,
H.
2005a.
Once
found,
what
then?
A
study
of
keeping
behaviors
in
the
personal
use
of
Web
information.
In
Proceedings
of
the
Amer.
Soc.
Inform.
Sci.
Techn.
39,
391
402.
JONES,
W.,
MUNAT,
C.,
AND
BRUCE,
H.
2005b.
The
universal
labeler:
Plan
the
project
and
let
your
information
follow.
In
Proceedings
of
ASIST.
JONES,
W.,
PHUWANARTNURAK,
A.
J.,
GILL,
R.,
AND
BRUCE,
H.
2005c.
Don
t
take
my
folders
away!:
Organizing
personal
information
to
get
things
done.
In
Proceedings
of
the
SIGCHI
Conference
Extended
Abstracts
on
Human
Factors
in
Computing
Systems
CHI
05
.
ACM
Press,
1505
1508.
KALNIKAIT
,
V.
AND
WHITTAKER,
S.
2007.
Software
or
wetware?:
discovering
when
and
why
people
E
use
digital
prosthetic
memory.
In
Proceedings
of
the
SIGCHI
Conference
on
Human
Factors
in
Computing
Systems
CHI
07
.
ACM
Press,
71
80.
KARGER,
D.
R.
2007.
Unify
Everything:
It
s
All
the
Same
To
Me.
In
Personal
Information
Management,
W.
Jones
and
J.
Teevan,
Eds.
University
of
Washington
Press,
Seattle,
WA,
127
152.
KARGER,
D.
R.
AND
QUAN,
D.
2004.
Haystack:
A
user
interface
for
creating,
browsing,
and
organizing
arbitrary
semistructured
information.
In
Proceeding
of
the
Extended
Abstracts
on
Human
Factors
in
Computing
Systems
CHI
04
.
ACM
Press,
777
778.
KELLEY,
D.
AND
TEEVAN,
J.
2007.
Understanding
what
works:
Evaluating
PIM
tools.
In
Personal
Information
Management,
W.
Jones
and
J.
Teevan,
Eds.
University
of
Washington
Press,
Seattle,
WA,
190
205.
KHAN,
F.
1993.
A
survey
of
note-taking
practices.
Tech.
rept.
Hewlett
Packard,
HPL-93-107.
KINDBERG,
T.,
SPASOJEVIC,
M.,
FLECK,
R.,
AND
SELLEN,
A.
2004.
How
and
why
people
use
camera
phones
Tech.
rept.
Hewlett
Packard,
HPL-2004
216.
KIRSH,
D.
AND
MAGLIO,
P.
P.
1994.
On
distinguishing
epistemic
from
pragmatic
action.
Cogn.
Sci.
18,
513
549.
LAMMING,
M.,
BROWN,
P.,
CARTER,
K.,
ELDRIDGE,
M.,
FLYNN,
M.,
LOUIE,
G.,
ROBINSON,
P.,
AND
SELLEN,
A.
1994.
The
design
of
a
human
memory
prosthesis.
Comput.
J.
37,
153
163.
LANSDALE,
M.
W.
1988.
The
psychology
of
personal
information
management.
Appl.
Ergonomics
19,
55
66.
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
24,
Publication
date:
September
2008.
Information
Scraps
24:45
LICHTENSTEIN,
S.,
FISCHHOFF,
B.,
AND
PHILLIPS,
L.
D.
1982.
Calibration
of
probabilities:
The
state
of
the
art
to
1980.
In
Judgment
under
Uncertainty:
Heuristics
and
Biases,
Kahneman,
D.,
Slovic,
P.,
and
Tversky,
A.,
Eds.
Cambridge
University
Press,
Cambridge,
UK,
306
334.
LIN,
M.,
LUTTERS,
W.
G.,
AND
KIM,
T.
S.
2004.
Understanding
the
micronote
life
cycle:
Improving
mobile
support
for
informal
note
taking.
In
Proceedings
of
the
SIGCHI
Conference
on
Human
Factors
in
Computing
Systems
CHI
04
.
ACM
Press,
687
694.
MALONE,
T.
W.
1983.
How
do
people
organize
their
desks?:
Implications
for
the
design
of
office
information
systems.
ACM
Trans.
Inform.
Syst.
1,
1,
99
112.
MANDER,
R.,
SALOMON,
G.,
AND
WONG,
Y.
Y.
1992.
A
pile
metaphor
for
supporting
casual
organization
of
information.
In
Proceedings
of
the
SIGCHI
Conference
on
Human
Factors
in
Computing
Systems
CHI
92
.
ACM
Press,
627
634.
MCALPINE,
H.,
HICKS,
B.
J.,
HUET,
G.,
AND
CULLEY,
S.
J.
2006.
An
investigation
into
the
use
and
content
of
the
engineer
s
logbook.
Des.
Stud.
27,
4,
481
504.
O
DAY,
V.
L.
AND
JEFFRIES,
R.
1993.
Orienteering
in
an
information
landscape:
How
information
seekers
get
from
here
to
there.
In
Proceedings
of
the
SIGCHI
Conference
on
Human
Factors
in
Computing
Systems
CHI
93
.
ACM
Press,
438
445.
OULASVIRTA,
A.
AND
SUMARI,
L.
2007.
Mobile
kits
and
laptop
trays:
Managing
multiple
devices
in
mobile
information
work.
In
Proceedings
of
the
SIGCHI
Conference
on
Human
Factors
in
Computing
Systems
CHI
07
.
ACM
Press,
1127
1136.
PINHANEZ,
C.
S.
2001.
The
Everywhere
Displays
Projector:
A
Device
to
Create
Ubiquitous
Graphical
Interfaces.
In
Proceedings
of
the
3rd
International
Conference
on
Ubiquitous
Computing
UbiComp
01
.
Springer-Verlag,
315
331.
POPESCU,
A.,
ETZIONI,
O.,
AND
KAUTZ,
H.
2003.
Towards
a
theory
of
natural
language
interfaces
to
databases.
In
Proceedings
of
the
8th
International
Conference
on
Intelligent
User
Interfaces
IUI
03
.
ACM
Press,
149
157.
RHODES,
B.
1997.
The
wearable
remembrance
agent:
A
system
for
augmented
memory.
Person.
Ubiquitous
Comput.
1,
4,
218
224.
ROBERTSON,
G.,
CZERWINSKI,
M.,
LARSON,
K.,
ROBBINS,
D.
C.,
THIEL,
D.,
AND
DANTZICH,
M.
V.
1998a.
Data
mountain:
Using
spatial
memory
for
document
management.
In
Proceedings
of
the
11th
Annual
ACM
Symposium
on
User
Interface
Software
and
Technology
UIST
98
.
ACM
Press,
153
162.
ROSS,
L.
AND
NISBETT,
R.
1991.
The
Person
and
the
Situation:
Perspectives
of
Social
Psychology.
McGraw-Hill.
New
York,
NY.
SCHRAEFEL,
M.
C.,
HUGHES,
G.
V.,
MILLS,
H.
R.,
SMITH,
G.,
PAYNE,
T.
R.,
AND
FREY,
J.
2004.
Breaking
the
book:
Translating
the
chemistry
lab
book
into
a
pervasive
computing
lab
environment.
In
Proceedings
of
the
SIGCHI
Conference
on
Human
Factors
in
Computing
Systems
CHI
04
.
ACM
Press,
25
32.
SELLEN,
A.
J.,
FOGG,
A.,
AITKEN,
M.,
HODGES,
S.,
ROTHER,
C.,
AND
WOOD,
K.
2007.
Do
life-logging
technologies
support
memory
for
the
past?:
An
experimental
study
using
sensecam.
In
Proceedings
of
the
SIGCHI
Conference
on
Human
Factors
in
Computing
Systems
CHI
07
.
ACM
Press,
81
90.
SELLEN,
A.
J.
AND
HARPER,
R.
H.
R.
2003.
The
Myth
of
the
Paperless
Office.
MIT
Press,
Cambridge,
MA.
STIFELMAN,
L.,
ARONS,
B.,
AND
SCHMANDT,
C.
2001.
The
audio
notebook:
paper
and
pen
interaction
with
structured
speech.
In
Proceedings
of
the
SIGCHI
Conference
on
Human
Factors
in
Computing
Systems
CHI
01
.
ACM
Press,
182
189.
TEEVAN,
J.,
ALVARADO,
C.,
ACKERMAN,
M.
S.,
AND
KARGER,
D.
R.
2004.
The
perfect
search
engine
is
not
enough:
A
study
of
orienteering
behavior
in
directed
search.
In
Proceedings
of
the
SIGCHI
Conference
on
Human
Factors
in
Computing
Systems
CHI
04
.
ACM
Press,
415
422.
TRAPANI,
G.
2007.
Lifehacker.
http:
www.lifehacker.com.
VAN
KLEEK,
M.,
BERNSTEIN,
M.,
ANDRE,
P.,
KARGER,
D.
R.,
AND
SCHRAEFEL,
M.
2008.
Simplifying
knowledge
creation
and
access
for
end-users
on
the
SW.
In
CHI
2008
Workshop
on
Semantic
Web
User
Interfaces.
VAN
KLEEK,
M.,
BERNSTEIN,
M.,
KARGER,
D.
R.,
AND
SCHRAEFEL,
M.
2007.
GUI--Phooey!:
The
case
for
text
input.
In
Proceedings
of
the
20th
Annual
ACM
Symposium
on
User
Interface
Software
and
Technology
UIST
07
.
ACM
Press,
193
202.
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
24,
Publication
date:
September
2008.
24:46
M.
Bernstein
et
al.
VAN
KLEEK,
M.
AND
SHROBE,
H.
2007.
A
Practical
Activity
Capture
Framework
for
Personal,
Lifetime
User
Modeling.
In
Proceedings
of
User
Modeling.
Springer.
VENOLIA,
G.
D.,
DABBISH,
L.
A.,
CADIZ,
J.
J.,
AND
GUPTA,
A.
2001.
Supporting
Email
Workflow.
Tech.
rep.
Microsoft
Research,
MSR-TR-2001
88.
WAGENAAR,
W.
A.
1986.
My
memory:
A
study
of
autobiographical
memory
over
six
years.
Cognitive
psych.
18,
225
252.
WHITTAKER,
S.
AND
HIRSCHBERG,
J.
2001.
The
character,
value,
and
management
of
personal
paper
archives.
ACM
Trans.
Comput.-Hum.
Interac.
8,
2,
150
170.
WHITTAKER,
S.
AND
SIDNER,
C.
1996.
Email
overload:
exploring
personal
information
management
of
email.
In
Proceedings
of
the
SIGCHI
Conference
on
Human
Factors
in
Computing
Systems
CHI
96
.
ACM
Press,
276
283.
YATES,
F.
A.
1966.
The
Art
of
Memory.
University
of
Chicago
Press,
Chicago,
IL.
YEE,
K.,
SWEARINGEN,
K.,
LI,
K.,
AND
HEARST,
M.
2003.
Faceted
metadata
for
image
search
and
browsing.
In
Proceedings
of
the
SIGCHI
Conference
on
Human
Factors
in
Computing
Systems
CHI
03
.
ACM
Press,
401
408.
YEH,
R.,
LIAO,
C.,
KLEMMER,
S.,
GUIMBRETI
RE,
F.,
LEE,
B.,
KAKARADOV,
B.,
STAMBERGER,
J.,
AND
PAEPCKE,
E
A.
2006.
ButterflyNet:
A
mobile
capture
and
access
system
for
field
biology
research.
In
Proceedings
of
the
SIGCHI
Conference
on
Human
Factors
in
Computing
Systems
CHI
06
.
ACM
Press,
571
580.
Received
June
2007;
revised
February
2008;
accepted
April
2008
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
24,
Publication
date:
September
2008.
Editorial:
Reviewer
Merits
and
Review
Control
in
an
Age
of
Electronic
Manuscript
Management
Systems
GARY
MARCHIONINI
Editor-in-Chief
2002
2008
Peer
review
is
an
important
resource
of
scholarly
communities
and
must
be
managed
and
nurtured
carefully.
Electronic
manuscript
management
systems
have
begun
to
improve
some
aspects
of
workflow
for
conferences
and
journals
but
also
raise
issues
related
to
reviewer
roles
and
reputations
and
the
control
of
reviews
over
time.
Professional
societies
should
make
their
policies
related
to
reviews
and
reviewer
histories
clear
to
authors
and
reviewers,
develop
strategies
and
tools
to
facilitate
good
and
timely
reviews,
and
facilitate
the
training
of
new
reviewers.
Categories
and
Subject
Descriptors:
I.7.4
Publishing
General
Terms:
Documentation
Additional
Key
Words
and
Phrases:
Peer
review,
manuscript
management
systems
ACM
Reference
Format:
Marchionini,
G.
2008.
Editorial:
Reviewer
merits
and
review
control
in
an
age
of
electronic
manuscript
management
systems.
ACM
Trans.
Inform.
Syst.
26,
4,
Article
25
September
2008
,
6
pages.
DOI
10.1145
1402256.1402264
http:
doi.acm.org
10.1145
1402256.1402264
Document
and
Text
Processing
:
Electronic
1.
INTRODUCTION
The
ways
that
papers
are
produced,
submitted,
reviewed,
and
published
continue
to
evolve
with
each
generation
of
information
technology.
Word
processing
changed
the
authoring
process
years
ago
and
the
Web
is
rapidly
changing
publication
and
distribution.
For
example,
the
ACM
Digital
Library
rather
than
hardcopy
is
the
primary
means
of
distribution
for
conference
proceedings
and
journals,
and
ACM
Transactions
journals
have
stopped
using
continuous
page
numbering
per
issue
or
volume.
Grudin
2004
provides
an
excellent
overview
of
the
changes
electronic
publishing
brings
and
the
implications
of
those
changes
for
journals.
The
changes
that
have
come
most
slowly,
however,
Authors
address:
100
Manning
Hall
School
of
Information
and
Library
Science,
University
of
North
Carolina,
Chapel
Hill,
NC
27599.
Permission
to
make
digital
or
hard
copies
of
part
or
all
of
this
work
for
personal
or
classroom
use
is
granted
without
fee
provided
that
copies
are
not
made
or
distributed
for
profit
or
direct
commercial
advantage
and
that
copies
show
this
notice
on
the
first
page
or
initial
screen
of
a
display
along
with
the
full
citation.
Copyrights
for
components
of
this
work
owned
by
others
than
ACM
must
be
honored.
Abstracting
with
credit
is
permitted.
To
copy
otherwise,
to
republish,
to
post
on
servers,
to
redistribute
to
lists,
or
to
use
any
component
of
this
work
in
other
works
requires
prior
specific
permission
and
or
a
fee.
Permissions
may
be
requested
from
Publications
Dept.,
ACM,
Inc.,
2
Penn
Plaza,
Suite
701,
New
York,
NY
10121-0701,
USA,
fax
1
212
869-0481,
or
permissions
acm.org.
C
2008
ACM
1046-8188
2008
09-ART25
5.00
DOI
10.1145
1402256.1402264
http:
doi.acm.org
10.1145
1402256.1402264
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
25,
Publication
date:
September
2008.
25
25:2
G.
Marchionini
are
in
the
manuscript
management
and
review
phases
of
scholarly
publication.
Most
commercial
and
non-profit
journals
have
created
or
are
adopting
commercial
manuscript
management
systems
MMS
that
in
turn
are
beginning
to
affect
workflow
and,
arguably,
author
and
reviewer
behavior.
Some
of
these
changes
are
obvious
and
others
are
more
subtle.
This
note
is
particularly
focused
on
the
evolution
of
peer
review,
especially
the
roles
of
and
effects
on
reviewers.
It
assumes
that
peer
review
adds
significant
value
to
the
quality
of
new
knowledge
and
will
continue
in
some
form
for
the
foreseeable
future.
Three
premises
and
their
associated
implications
are
outlined:
reviewers,
reviews,
and
tools
to
support
reviewers
and
editors.
2.
REVIEWERS
ARE
A
SCARCE
AND
VALUABLE
RESOURCE
Ask
any
editor
of
a
peer-review
journal
what
their
greatest
challenge
is
and
the
response
will
probably
be
related
to
finding
expert
reviewers
for
manuscripts
and
to
encouraging
the
reviewers
to
return
high-quality
reviews
in
a
timely
fashion
and
subsequently
to
agree
to
review
revisions.
That
reviewers
are
the
linchpins
in
scientific
publishing
was
articulated
by
then
Science
editor,
Donald
Kennedy:
the
process
of
scientific
publication
depends
on
the
volunteer
services
of
thousands
of
experts
all
over
the
world
who
willingly
provide,
without
compensation,
confidential
and
candid
evaluations
of
the
work
of
others
Kennedy
2008,
p.
1009
.
As
such,
reviewers
deserve
credit
for
their
contributions
to
the
production
of
new
knowledge.
Many
reviewers
add
to
their
vitae
the
names
of
conferences
and
journals
for
which
they
review,
thus
providing
self-documentation
for
their
efforts
at
a
very
general
level.
Some
universities
and
companies
take
these
efforts
into
account
when
assessing
employee
merit.
Conferences
and
some
journals
publish
honor
roles
of
reviewers
to
provide
some
recognition.
In
some
cases,
authors
give
acknowledgments
to
the
anonymous
reviewers,
and
perhaps
authors
can
be
encouraged
to
point
to
specific
improvements
or
insights
provided
by
anonymous
reviewers.
This
does
not
bring
personal
credit
to
the
individual
reviewer
but
does
reinforce
the
contributions
of
reviewers
at
large.
This
is
a
small
additional
burden,
but
already
authors
in
some
journals
are
asked
to
provide
assurances
of
independence
from
financial
support
and
to
provide
breakdowns
of
relative
author
contributions
for
articles
with
many
co-authors.
These
are
trends
toward
bolstering
quality
assurance
as
well
as
making
the
social
nature
of
scientific
progress
more
transparent.
Linking
ideas
to
a
faceless
review
community
is
another
possible
step
in
this
direction.
In
some
cases,
editors
are
asked
to
write
letters
verifying
the
scientific
contributions
of
reviewers
for
purposes
of
immigration
documentation
or
scholarly
productivity,
thus
explicitly
rewarding
good
reviewer
contributions.
But
the
rewards
of
reviewing
sit
on
the
inherent
satisfaction
of
knowing
one
has
contributed
to
the
advance
of
science,
on
the
new
knowledge
gained
by
reading
cutting
edge
work,
and
in
the
social
capital
that
grows
in
the
research
community.
Experienced
reviewers
learn
that
they
earn
reputation
capital
with
their
reviews,
and
this
capital
becomes
part
of
the
research
community
s
rich
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
25,
Publication
date:
September
2008.
Editorial:
Reviewer
Merits
and
Review
Control
in
an
Age
of
Electronic
MMS
25:3
experience
base,
though
electronic
systems
are
beginning
to
affect
this
experience
base
and
possibly
reviewer
behavior.
The
adoption
of
electronic
MMSs
by
all
major
journals
and
conferences
today
has
the
potential
of
making
reviewer
reward
much
more
tractable
and
may
therefore
make
review
activity
more
explicit
in
the
record
of
individual
scholarship.
In
the
case
of
journal
MMSs,
there
are
simple
rating
scales
for
review
timeliness
and
quality.
These
ratings
can
be
used
by
editors
at
their
discretion
and
at
present
offer
fairly
simplistic
scales.
The
critical
element
of
these
systems
is
that
the
reviews
themselves
as
well
as
these
ratings
are
persistent,
outliving
the
terms
and
memories
of
individual
editors,
thus
reducing
community
memory
to
simple
scales
that
persist
beyond
the
memories
and
perceptions
of
individuals
in
a
community.
As
they
become
more
uniformly
adopted
and
more
sophisticated
in
scaling
review
contribution,
the
ratings
and
the
reviews
themselves
become
the
basis
for
evaluation
of
scholarly
productivity.
Thus,
editors
may
be
increasingly
called
upon
to
provide
letters
assessing
reviewer
productivity
for
a
variety
of
purposes,
including
promotion
and
tenure
in
academic
settings.
Reviewers
themselves
must
be
aware
of
such
possibilities
and
redouble
their
efforts
to
produce
timely
and
high-quality
reviews
this
point
was
raised
in
editorials
in
several
journals
over
the
past
several
years,
e.g.,
Marchionini
et
al.
2007
.
Raising
this
issue
drew
strong
responses
from
some
scholars
who
objected
to
the
grading
of
their
reviews
and
wanting
to
know
what
their
grades
were.
At
present,
there
is
no
clear
evidence
about
how
such
ratings
are
being
used
or
policies
about
how
they
can
be
used,
nor
are
there
consistent
mechanisms
for
reviewers
to
access
their
grades
or
address
perceived
injustice.
The
main
point
is
that
reviewers
must
be
aware
of
the
importance
of
their
evaluative
responsibilities
and
the
possibilities
for
how
their
evaluations
in
turn
reflect
upon
their
own
scholarship.
3.
REVIEWS
ARE
PART
OF
THE
SCHOLARLY
RECORD
An
important
element
of
scholarly
articles
is
that
the
published
version
reflects
the
insights
of
the
authors
as
well
as
a
set
of
peers
who
reviewed
it.
Although
electronic
publication
and
dissemination
blurs
some
of
the
boundaries
between
the
authoritative
published
version
and
Web-based
preprints
or
subsequent
annotated
versions,
the
journal
or
conference
version
preserved
in
the
organization
s
publication
repository
e.g.,
the
ACM
Digital
Library
includes
insights
made
by
reviewers.
In
effect,
scientific
publishing
is
increasingly
recognized
as
a
social
process
that
not
only
includes
larger
numbers
of
authors
but
also
contributions
from
reviewers
and
editors,
and,
as
electronic
publishing
becomes
the
norm,
contributions
from
readers
and
annotators.
The
electronic
MMSs
in
use
raise
new
kinds
of
questions
for
reviews
in
particular
and
foreshadow
issues
for
Web-based
annotations
of
published
papers
in
the
future.
Two
kinds
of
questions
arise.
Who
controls
reviews
and
how
will
they
be
used
in
data
mining
applications?
Who
controls
reviewing
histories
and
how
can
reviewing
performance
be
systematically
appraised
and
rewarded?
On
one
hand,
mining
of
reviews
can
provide
valuable
data
for
professional
organizations
and
historians
of
science.
For
example,
ACM
might
want
to
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
25,
Publication
date:
September
2008.
25:4
G.
Marchionini
examine
all
ACM
reviews
across
journals
or
examine
how
ACM
reviews
differ
from
IEEE
or
other
field
reviewers;
a
scholar
might
study
different
review
patterns
across
disciplines.
Moreover,
as
we
all
know,
many
good
ideas
come
out
of
the
reviews
and
a
history
of
science
investigation
might
track
the
development
of
key
ideas
in
a
field
by
examining
reviews.
In
email
discussions
with
associate
editors
in
several
journals,
good
suggestions
included
allowing
authors
and
editors
to
nominate
exemplary
reviews
for
inclusion
in
a
review
repository
that
would
be
publicly
available
and
establishing
a
50-year
archive
of
all
reviews
that
could
be
made
available
to
future
generations
to
mine
and
study.
Both
of
these
approaches
would
also
provide
exemplars
for
students
and
young
scholars
to
learn
how
to
write
good
reviews.
The
key
issue
at
present
is
one
of
control
over
review
databases.
Commercial
systems
serve
many
journals
and
editors
should
understand
the
contractual
agreements
regarding
who
controls
use
and
access
to
reviews.
On
the
other
hand,
journals
must
protect
the
review
authority
and
confidentiality
if
peer
review
is
to
continue
to
be
effective.
The
recent
judicial
rulings
on
the
Pfizer
lawsuits
against
the
New
England
Journal
of
Medicine
and
the
Journal
of
the
American
Medical
Association
affirm
the
importance
of
reviewer
confidentiality
but
this
is
likely
to
be
only
the
first
of
many
issues
that
professional
societies
and
journals
will
face.
In
these
cases,
Pfizer
argued
that
the
articles
in
the
journals
were
used
in
lawsuits
against
two
of
the
company
s
products
and
that
it
should
have
access
to
the
peer
reviews
of
those
articles
as
well
as
reviews
of
rejected
articles
on
the
topic
in
order
to
argue
its
case.
Courts
in
Illinois
and
Massachusetts
both
upheld
the
journals
rights
to
protect
the
confidentiality
of
the
reviews
e.g.,
see
Curfman
et
al.
2009
.
ACM
and
other
professional
organizations
must
be
similarly
vigilant
to
protect
the
confidentiality
of
reviews
and
reviewers.
Likewise,
clear
guidelines
for
how
reviews
might
be
aggregated
either
anonymously
or
not
and
used
for
commercial
or
scholarly
purposes
must
be
established
in
the
system
contracts
that
publishers
sign,
and
these
guidelines
must
be
clearly
specified
to
the
entire
scholarly
community.
The
question
of
reviewer
history
is
likewise
double-edged.
If
reviewers
have
access
to
their
histories,
they
can
use
them
as
evidence
of
their
contributions
or
challenge
the
ratings
that
become
associated
with
their
reviews.
Publisher
and
editor
workloads
will
likely
increase
on
either
count
as
editors
are
called
upon
to
support
or
refute
reviewer
claims.
Insuring
the
confidentiality
of
reviewer
history
seems
as
important
as
insuring
the
confidentiality
of
individual
reviews.
4.
WE
NEED
BETTER
TOOLS
AND
MODELS
FOR
REVIEWING
Email
and
electronic
MMSs
have
made
distribution
of
manuscripts
and
reviews
more
efficient.
However,
the
workflow
bottlenecks
continue
to
lie
in
editor
and
reviewer
reading
and
writing
time.
The
IT
community
is
developing
better
tools
to
support
specific
subtasks
but
there
is
a
long
way
to
go
to
find
effective
techniques
and
develop
tools
that
can
assist
the
review
process.
A
variety
of
immediate
challenges
arise
for
designers
and
developers.
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
25,
Publication
date:
September
2008.
Editorial:
Reviewer
Merits
and
Review
Control
in
an
Age
of
Electronic
MMS
25:5
Better
user
interfaces
for
review
systems
are
needed.
Today
s
MMSs
are
monolithic
because
they
must
serve
a
variety
of
journals
or
conferences
each
of
which
has
its
own
cultures
and
policies.
Editors
and
program
chairs
are
left
to
develop
workarounds
for
different
defaults.
Fundamental
policies
vary
widely
across
different
journals
and
conferences
e.g.,
review
tiers,
review
discussion
and
rebuttal,
reminder
frequencies,
and
scope
decisions
.
Customizable
user
interfaces
can
flatten
the
learning
curve
and
reduce
the
need
for
workarounds.
Plagiarism
and
self-plagiarism
are
significant
issues
today
as
text
can
be
retrieved
and
reused
so
easily.
Assessing
both
outright
plagiarism
and
the
more
slippery
reuse
of
one
s
own
text
consume
considerable
editorial
time.
ACM
has
well-publicized
policies
on
plagiarism
and
self-plagiarism
and
these
help
reviewers
and
editors
make
such
assessments.
Additionally,
many
publishers
are
testing
automatic
plagiarism
detection
systems,
and
it
is
likely
that
authors
will
be
required
to
agree
to
have
their
papers
scanned
against
the
published
and
perhaps
open
Web
text
corpuses.
Plagiarism
detection
systems
that
are
effective
and
efficient
while
protecting
intellectual
rights
and
minimizing
abuse
will
help
both
reviewers
and
editors
in
expediting
reviews.
Less
well-defined
are
editing
tools
or
widgets
that
authors,
reviewers,
and
editors
can
use
while
reading
and
writing.
For
authors,
support
for
terminology
and
structure
patterns
that
help
retrieval
of
the
final
product
would
be
useful.
For
reviewers
and
editors,
widgets
that
parse
text
and
link
to
cited
work
and
related
work
would
save
much
of
the
ad
hoc
Web
searching
used
to
compare
and
verify
claims.
Simple
English-language
copyediting
tools
beyond
spell
checking
would
be
especially
welcome
given
the
globalized
research
community.
Better
media
handling
tools
will
also
be
important
as
papers
increasingly
include
multimedia
and
code-based
components.
Reviewing
is
a
learned
and
highly
demanding
activity
and
better
models
for
preparing
young
scholars
are
needed.
Some
conferences
and
journals
allow
reviewers
to
use
secondary
reviewers
with
acknowledgement
and
assurance
that
the
reviews
have
been
checked
who
are
often
students.
Databases
of
exemplary
reviews
as
noted
previously
are
one
approach
that
could
benefit
newbies
as
well
as
give
credit
to
reviewers
who
wrote
the
reviews.
Perhaps
a
book
of
readings
in
information
retrieval
with
not
only
papers
but
the
reviews
would
be
a
good
contribution.
SUMMARY
Peer
review
is
not
without
its
flaws,
but
it
has
served
science
well
for
much
of
the
last
century
and
is
worth
preserving
and
improving.
Peer
review
depends
on
high-quality
reviews
and
every
effort
must
be
made
to
encourage
and
support
reviewing
excellence.
Such
support
ranges
from
finding
ways
to
provide
credit,
while
preserving
blind
review,
creating
better
tools
to
support
the
review
process,
protecting
reviewer
and
review
confidentiality,
and
helping
new
scholars
develop
excellent
review
skills.
Electronic
systems
are
disruptive
to
scholarly
publishing,
and
peer
review
will
also
be
affected.
However
this
plays
out
in
the
coming
decades,
it
is
crucial
that
scientific
progress
be
open
and
credible.
For
reviews
and
reviewer
histories,
confidentiality
is
crucial
to
the
credibility
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
25,
Publication
date:
September
2008.
25:6
G.
Marchionini
and
should
be
preserved.
What
must
be
open
are
the
policies
associated
with
reviewers,
reviews,
and
the
persistent
histories
they
accrue.
REFERENCES
CURFMAN,
G,
MORRISSEY,
S,
ANNAS,
G.,
AND
DRAZEN,
J.
2008.
Peer
review
in
the
balance.
New
England
J.
Medicine
328,
21.
2276
2277.
GRUDIN,
J.
2004.
Crossing
the
divide.
ACM
Trans.
Comput.-Hum.
Interac.
11,
1,
1
25.
KENNEDY,
D.
2008.
Editorial.
Confidential
review--or
Not?
Science
319,
5866,
1009.
MARCHIONINI,
G.,
SARACEVIC,
T.,
CARROLL,
J.,
KRAFT,
D.,
HERSH,
W.,
MOTHE,
J.,
ZOBEL,
J.,
HERNON,
P.,
AND
SCHWARTZ,
C.
2007.
Reviewer
merits.
Inform.
Proces.
Manag.,
43,
1,
1
2.
MARCHIONINI,
G.
2008.
Rating
reviewers.
Letters
.
Science,
319,
no5868,
1335.
Received
June
2008;
accepted
June
2008
ACM
Transactions
on
Information
Systems,
Vol.
26,
No.
4,
Article
25,
Publication
date:
September
2008.
A
Statistical
Profile
of
the
N
a
m
e
d
Entity
Task
David
D.
P
a
l
m
e
r
and
David
S.
Day
The
MITRE
Corporation
202
Burlington
Road
Bedford,
MA
01730,
USA
palmer,day
mitre,org
Abstract
In
this
paper
we
present
a
statistical
profile
of
the
Named
Entity
task,
a
specific
information
extraction
task
for
which
corpora
in
several
languages
are
available.
Using
the
results
of
the
statistical
analysis,
we
propose
an
algorithm
for
lower
bound
estimation
for
Named
Entity
corpora
and
discuss
the
significance
of
the
cross-lingual
comparisons
provided
by
the
analysis.
Human
performance
on
the
NE
task
has
been
determined
to
be
quite
high,
with
F-measures
better
than
96
Sundheim,
1995b
.
Despite
the
fact
that
some
systems
in
recent
evaluations
have
performance
approaching
this
human
performance,
it
is
important
to
note
that
named-entity
recognition
is
by
no
means
a
solved
problem.
The
fact
that
existing
systems
perform
extremely
well
on
mixed-case
English
newswire
corpora
is
certainly
related
to
the
years
of
research
and
organized
evaluations
on
this
specific
task
in
this
language.
Although
performance
by
MUC-6
and
M
E
T
systems
is
encouraging,
it
is
not
clear
what
resources
are
required
to
adapt
systems
to
new
languages.
It
is
also
unknown
how
the
existing
high-scoring
systems
would
perform
on
less
well-behaved
texts,
such
as
single-case
texts,
non-newswire
texts,
or
texts
obtained
via
optical
character
recognition
OCR
.
There
has
been
little
discussion
of
the
linguistic
significance
of
performing
NE
recognition,
or
of
how
much
linguistic
knowledge
is
required
to
perform
well
on
such
an
evaluation.
However,
any
given
language
task
should
be
examined
carefully
to
establish
a
baseline
of
performance
which
should
be
attainable
by
any
system;
only
then
can
we
adequately
determine
the
significance
of
the
results
reported
on
that
task.
In
this
paper
we
give
the
results
of
an
analysis
of
NE
corpora
in
six
languages
from
the
point
of
view
of
a
system
with
no
knowledge
of
the
languages;
that
is,
we
performed
an
analysis
based
purely
on
the
strings
of
characters
composing
the
texts
and
the
named-entity
phrases.
The
performance
of
such
a
straw-man
system,
which
did
not
use
language-specific
lexicons
or
word
lists
or
even
information
about
tokenization
segmentation
or
part-of-speech,
can
serve
as
a
baseline
score
for
comparison
of
more
sophisticated
systems.
1
T
h
e
Named
Entity
task
There
is
currently
much
interest,
in
both
research
and
commercial
arenas,
in
natural
language
processing
systems
which
can
perform
multilingual
i
n
f
o
r
m
a
t
i
o
n
e
x
t
r
a
c
t
i
o
n
IE
,
the
task
of
automatically
identifying
the
various
aspects
of
a
text
that
are
of
interest
to
specific
users.
An
example
of
IE
is
the
N
a
m
e
d
E
n
t
i
t
y
NE
task,
which
has
become
established
as
the
important
first
step
in
many
other
IE
tasks,
providing
information
useful
for
coreference
and
template
filling.
Named
Entity
evaluation
began
as
a
part
of
recent
Message
Understanding
Conferences
MUC
,
whose
objective
was
to
standardize
the
evaluation
of
IE
tasks
Sundheim,
1995b
.
Several
organized
evaluations
have
been
held
to
determine
the
state-of-the-art
in
NE
systems,
and
there
are
commercial
systems
available.
The
goal
of
the
NE
task
is
to
automatically
identify
the
boundaries
of
a
variety
of
phrases
in
a
raw
text,
and
then
to
categorize
the
phrases
identified.
There
are
three
categories
of
named-entities
defined
by
the
guidelines:
TIMEX,
NUMEX,
and
ENAMEX.
T
I
M
E
X
phrases
are
temporal
expressions,
which
are
subdivided
into
date
expressions
April
7
and
time
expressions
noon
EST
.
NUMEX
phrases
are
numeric
expressions,
which
are
subdivided
into
percent
expressions
3.2
and
money
expressions
180
million
.
E
N
A
M
E
X
phrases
are
proper
names,
representing
references
in
a
text
to
persons
Jeffrey
H.
Birnbaum
,
locations
New
York
,
and
organizations
Northwest
Airlines
.
Evaluation
of
system
performance
for
the
NE
task
is
done
using
an
automatic
scoring
program
Chinchor,
1995
,
with
the
scores
based
on
two
measures
-
r
e
c
a
l
l
and
p
r
e
c
i
s
i
o
n
.
Recall
is
the
percent
of
the
correct
named-entities
that
the
system
identifies;
precision
is
the
percent
of
the
phrases
that
the
system
identifies
that
are
actually
correct
NE
phrases.
The
component
recall
and
precision
scores
are
then
used
to
calculate
a
balanced
F-measure
Rijsbergen,
1979
,
where
2
The
Corpora
F
2PR
P
R
.
The
definition
of
the
NE
task
we
discuss
in
this
paper
was
taken
from
the
guidelines
for
the
Sixth
Message
Understanding
Conferences
MUC-6
Sundheim,
1995a
and
the
recent
Multilingual
Entity
Task
MET,
May
1996
,
both
sponsored
by
the
T
I
P
S
T
E
R
program.
MUC-6
evaluated
English
NE
systems,
and
MET
evaluated
Spanish,
Japanese,
and
Chinese
NE
systems.
The
Spanish,
Japanese,
and
Chinese
corpora
we
analyzed
each
consisted
of
the
MET
training
documents;
similarly,
the
English
corpus
contains
60
Wall
Street
Journal
articles
prepared
for
the
MUC-6
dry-run
and
official
evaluation.
In
addition
to
the
four
corpora
available
from
the
recent
organized
NE
evaluations,
we
analyzed
similar-sized
French
and
190
Portuguese
corpora
1
which
were
prepared
according
to
the
MET
guidelines.
Table
1
shows
the
sources
for
the
corpora,
language,
as
well
as
a
breakdown
of
total
phrases
into
the
three
individual
categories.
Language
Chinese
English
French
Japanese
Portuguese
Spanish
NE
4454
2242
2321
2146
3839
3579
TIM
17.2
0
10.7
18.6
26.4
17.7
24.6
NUM
1.8
0
9.5
3.0
4.0
12.1
3.0
ENA
80.9
0
79.8
78.4
69.6
70.3
72.5
Chinese
English
French
Japanese
Portuguese
Spanish
Xinhua
Wall
Street
Journal
Le
Monde
Kyodo
Radiobras
Agence
France
Presse
China
USA
France
Japan
Brazil
France
Table
1:
Corpora
sources.
All
six
corpora
consisted
of
a
collection
of
newswire
articles,
and
none
of
the
articles
in
any
language
was
a
translation
of
an
article
in
another
language.
There
were
important
differences
in
the
makeup
of
these
individual
corpora
that
affected
this
analysis.
The
French
corpus,
for
example,
contained
a
wide
range
of
articles
from
a
single
issue
of
Le
Monde,
so
the
topics
of
the
articles
ranged
from
world
politics
to
the
Paris
fashion
scene.
The
articles
in
the
English
and
Spanish
corpora
were
specifically
selected
by
the
MUC-6
and
MET
evaluation
organizers
because
they
contained
references
to
press
conferences.
While
the
content
was
more
homogeneous
in
the
English
corpus,
the
articles
were
nevertheless
drawn
from
a
range
of
several
months
of
the
Wall
Street
Journal,
so
the
specific
topics
and
constituent
Named
Entities
were
very
diverse.
The
Chinese
Xinhua
corpus
was,
in
contrast,
extremely
homogeneous.
These
differences
demonstrate
a
number
of
difficulties
presented
by
corpora
in
different
languages.
In
order
to
estimate
the
complexity
of
the
NE
task,
we
first
determined
the
vocabulary
size
of
the
corpora
involved
i.e.
count
the
words
,
in
terms
of
individual
lexemes
of
the
language.
For
our
analysis
of
the
European-language
corpora,
we
considered
a
token
to
be
any
sequence
of
characters
delimited
by
white
space,
and
we
ignored
the
case
of
all
letters.
The
Japanese
corpus
was
segmented
using
N
E
W
J
U
M
A
N
,
the
Chinese
corpus
with
a
segmenter
made
available
by
New
Mexico
State
University.
This
segmentation
information
was
used
only
to
estimate
the
corpora
sizes
and
was
not
used
in
any
of
the
other
portions
of
our
analysis.
Since
many
words
occurred
frequently
within
a
corpus,
the
linguistic
type-token
distinction
was
important
to
our
analysis.
An
example
of
this
distinction
would
be
the
sentence
a
pound
costs
a
pound,
which
has
5
lexeme
tokens
and
3
lexeme
types.
The
ratio
of
lexeme
tokens
to
types,
which
can
be
thought
of
as
the
average
occurrence
of
each
lexeme,
is
shown
in
Table
2
with
the
vocabulary
sizes
of
the
six
corpora.
Lexeme
Tokens
34782
24797
35997
21484
42621
31991
Lexeme
Types
4584
5764
8691
3655
7756
7850
Token
Type
7.6
4.3
4.1
5.9
5.5
4.1
Table
3:
NE
phrases,
by
subcategory.
2.1
NUMEX
and
TIMEX
phrases
From
Table
3
we
see
that
T
I
M
E
X
and
NUMEX
phrases
together
composed
only
20-30
of
all
NE
phrases
in
each
language.
Furthermore,
these
phrases
were
the
easiest
to
recognize,
because
they
could
be
represented
by
very
few
simple
patterns.
Upon
inspection
of
the
corpora,
for
example,
we
were
able
to
represent
nearly
all
NUMEX
phrases
in
each
of
the
six
corpora
with
just
5
patterns.
2
Similarly,
given
a
simple
list
of
the
basic
temporal
phrase
words
for
a
language
months,
days
of
the
week,
seasons,
etc.
,
it
was
possible
to
construct
a
series
of
patterns
to
represent
most
of
the
T
I
M
E
X
phrases.
3
We
were
able
to
represent
at
least
95
of
all
T
I
M
E
X
in
each
language
in
similar
ways
with
just
a
few
patterns
less
than
30
per
language
,
constructed
in
a
few
hours.
Since
we
found
most
NUMEX
and
T
I
M
E
X
phrases
to
be
easy
to
recognize,
we
therefore
restricted
our
further
analysis
of
the
corpora
to
ENAMEX
phrases,
which
proved
to
be
significantly
more
complex.
2.2
ENAMEX
phrases
Table
4
shows
the
numbers
of
ENAMEX
phrases
tokens
contained
by
the
six
corpora.
The
average
occurrence
of
each
token
in
each
language
was
quite
low
much
lower
than
the
average
occurrence
of
each
lexeme
,
which
indicated
that
many
phrases
occurred
very
infrequently
in
the
corpus.
ENAMEX
Tokens
3605
1789
1820
1493
2698
2593
ENAMEX
Types
887
840
1085
614
981
1177
Token
Type
4.1
2.1
1.7
2.4
2.8
2.2
Language
Chinese
English
French
Japanese
Portuguese
Spanish
Language
Chinese
English
French
Japanese
Portuguese
Spanish
T
a
b
l
e
4:
C
o
r
p
o
r
a
size
by
E
N
A
M
E
X
phrases.
Nevertheless,
a
large
number
of
all
phrase
tokens
could
be
accounted
for
by
a
few
frequently-occurring
phrase
types.
For
example,
the
Chinese
corpus
contained
2156
total
LOCATION
phrases,
but
449
of
these
locations
20.8
could
be
2An
example
of
a
NUMEX
pattern
representing
a
Spanish
P
E
R
C
E
N
T
would
be
a
sequence
of
digits
followed
by
either
the
percent
sign
or
the
words
por
ciento
.
3An
example
of
a
NUMEX
pattern
representing
a
Spanish
DATE
would
be
the
name
of
a
month
or
its
abbreviation
followed
by
a
sequence
of
digits
the
day
,
optionally
followed
by
a
comma
and
another
sequence
of
digits
the
year
.
T
a
b
l
e
2:
C
o
r
p
o
r
a
size
b
y
lexeme.
Table
3
shows
the
total
number
of
NE
phrases
for
each
1The
French
corpus
was
prepared
by
Marc
Vilain;
the
Portuguese
corpus
was
prepared
by
Sasha
Caskey.
191
accounted
for
by
the
three
common
Chinese
words
for
China.
Figure
1
shows
a
graph
of
the
cumulative
percentage
of
all
phrases
of
the
corresponding
category
represented
by
the
z
most
frequently-occurring
phrases
of
that
type
in
the
given
language.
10o
6o
1o..!
o
0
50
10o
Figure
1:
G
r
a
p
h
of
the
cumulative
of
phrase
tokens
provided
by
of
phrase
types.
The
graph
shows
a
similar
shape
for
all
subcategories
of
ENAMEX
phrases
in
all
the
languages
investigated,
although
the
rate
of
increase
varies
slightly.
It
is
clear
from
the
classic
Zipfian
distribution
cf.
Zipf,
1932;
Zipf,
1949
shown
by
the
graph
that
a
significant
percentage
of
the
ENAMEX
phrase
tokens
could
be
represented
by
a
small
amount
of
frequentlyoccurring
phrase
types.
However,
Zipf
s
law
also
tells
us
that
a
non-trivial
percentage
of
the
phrases
those
in
the
tail
of
the
graph
are
very
infrequent,
most
likely
never
occurring
in
any
amount
of
training
data.
Unlike
the
distribution
of
the
overall
NE
phrases,
the
relative
proportion
of
constituent
ENAMEX
phrase
subcategories
PERSON,
LOCATION,
and
ORGANIZATION
varied
greatly
by
language.
The
breakdown
by
ENAMEX
phrase
subcategory
is
shown
in
Table
5.
Language
English
French
Japanese
Portuguese
Spanish
Org
20.2
o
Since
high
performance
on
training
texts
is
meaningless
i
f
a
system
performs
poorly
on
new,
unseen
texts,
we
estimated
the
performance
of
a
simple
memorization
algorithm
on
unseen
data.
For
our
simple
system,
the
answer
to
the
question
depended
on
the
v
o
c
a
b
u
l
a
r
y
t
r
a
n
s
f
e
r
r
a
t
e
of
the
corpus,
the
percentage
of
phrases
occurring
in
the
training
corpus
which
also
occurred
in
the
test
corpus.
To
measure
the
vocabulary
transfer
rate
for
the
six
corpora,
we
randomly
divided
each
corpus
into
a
training
set
and
a
test
set,
with
each
test
set
containing
about
450
ENAMEX
phrases,
and
each
training
set
containing
all
remaining
phrases.
We
then
examined
the
ENAMEX
phrases
in
the
training
set
to
determine
how
many
also
occurred
in
the
test
set.
The
results
of
this
experiment
showed
that,
to
a
certain
extent,
a
word
list
built
from
the
training
set
provided
reasonable
performance.
Just
as
some
frequent
phrase
types
comprised
a
large
percentage
of
the
phrase
tokens
within
a
corpus,
a
small
number
of
phrase
types
from
the
training
set
accounted
for
many
tokens
in
the
test
set.
As
shown
by
the
transfer
curve
for
the
six
languages
in
Figure
2,
the
transfer
rate
varied
dramatically
depending
on
the
language,
but
the
graph
has
the
same
shape
for
each,
even
though
the
six
corpora
contained
different
amounts
of
training
data
thus
the
lines
of
different
length
.
100.
90,
riO.
f
ohlnese
60.
frO.
40,
I0,
20,
I0.
0
0
I
200
I
I
,
..
sl
n
h
a
tm
jz
h
-
,
,
-
-
-
-
V,ach
Loc
59.8
o
56,2
33.8
39.2
14.5
30.0
40.8
Pers
20.0
o
29.2
38.1
20.0
I
I
400
600
eO0
000
49.9
28.6
19.5
43.5
30.1
27.9
Figure
2:
G
r
a
p
h
of
the
cumulative
test
phrase
tokens
covered
by
training
phrase
types.
In
each
language,
the
transfer
rate
for
the
most
frequent
phrase
types
the
steep
part
of
the
graph
was
quite
high;
however,
the
graph
rapidly
peaks
and
leaves
a
large
percentage
of
the
phrases
uncovered
by
the
training
phrases.
The
remaining
uncovered
phrases
can
only
be
recognized
by
means
other
than
memorization,
such
as
by
examining
contextual
clues.
Table
6
shows
the
transfer
rates
of
phrase
tokens.
The
accuracy
of
the
pure
memorization
can
be
reduced
by
two
forms
of
ambiguity.
Phrases
or
parts
of
phrases
can
occur
within
two
or
more
named-entity
categories,
such
as
the
string
Boston,
which
by
itself
is
a
location
but
within
Boston
Red
Sox
is
an
organization.
In
most
cases
this
ambiguity
can
be
resolved
using
a
simple
longest-match
heuristic.
Another
source
of
ambiguity
occurs
when
a
string
can
occur
both
as
a
Table
5:
E
N
A
M
E
X
phrases
by
subcategory.
The
significance
of
this
result
is
that
each
ENAMEX
phrase
subcategory
had
to
be
treated
as
equivalent.
It
was
not
possible
to
focus
on
a
particular
subcategory
to
obtain
a
consistently
high
score.
In
other
words,
a
strategy
that
focuses
on
locations
would
do
well
on
the
Chinese
corpus
where
locations
comprise
59.8
of
the
ENAMEX
phrases,
but
would
do
poorly
on
the
English
corpus,
where
locations
are
only
14.5
of
the
ENAMEX.
3
Training
and
ambiguity
A
logical
question
to
pose
is,
How
well
can
our
system
perform
if
it
simply
memorizes
the
phrases
in
the
training
texts?
192
Overall
Language
Chinese
English
French
Japanese
Portuguese
Spanish
ENAMEX
73.2
21.2
23.6
0
59.2
Org
Loc
Pers
46.9
87.1
42.6
17.7
42.7
13.3
13.4
45.9
11.2
56.2
56.4
49.8
72.7
57.4
71.4
37.5
47.9
13.7
61.3
48.1
Language
Chinese
Xinhua
English
WSJ
French
Le
Monde
Japanese
Kyodo
Portuguese
Radiobras
Spanish
A
F
P
Lower
Bound
71.8
38.4
34.5
70.1
71.3
59.3
Table
6:
Vocabulary
transfer
tokens
.
NE
phrase
and
as
a
non-phrase,
such
as
Apple,
which
would
sometimes
refer
to
the
computer
company
and
thus
be
tagged
an
organization
and
sometimes
refer
to
the
fruit
and
thus
not
be
tagged
at
all
.
Such
cases,
although
infrequent,
would
result
in
precision
errors
which
we
do
not
factor
into
the
following
estimation
of
a
recall
lower
bound.
T
a
b
l
e
7:
E
s
t
i
m
a
t
e
d
lower
b
o
u
n
d
s
.
matching,
a
large
percentage
of
the
E
N
A
M
E
X
phrases
could
be
codified
given
an
adequate
analysis
of
the
phrasal
contexts
in
the
training
documents.
Furthermore,
lists
of
titles,
geographic
units,
and
corporate
designators
would
assist
this
contextual
analysis
and
improve
the
expected
baseline.
Indeed,
such
simple
strategies
drive
most
current
NE
systems.
4
E
s
t
i
m
a
t
i
n
g
a
lower
b
o
u
n
d
5
Discussion
Given
the
above
statistical
analysis,
we
estimated
a
baseline
score
for
our
straw-man
algorithm
on
the
NE
task,
a
score
which
should
easily
be
attainable
by
any
system
attempting
to
perform
the
task.
First,
we
estimated
that
any
system
should
be
able
to
recognize
a
large
percentage
of
NUMEX
and
T
I
M
E
X
phrases;
our
experience
indicates
that
95
is
possible
due
to
the
small
number
of
patterns
which
compose
most
of
these
phrases.
In
order
to
estimate
a
lower
bound
for
ENAMEX
recognition,
we
relied
on
the
transfer
graph
in
Figure
2.
It
is
clear
from
the
graph
that
the
contribution
of
the
training
d
a
t
a
has
leveled
off
in
each
language
by
the
time
the
number
of
training
types
is
roughly
equal
to
the
size
of
the
test
d
a
t
a
450
in
this
case
.
Selecting
this
point
on
the
graph
allowed
us
to
directly
compare
memorization
performance
for
the
six
languages.
An
ideal
memorization-based
algorithm
would
be
able
to
recognize
phrases
according
to
the
transfer
rate
corresponding
to
this
amount
of
training
data.
Our
lower
bound
formula
would
thus
be
N
vMSX
NTIM
X
E
A
E
TENAM
X
NNMX
where
a
0.95
in
our
experience
N
at
Percentage
of
NE
phrases
represented
by
category
from
Table
3
TENAMEX
ENAMEX
transfer
rate
from
Figure
2
The
results
of
this
analysis
indicate
that
it
is
possible
to
perform
much
of
the
task
of
named-entity
recognition
with
a
very
simple
analysis
of
the
strings
composing
the
NE
phrases;
even
more
is
possible
with
an
additional
inspection
of
the
common
phrasal
contexts.
The
underlying
principle
is
Zipf
s
Law;
due
to
the
prevalence
of
very
frequent
phenomena,
a
little
effort
goes
a
long
way
and
very
high
scores
can
be
achieved
directly
from
the
training
data.
Yet
according
to
the
same
Law
that
gives
us
that
initial
high
score,
incremental
advances
above
the
baseline
can
be
arduous
and
very
language
specific.
Such
improvement
can
most
certainly
only
be
achieved
with
a
certain
amount
of
well-placed
linguisticintuition.
The
analysis
also
demonstrated
the
large
differences
in
languages
for
the
N
E
task,
suggesting
that
we
need
to
not
only
examine
the
overall
score
but
also
the
ability
to
surpass
the
limitations
of
word
lists,
especially
since
extensive
lists
axe
available
in
very
few
languages.
It
is
particularly
important
to
evaluate
system
performance
beyond
a
lower
bound,
such
as
that
proposed
in
Section
4.
Since
the
baseline
scores
will
differ
for
different
languages
and
corpora,
scores
for
different
corpora
that
appear
equal
m
a
y
not
necessarily
be
comparable.
References
Nancy
Chinchor.
Maryland.
Steven
Maiorano
and
Terry
Wilson.
1996.
Multilingu
l
Entity
Task
M
E
T
:
Japanese
Results.
In
Proceedings
of
T
I
P
S
T
E
R
Text
Program
Phase
1I
,
May.
Roberta
Merchant
and
Mary
Ellen
Okurowski.
1996.
T
h
e
MultilinguM
Entity
Task
M
E
T
Overview.
In
Proceedings
o
T
I
P
S
T
E
R
Text
Program
Phase
11
,
May.
MET.
May
1996.
Task
definition.
MultilinguM
Entity
Task.
C.
J.
Van
P
ijsbergen.
1979.
Information
Retrieval.
n
u
t
t
e
r
w
o
r
t
h
s
,
London.
Beth
Sundheim.
1995a.
MUG6
named
entity
task
definition,
Version
2.1.
In
1995.
MUC-5
evaluation
metrics.
In
Proceedings
of
the
Fifth
Message
Understanding
Conference
MUCS
,
pages
69-78,
Baltimore,
The
resulting
lower
bound
scores,
shown
in
Table
7,
were
surprisingly
high,
indicating
that
a
very
simple
NE
system
could
easily
achieve
a
recall
above
70
for
some
languages.
The
range
of
lower
bound
scores
can
partly
be
attributed
to
the
differences
in
corpus
makeup
discussed
in
Section
3,
but
the
range
also
illustrates
the
large
score
differences
which
are
possible
from
one
corpus
to
the
next.
The
upper
bounds
of
memorization
algorithms
implied
by
the
preceding
analysis
do
not
require
that
a
deeper
understanding
of
the
linguistic
phenomena
of
a
target
language
is
necessary
to
generalize
NE
recognition
in
unseen
test
data.
Contextual
clues
can
improve
the
expected
score
of
a
baseline
system
without
requiring
extensive
linguistic
knowledge.
Just
as
most
of
the
T
I
M
E
X
and
NUMEX
phrases
in
any
language
can
be
recognized
upon
inspection
using
simple
pattern
Proceedings
of
the
Sizth
Message
Understanding
Conference
MUC6
.
Beth
M.
Sundheim.
1995b.
Overview
of
results
of
the
MUC-6
evaluation.
In
Proceedings
o
the
Si
th
Message
Understanding
Conference
MUC6
.
G.
Zipf.
1932.
Selected
Studies
o
the
Principle
of
Relative
Frequency
in
Language.
Harvard
University
Press,
Cambridge,
MA.
G.
Zipf.
1949.
Human
Behavior
and
the
principle
of
least
effort.
Hafner,
New
York.
193
A
Computer
Readability
Formula
of
Japanese
Texts
for
Machine
Scoring
TATEISI
Yuka,
ONO
Yoshihiko,
YAMADA
ltisao
Departmentof
InformationScience,Facultyof
Science,Universityof
Tokyo,
7-3-1,
Hongo,Bunkyo-ku,Tokyo,
113,Japan
Abstract
A
readability
formula
is
obtained
that
can
be
used
by
computer
programs
for
style
checking
of
Japanese
texts
and
need
not
syntactic
or
semantic
information.
The
formula
is
derived
as
a
linear
combination
of
tile
surface
characteristics
of
the
text
that
are
related
to
its
readability:
1
the
average
number
of
characters
per
sentence,
2
for
each
type
of
characters
Roman
alphabets,
kanzis,
hiraganas,
katakanas
,
relative
frequencies
of
rims
maximal
swings
that
,:onsists
only
of
that
type
of
characters,
3
the
average
number
of
characters
per
each
type
of
runs,
and
4
tooten
comma
to
kuten
period
ratio.
To
find
the
proper
weighting,
principal
component
analysis
PCA
was
appliedto
these
characteristics
taken
from
77
sample
texts.
We
have
found
a
component
which
is
related
to
the
readability.
Its
scores
match
to
the
empirical
knowledges
of
reading
ease.
We
have
also
obtained
experimental
confirmation
that
the
component
is
an
adequate
measure
for
stylistic
ease
of
reading,
by
the
cloze
procedure
and
by
the
examination
on
the
average
lime
taken
to
fill
out
one
blank
of
the
cloze
texts.
1o
Introduction
This
study
aims
to
obtain
a
readability
formula
that
can
be
used
by
computer
programs
for
style
checking
of
Japanese
texts.
A
readability
formula
predicts
the
difficulty
of
a
document
that
may
result
from
its
writing
style,
but
not
from
its
content,
organization,
or
format.
A
readability
index
is
calculated
from
the
measures
of
surface
characteristics
of
the
document
that
are
thought
to
indicate
the
stylistic
difficulty
without
an
attempt
to
parse
sentences
or
to
consult
a
large
dictionary.
Many
of
the
readability
formulae
for
English,
for
example,
Flesch
s
Reading
Ease
Score
Fleseh
1949
and
Automated
Readability
Index
Smith
1970
,
use
the
average
length
number
of
syllables
or
lettors
of
words
and
the
average
number
of
words
in
sentences
in
a
document
for
calculating
the
readability
index.
Word
length
is
a
measure
of
the
lexical
difficulty,
i.e.,
difficulty
of
the
vocabulary
used
in
the
document.
Sentence
length
is
a
measure
of
the
Syntactic
difficulty
or
complexity
of
the
sentence.
While
reao
dability
indices
are
derived
from
simple
formulae,
they
predict
reasonably
well
the
difficulty
of
a
document.
This
is
because
the
sentence
length
and
the
word
length
are
highly
correlated
with
features
such
as
the
complexity
of
the
sentence
and
the
difficulty
of
the
word,
respectively.
Existing
scoring
methods
for
Japanese,
such
as
the
one
proposed
b
y
M
o
r
i
o
k
a
1958
or
Yasumoto
1983
,
use
the
sentenco
length
measured
in
letters
instead
of
words
and
the
percentage
of
kanzis
Chinese
characters
,
the
latter
used
for
estimating
the
difficulty
of
the
vocabulary.
Both
rate
the
average
number
of
letters
per
sentence
and
the
percentage
of
kanzis
in
the
text
independen
ly
and
do
not
combine
the
two
factors
into
a
single
index.
A
te
t
with
longer
sentences
is
estimated
as
difficult,
and
a
text
with
more
kanzis
is
also
estimated
as
difficult.
Morioka,
who
surveyed
on
school
textbooks,
showed
that
the
upper
grade
textbooks
contain
longer
sentences
on
the
average
and
more
kanzi.
Yasumoto
states
that
documents
with
more
kanzi
are
less
readable
even
for
adults,
for
the
following
reason.
Kanzi
are
logograms,
one
roughly
corresponding
to
a
word.
Documents
using
more
kanzis,
therefore,
apt
to
include
more
different
words
and
should
demand
more
reading
skill.
A
problem
of
rating
the
sentence
length
and
the
percentage
of
kanzi
independently
is
that
these
two
may
yield
an
inconsistent
rating.
Generally,
a
sentence
becomes
longer
if
its
kanzis
are
rewritten
in
kanas.
Thus
sentence
lengths
depend
on
representations.
There
seems
to
have
been
no
attempt
on
combining
the
factors
of
sentence
length
and
the
proportion
of
kanzi.
On
the
other
hand,
no
rationale
is
given
for
the
separate
measurements.
It
is
possible
to
derive
a
single
index
that
can
assess
readability
of
Japanese
text.
Sakamoto
1967
proposed
a
method
of
scoring
the
relative
difficulty
of
children
s
books
to
match
the
reading
skill
of
the
intended
readers.
His
method
consists
of
three
independent
ratings;
1
the
proportion
of
fundamental
words
based
on
Sakamoto
1958
,
2
the
proportion
of
sentences
that
are
made
of
more
than
10
words,
and
3
the
proportion
of
kanzi.
However,
Sakamoto
s
method
introduce
the
problem
of
measuring
sentence
length
in
words
in
place
of
tile
conflict
between
sentence
length
and
representation.
Using
word
count
o1
word
length
as
an
estimator
of
readability
is
not
practical
in
the
case
of
Japanese.
Since
Japanese
does
not
use
word
segmentations
in
nomaal
writing,
dividing
sentences
into
words
needs
parsing
and
consulting
dictionary.
Thus,
a
scoring
method
based
on
words,
such
as
Sakamoto
s,
is
costly.
This
is
especially
so
when
scoring
is
done
by
a
computer,
because
extra
devices
such
as
parsers,
a
large
dictionary,
and,
sometimes,
semantic
analyzers
are
required
for
word
segmentation
alone.
Another
problem
with
the
traditional
scoring
methods
is
that
they
have
ignored
katakana,
which
are
used
to
represent
foreign
words.
Recent
documents,
especially
scientific
and
technical
ones,
use
a
lot
of
foreign
words.
Watanabe
1983
reports
that,
in
a
year
s
issues
of
the
Jonrual
of
lnfonnation
Processing
Society
of
Japan,
Vol.
17,
about
an
eighth
among
the
characters
used
is
katakana.
Satake
1982
surveyed
the
article
of
magazines
published
today
and
found
that
the
ratio
of
katakana
ranged
from
4.44
to
13.75
percent.
Thus
percentage
of
katakana
is
not
negligible
in
scoring
today
s
documents.
Katakana
words
mean
imported
foreign
words,
old
and
new,
which
are
often
unfamiliar
to
readers.
Yet
existing
measures
take
into
account
only
kanzi
and
are
insufficient
to
score
the
today
s
technically
oriented
documents.
2.
Factors
of
Readability
We
have
chosen
the
following
four
surface
characteristics
as
factors
of
readability:
1
relative
frequency
of
characters
for
each
type
of
characters,
2
the
length
of
a
run
maximal
string
that
consists
of
one
type
of
characters
,.
3
the
length
of
a
sentence,
and
4
the
number
of
tooten
s
commas
per
sentence.
The
former
two
are
related
to
the
difficulty
of
vocabulary
in
a
document;
the
latter
two
are
related
to
the
complexity
of
sentences
in
a
document.
649
Character
Frequencies
The
most
common
Japanese
writing
system
is
based
on
the
mixture
of
kanzis,
kanas
hiraganas
and
katakanas
,
the
Roman
alphabets,
Arabic
numerals,
and
some
other
alphabets
and
symbols.
Almost
al!
normal
writing
is
a
mixture
of
kanzis,
hiraganas,
and
katakanas
and
others
.
Frequencies
of
types
of
characters
in
a
Japanese
text
are
known
to
affect
its
readability
at
least
in
the
following
manner:
Kanzi,
as
mentioned
before,
are
considered
to
make
texts
difficult.
Since
katakana
and
alphabets
are
used
for
foreign
words,
high
frequencies
of
these
characters
indicate
that
the
text
contain
many
unfamiliar
words.
Hiragana
are
used
to
represent
the
rest
of
the
text
and
more
of
them
are
considered
to
make
texts
easier.
There
is
no
rigid
orthography
for
Japanese.
Nevertheless,
the
way
an
adult
Japanese
spells
out
a
sentence
in
usual
writing
is
roughly
fixed.
Kanzis
are
used
for
nouns
and
for
the
root
parts
of
verbs,
adjectives,
adverbs,
and
the
like.
Hiraganas
are
used
to
write
inflections
and
other
grammatical
parts
of
sentences,
and
katakanas
are
used
mainly
for
the
transcription
of
foreign
words.
So
in
passages
written
in
the
common
way,
the
use
of
types
of
characters,
i.e.,
kanzi,
hiragana,
katakana,
etc.,
reflects
the
use
of
vocabulary
and
can
be
an
indicator
of
the
difficulty
of
the
passage.
It
is
possible
to
write
the
words
usually
written
in
kanzi
in
hiragana.
However,
psychological
experiments
such
as
the
ones
conducted
by
Kitao
1960
or
Hirose
1983
a
reader
finds
it
difficult
to
read
the
texts
represented
in
the
way
unfamiliar
to
the
reader.
In
Kitao
s
experiment,
subjects
took
less
time
to
read
and
recognize
the
word
or
the
sentence
written
in
a
common
way
than
written
solely
in
hiragana.
In
Hirose
s
experiment,
the
words
usually
written
in
kanzi
are
harder
to
recognize
than
the
words
usually
written
in
kana
when
both
type
of
words
are
written
in
kana.
Both
results
show
that
words
or
sentences
in
the
representation
more
familiar
to
a
reader
are
more
readable
than
those
in
less
familiar
representation.
Runs
In
the
ordinary
representation,
a
boundary
of
the
types
of
characters
corresponds
to
the
boundary
of
words
or
smaller
grammatical
parts
thereof.
That
is,
a
series
of
letters
of
the
same
type
in
the
text,
bounded
by
other
character
types
corresponds
to
a
word
or
a
smaller
grammatical
part.
We
will
call
such
a
series
a
run,
i.e.,
a
run
is
a
maximal
string
that
consists
of
only
one
type
of
characters.
It
is
not
a
grammatical
unit.
Usually,
a
run
corresponds
to
one
or
more
words.
A
verb
or
an
adjective
is
often
found
across
two
runs.
Such
a
word
norulally
has
its
root
part
written
in
kanzi
and
its
inflection
part
in
hiragana.
As
the
boundary
of
runs
roughly
correspond
to
the
boundary
of
words,
the
different
graphic
appearance
of
kanzi
and
kana
letters
helps
a
reader
to
parse
a
sentence.
Hence,
long
runs,
when
they
happen,
hide
the
word
boundaries
and
makes
a
sentence
less
readable.
L
o
n
g
kanzi
runs
give
another
problem
to
the
readability.
Kango.
can
be
formed
into
a
compound
word
simply
by
concatenating
two
or
more
of
them
successively.
The
meaning
of
the
new
word
is
formed
by
the
meanings
of
its
elements.
However,
how
each
element
is
related
to
each
other
in
the
compound
word
is
not
clear
from
mere
concatenation.
A
reader
must
pragmatically
see
the
relation.
Therefore,
it
is
often
the
case
that
the
meaning
of
a
compound
kango
is
ambiguous.
For
example,
siken-ki
can
be
read
as
siken-suru-kikai
testing
machine
or
as
siken-sareru-kikai
machine
to
be
tested
;
rinzi-kyouiku-singi-kai
meaning
rinzi-nikyouiku-ni-tuite-singi-suru-kai
an
ad
hoe
council
to
deliberate
on
education
can
be
read
as
rinzi-no-kyouiku-ni-tuite-singi-suru-kai
a
council
to
deliberate
on
an
ad
hoe
education
.
650
It
is
unlikely
that
there
may
be
any
good
theory
possible
about
the
relationship
between
run
frequencies
and
readability.
Nevertheless,
the
run
frequencies
may
be
used
in
a
similar
manner
as
character
frequencies.
In
a
study
preceding
this
Tateisi
1987
we
found
that
the
run
frequencies
are
correlated
with
the
frequencies
of
the
character
of
corresponding
types
0.6
r
0.9,
depending
on
character
types
and
a
unit
of
run
is
sufficient
to
obtain
the
information
otherwise
supplied
by
both
characters
and
runs.
Sentence
Length
The
length
of
sentences
is
a
known
factor
of
readability
as
Morioka
1958
and
other
surveys
show.
In
Japanese,
as
in
other
languages,
long
sentences
tend
to
have
complicated
structures.
Sentence
length
can
be
measured
in
the
number
of
characters
it
contains.
Though
Sakamoto
s
survey
of
children
s
textbook
Sakamoto
1963
shows
that
the
number
of
words
per
sentences
is
a
more
accurate
indicator
of
the
grade
level
than
the
number
of
characters,
it
also
shows
that
the
two
are
in
good
proportion,
the
correlation
coefficient
being
1.00.
Punctuation
Tootens,
like
commas,
are
put
at
the
end
of
a
phrase.
The
number
of
tootens
per
sentence
corresponds
to
the
number
of
phrases
per
sentence.
Hayasi
1959
found
that
junior
high
school
sU:dents
and
senior
high
students
understood
the
text
more
precisely
if
modifying
phrases
are
separated
and
made
into
independent
sentences.
Following
this
result,
a
sentence
with
smaller
number
of
phrase
is
easier
to
understand.
Kozuru
1987
found
that
the
average
number
of
tootens
in
a
sentence
increases
with
student
s
grade
level.
These
findings
indicate
that
the
number
of
tootens
in
a
sentence
is
greater
in
more
difficult-to-read
texts.
Thus,
the
number
of
tootens
is
a
factor
of
readability.
-
3.
The
Method
of
Analysis
We
shall
first
extract
several
numerical
characteristics
of
style
from
texts
and
then
derive
a
readability
formula
as
a
linear
combination
of
the
values
of
those
characteristics.
A
nurflerical
index
is
only
a
rough
scale
of
readability.
It
should
be
calculated
with
simple
devices
and
methods.
We
use
character
as
the
unit
of
measuring
length
for
the
sake
of
simple
calculation.
Several
surface
characteristics
are
extracted
from
the
materials.
Difference
of
the
characteristics
among
materials
consists
of
several
factors.
It
may
be
factored
into
variation
of
the
topic
area
of
the
texts,
and
the
variation
of
style.
Style
may
differ
by
the
writer
or
by
the
intentions
of
the
text.
Introductory
textbooks
should
be
written
easier
than
technical
papers
intended
for
experts
and
the
authors
will
be
careful
not
to
make
it
difficult
to
read.
Thus
they
will
be
written
in
a
style
easier
to
read
than
the
style
of
technical
papers.
Translations
tend
to
have
a
particular
style,
highly
dependent
on
the
syntax
of
the
original
language.
The
particular
style
of
translations
is
often
found
awkward
as
Japanese
and
less
readable.
The
distinctive
feature
of
the
texts
with
different
intentions
can
be
used
as
a
criteria
of
assessing
readability.
To
find
the
distinctive
feature
of
texts
from
the
surface
characteristics,
the
principal
component
analysis
PCA
extracts
factors
of
variance
of
the
characteristics.
We
will
then
examine
tile
components,
by
comparing
component
scores
for
the
materials
with
the
empirical
knowledges
of
readability.
In
this
way
we
shall
choose
a
component
relevant
to
the
stylistic
readability.
A
principal
component
is
a
linear
combination
of
the
variables.
The
formula
which
computes
the
component
can
be
used
as
a
readability
formula.
Variables
We
have
chosen
the
ten
variables
that
represent
the
four
fac
tots
of
readal
ility:
1
for
each
type
of
characters
Roman
alphabets,
kanzis,
hiraganas,
katakanas
,
relative
frequency
of
runs
maximal
strings
that
consists
only
of
that
type
of
characters,
the
avelage
number
of
letters
per
each
type
of
runs,
the
avelage
number
of
letters
per
sentence,
and
tooten
lo
k
u
t
e
n
ratio.
The
following
are
observed
for
the
second
component.
2-1
2-2
This
component
separates
the
texts
with
long
sentences
and
long
kanzi
runs
from
the
other
texts.
The
component
score
agrees
with
human
judgement
about
easy
difficult
texts.
It
is
high
on
tile
texts
judged
easy
and
low
on
the
texts
judged
difficult.
The
second
component
score
shows
the
distinction
more
clearly
than
the
first
or
the
third.
Introductory
textbooks
have
generally
higher
scores
than
papers.
Again,
the
second
component
score
shows
the
distinction
more
clearly
than
tile
first
or
file
third.
2
3
4
2-3
Sentence
lengfl
is
measured
in
the
number
of
characters
between
two
adjacent
sentence-ending
marks
kuten,
exclamation
marks,
and
question
marks
.
Kuten,
unlike
period,
is
placed
only
at
the
end
of
a
sentence,
not
as
an
indicator
of
abbreviations.
Therefore,
the
end
of
a
sentence
is
ahnost
always
detected
by
detecting
kuten,
although
the
end
quotation
embedded
in
a
sentence
is
also
counted
as
the
end
of
a
sentence.
Since
long
sentences
and
long
kanzi
runs
make
texts
less
readable
as
stated
before,
2-1
indicates
that
the
second
component
can
be
an
indicator
of
readability.
2-2
and
2-3
also
indicates
that
the
second
component
is
related
to
readability.
The
third
component
shows
a
difference
of
proportions
of
katakana
and
kanzi.
From
table
4-1
we
can
find
that
the
variables
on
kanzi
have
positive
loadings
and
the
variables
on
hiragana
and
katakana
have
negative
loadings
on
the
component.
Thus,
the
component
shows
the
proportion
of
kanzi,
in
the
way
that
it
increases
with
texts
with
more
kanzi.
Samples
We
must
compare
the
readability
anaong
the
texts
written
in
tile
common
way,
that
is,
the
texts
written
by
authors
as
they
are.
For
exampl
,
the
textbooks
for
elementary
school
children
are
inadequate.
This
is
because
those
textbooks
are
written
in
an
unusual
way.
They
use
hiragana
where
most
adults
use
kanzi,
transcribing
the
kanzi
the
readers
are
not
expected
to
learn
yet.
We
will
therefore
take
the
documents
written
by
adults
for
adults
as
materials
of
the
analysis.
Seventy-seven
77
documents
were
selected
as
sample
texts
to
extract
the
data
from.
Seventy
of
the
samples
are
machinereadable
documents
that
were
stored
in
our
laboratory.
They
are
technical
papers,
textbooks
for
collage
students,
and
translations
of
computer
science
materials,
written
by
13
authors.
Seven
of
the
samples
are
included
as
indicators
for
reading
ease.
Five
of
these
indicators
a
e
text
judged
as
easy.
Three
of
therft
are
taken
from
the
books
on
technical
writing;
two
are
taken
from
essays
for
general
readers.
They
are
considered
to
be
easier
than
the
papers
or
textbooks
for
scientists.
The
remaining
two
are
the
text
judged
as
difficult.
One
of
them
is
a
decision
on
the
case
of
an
infringement
of
copyright
of
a
computer
program;
the
other
is
a
juridical
paper
about
copyright
and
new
media
such
as
magnetic
tapes.
Juridical
texts
are
empirically
known
as
hard
to
read.
Tables
figures,
references,
and
expressions
which
are
displayed
iitdependently
from
the
passage
are
deleted
from
tile
samples.
5.
Principal
Component
Scores
and
Style
We
have
observed
tile
following
phenomena
on
the
second
component.
Improvement
and
Principal
Component
Scores
Five
of
the
sample
texts
are
chapters
indicated
T
in
the
figure
4-1
of
the
final
versions
of
the
translation
of
an
English
paper
by
different
translators.
Their
component
scores
were
compared
with
those
of
tile
respective
draft
versions.
The
drafts
are
not
among
the
samples.
The
first
three
component
scores
of
the
final
manuscripts
were
uniformly
higher
than
those
of
drafts,
i.e.,
tile
scores
became
higher
with
the
improvement
of
their
style.
The
differences
between
the
final
versions
and
the
respective
draft
versions
are
shown
in
table
5-1.
The
mean
difference
of
the
second
eomponent
is
found
greater
than
that
of
the
first
at
the
5
percent
significance
17
0.044
and
greater
than
that
of
the
third
at
the
10
percent
significance
but
not
at
the
5
percent
significance
19
0.098
.
Thus,
the
difference
of
the
second
component
is
greater
than
the
other
two.
This
agrees
with
the
observations
on
the
distribution
of
texts,
that
is,
easier-to-read
texts
have
higher
second
component
score
than
difficult
ones,
since
a
text
becomes
easier
to
read
after
improvement
in
general.
Frequencies
of
Passive
Forms
Table
5-2
below
shows
tile
correlation
between
the
component
scores
and
the
frequencies
of
passive.
Passive
forms
are
counted
using
the
pattern
matching
method
proposed
by
Ushijima
1987
.
The
count
is
divided
by
the
number
of
the
kutens
in
a
sample,
yielding
the
ratio
to
passives
per
sentences,
o1
sentenceendings.
Japanese
passive
forms
are
also
used
for
potentials.
For
example,
m
i
r
a
r
e
r
u
may
mean
either
be
seen
passive
o1
can
see
potential
and
taberareru
may
have
one
of
three
meanings:
b
e
eaten,
can
eat,
and
can
b
e
eaten.
Thus,
frequent
use
of
passives
tend
to
make
a
doc
ment
vague
and
less
readable.
The
second
component
scores
have
a
higher
correlation
than
other
component
scores.
Note
that
the
correlation
coefficient
is
negative.
This
agrees
with
the
observation
that
the
second
component
score
is
lower
on
difficult-to-read
texts
and
that
the
frequency
of
passives
is
higher
on
such
texts.
Figure
5-1
shows
the
plot
of
the
second
component
scores
and
the
frequencies
of
passives
per
1000
sentences.
The
line
in
the
figure
is
the
regression
line.
651
4.
Result
of
the
Principal
Component
Analysis
PCA
The
plincipal
component
analysis
is
done
by
S
routines
Becker
1984
on
Vax
8600
at
the
Computer
Center
of
the
University
of
Tokyo.
The
components
and
the
loadings
of
each
variables
are
shown
in
table
4--1.
The
first
three
components
eigenvalue
1
are
examined.
Total
variance
explained
by
these
components
is
70
.
Figure
4-1
shows
the
s,:atter
plot
of
sample
texts.
The
letter
i
designate
introductory
textbooks,
m
magazine
articles
other
than
technical
papers,
p
tt
chnical
papers,
t
and
T
designate
translations
from
English
papers,
and
D
and
E
designate
the
difficult
and
easy
indicators,
respectively.
The
following
are
observed
for
the
first
component.
1-1
This
component
reflects
the
occurrences
of
alphabets;
separates
the
texts
with
little
alphabetic
content
and
the
text
aburtdant
with
alphabetic
content.
The
texts
with
many
equations
and
abbreviations
have
high
scor,
s
on
this
component.
The
sc.0re
on
this
component
shows
the
area
of
topic.
1-2
6.
The
Derived
Formula
The
results
above
support
the
adequacy
of
the
second
component
as
a
scale
of
readability.
To
summarize,
the
second
component
score
may
be
used
as
a
readability
index
because
of
the
following
facts.
1
The
component
score
agrees
with
human
judgement
about
easy
difficult
texts.
Easier-to-read
texts
yield
higher
valued
scores.
2
Introductory
materials
give
higher
scores
than
technical
papers,
3
The
score
increases
as
the
result
of
improvement
by
editing
of
texts,
4
The
frequencies
of
passive
forms
have
a
negative
correlation
-0.53
with
the
component
score.
The
first
component
and
the
third
component
do
not
possess
all
of
these
properties.
Thus
the
second
is
a
better
measure
of
readability
than
the
first
or
the
third.
The
second
component
score
is
transformed
so
that
the
mean
on
those
77
samples
equals
50,
the
standard
deviation
equals
10,
and
let
the
value
be
higher
on
easy
texts.
This
yields
formula,
RS
0.06
x
p
a
0.25
x
p
h
-
0.19
x
p
c
-
0.61
x
p
k
1
the
phase
of
reading
the
incomplete
text
and
understand
the
content
of
the
passage,
2
the
phase
of
surmising
what
is
missing
as
a
notion
,
3
the
phase
of
choosing
the
proper
word
to
supplement,
and
4
the
phase
of
writing
down
that
word.
The
time
for
writing
down
a
word
is
fairly
constant,
unless
the
word
contains
extremely
complicated
kanzi.
Therefore,
the
variation
of
time
from
text
to
text
is
the
variation
of
time
for
the
phase
1
,
2
,
and
3
,
i.e.,
understanding
the
passage,
surmising
the
missing
notion,
and
choosing
the
proper
word.
The
text
which
is
stylistically
difficult
takes
more
time
in
the
phase
1
.
Thus
the
difficult-to-read
texts
must
require
more
time
filling
out
blanks
than
easy-to-read
ones.
Materials
The
materials
of
the
experiment,
denoted
by
p
1
through
p
6,
were
taken
from
the
six
sample
papers
among
the
77
used
for
the
PCA.
Each
was
about
500
characters
in
size.
Three
of
them
p
1,
p
2,
p
3
had
high
RS
s
RS
50
and
the
three
p
4,
p
5,
a
6
had
low
RS
s
RS
50
.
Every
eighth
word
of
each
text
was
blanked
out,
i.e.,
the
proportion
of
blanked
out
words
to
the
whole
words
was
12.5
.
Ten
underscore
characters
were
put
where
a
word
was
blanked
out.
Among
several
different
definitions
of
Japanese
words
used,
the
one
which
gives
the
smallest
unit
was
taken.
The
materials
were
printed
out
on
a
sheet
of
A4-sized
paper,
one
material
per
paper.
Twenty-eight
subjects
25
undergraduate
students
and
3
graduate
students
participated
in
the
experiment.
Each
subject
was
assigned
three
materials
fi
m
p
1
-
p
6
selected
randomly,
so
that
the
half
of
the
subjects
were
assigned
each
material.
The
subjects
were
required
to
fill
blanks
underscored
parts
with
words
they
thought
most
appropriate
to
the
context,
taking
as
much
time
they
need.
The
subjects
were
told
that
each
word-unit
was
smallest
possible,
and
therefore
the
deleted
part
might
not
match
what
they
think
is
a
word.
The
subjects
were
also
told
that
the
materials
that
they
had
were
independent
from
each
other.
At
the
same
time,
the
subjects
were
required
to
record
the
time
when
he
she
start
to
fill
out
each
paper,
i.e.,
one
material,
and
when
he
she
completed,
for
each
paper,
to
the
unit
of
seconds.
Results
Completed
sheets,
expect
for
one
by
a
subject
who
gave
up
the
procedure
in
the
middle
are
analyzed.
Whether
the
word
filled
in
matched
the
original
or
not
was
judged
according
to
Shiba
1957
.
Some
sheets
are
without
the
record
of
the
time.
Such
sheets
are
included
for
calculation
of
doze
pereen
ages
but
excluded
from
the
analysis
of
time.
The
eloze
percentages
and
the
medians
of
the
time
taken
to
fill
a
blank
are
shown
in
table
7-1.
The
cloze
percentages
were
higher
on
texts
with
higher
R
S
,
although
the
correlation
was
not
statistically
significant
the
correlation
coefficient
between
cloze
percentages
and
RS
s
is
0.295
.
For
the
analysis
of
time
taken,
the
texts
were
divided
into
two
categories;
the
ones
with
RS
50
and
the
ones
with
RS
50.
The
average
time
for
filling
a
blank
was
compared
between
the
two
categories
of
the
texts
using
the
median
test.
The
result
is
shown
in
table
7-2.
The
difference
of
the
time
for
filling
a
blank
is
shorter
on
texts
with
high
RS
s.
In
addition,
we
compared
a
document
and
its
rewritten
version
by
the
same
procedure.
The
material
r
1
was
taken
from
the
final
report
of
rinzi-kyouiku-singikai
National
Council
of
Educational
Reform
of
Japanese
Government.
The
document
r
1
had
an
extremely
low
score
RS
27
.
The
material
r
2
was
rewritten
-1.34
x
Is
-1.35
x
la
7.52
x
lh
-
22.1
lc
-
5.3
x
lk
-3.87
x
cp
-
109.1
where
p
a
,
p
h
,
p
c
,
pk
are
the
percentages
of
alphabet
runs,
hiragana
runs,
kanzi
runs,
and
katakana
mns,
respectively;
Is
is
the
average
numbers
of
letters
per
sentence;
la,
lh,
lc,
Ik
are
the
average
numbers
of
letters
per
alphabet
run,
hiragana
run,
kanzi
run,
and
katakana
run,
respectively;and
cp
is
the
tooten
to
kuten
ratio.
7.
Validation
of
the
Derived
Formula
We
have
also
obtained
experimental
conformation
on
the
idea
that
the
RS
is
an
adequate
measure
for
stylistic
ease
of
reading,
by
the
cloze
procedure
Taylor
1953
,
Shiba
1957
.
Cloze
procedure
judges
the
relative
reading
difficulty
of
texts
to
a
particular
population.
This
difficulty
mostly
related
to
the
content
of
the
text.
Suppose
readers
have
no
background
knowledge
of
the
content.
They
are
not
likely
to
be
able
to
fill
a
blanked-out
word
where
a
technical
term
or
some
other
word
that
requires
the
knowledge
of
the
area
the
content
belongs
to
be
filled
in.
In
such
cases,
the
cloze
score
or
the
doze
percentage
becomes
low
for
the
text
even
if
an
experts
finds
it
very
easy
to
read.
Stylistic
difficulty
may
be
also
measured
by
this
procedure,
according
to
the
experiment
of
Kitao
1960
.
In
the
experiment
subsequent
to
the
one
mentioned
in
the
previous
section,
he
required
the
subjects
to
perform
the
cloze
procedure
on
two
materials;
the
same
text
represented
in
two
different
ways.
One
is
in
the
usual
representation,
mixing
kanzi
and
kana;
the
other
is
written
entirely
in
hiragana.
The
cloze
score
of
the
usual
form
was
higher
than
the
one
entirely
in
kana.
This
result
was
consistent
with
the
result
that
the
subjects
required
longer
time
in
reading
the
text
entirely
in
kana,
as
mentioned
in
section
2.
As
the
cloze
procedure
scores
both
the
difficulty
of
style
and
the
difficulty
of
the
content,
another
measure
is
needed
to
confirm
that
our
formula
is
a
measure
of
the
stylistic
readability.
For
this
purpose,
we
recorded
the
total
time
each
subject
took
to
complete
a
cloze
text.
The
recorded
time
was
divided
by
the
number
of
blanks,
thus
converted
into
the
average
time
taken
to
fill
out
one
blank.
The
process
a
subject
takes
to
fill
out
a
blank
is
composed
of
four
phases,
i.e.,
652
from
r
1
by
di
iding
long
sentences
into
shorter
sentences
and
substituting
Japanese
words
for
words
of
Chinese
origin.
The
inte.ution
of
the
rewriting
is
to
increase
RS.
The
RS
of
the
rewritten
text
is
47,
nearly
the
average
of
the
sample
texts
of
PCA
5
I
.
The
cloze
percentage
and
the
average
time
for
flling
a
blank
is
compm
ed
con
pared
as
above.
The
cloze
percentage
of
rite
rewritten
version
r
2
was
59.6
,
higher
than
that
of
the
original
r
1
56.6
.
The
average
time
for
billing
a
blank
is
shorter
for
the
rewritten
version
than
for
the
original
the
mexlian
was
9.6
sec.
for
r
2
and
10.9
see.
for
r
1
.
The
median
test
of
time
eompadug
the
two
materials
showed
that
the
difference
was
not
statistically
significant
:
:-
0
.
These
ret;ults
show
that
1
the
subjects
take
shorter
time
with
the
texts
of
higher
RS
in
understanding
and
guessing
the
nfissing
words
of
the
text,
than
wilh
the
text
of
lower
RS,
though
the
result
is
not
statistieally
significant,
and
that
the
subjt:cts
guess
the
missing
words
in
the
high
RS
texts
more
co
cectly
than
those
in
the
low
RS
texts.
References
Recker
1984
Becker,
R.
A.
and
Chambers,
J.
M
,
S:
An
Iutero
active
Environment
for
Data
Analysis
and
Graphics
,
Wadsworth,Behnont,
C
diIbrnia,
1984
Flesch
1949
Flesch,
R
,
The
Art
of
Readable
Writing
,
1Iarper,
1949
Blayasi
1959
IIayasi,
S.,
Yomi
no
Nooryoku
to
Yomiyasusa
no
ooin
to
Yomareta
Kekka
to
Mathematial
Linguistics,
Vol.
11,
pp.20-33,
1959
In
Japanese
firose
1983
ltirose,
T.,
The
Effect
of
Script
Freqnency
on
Semantic
Processing
of
Kanji
and
Kana
Words
,
Jail
J.
of
Psychol.,
Vol.
55,
No.
3,
pp.
173-176,
1984
In
Japanese
Kitao
1960
Kitao,
N.,
Comparative
Study
on
Readability
of
lliragan;t-bun
and
Kanji-majiri-bun
,
Jap.
J.
of
Educ.
Psychol.,
Vol.7,
No.
4,
pp.
1-5,
1960
In
Japanese
Kozuru
1987
Kozuru,
Y.,
Basic
Study
for
Readability
listimation
of
Japanese
Documents
,
Proc.
of
the
34th
Convenlion
of
IPSJ,
pp.
1295-1296,
1987
In
Japanese
Morioka
1958
Morioka,
K.,
Readability
.
In:
Endo
M.
ted
,
Kotoba
no
Kagaku
,
Nakayama
Shoten,
Tokyo,
t958
In
Japanese
Sakamoto
1958
Sakamoto,
I.,.
Kyooiku
Kihon
Goi
,
Gakugei--Tosyo
,Tokyo,
1958
In
Japanese
Sakanroto
1963
Sakamoto,
1.,
Assessing
the
Weight
of
SentenceLength
....
An
attempt
to
Approach
the
Readability
Science
of
Reading,
7,
pp.
1-6,
1963
hi
Japanese
Sakamoto
1967
Sakamoto,
I.,
A
Yardstick
lor
Readability
,
Sci..
ence
of
Reading,
14,
pp.
1-6,
1967
In
Japanese
Satake
1982
Satake,
H.,
On
the
Frequency
Ratio
of
Kinds
of
Lctters
in
All
Sorts
of
Sentence
,
Report
of
The
Natiorutl
I,anguage
Research
Institute
No.
71,
p.p.32
l.-
346,
1982
in
Japaues
Shiba
1957
Shiba,
S.,
A
study
of
Readability
Measurement
....
Application
of
Cloze
Procedure
to
Japanese
Language
Jap.
J.
of
Psychol.,
Vol.28
No.2,
pp.67-
73,
1957
In
Japanese
Smith
1970
Smith,
E.
A.
and
Kinkaid,
P.,
Derivation
and
Validation
of
the
Automated
Readability
Index
for
Use
with
Technical
Materials
,
Human
Factors,
Vol.
12,
pp.
457-464,
1970
Tateisi
1987
Taleisi,
Y.,
Ono,
Y.,
and
Yamada,
II.,
Statistical
Analysis
of
Japanese
Texts
as
a
Basic
Stuby
for
Readability
,
Proc.
of
the
3rd
Symposium
on
Fluman
lntert
aee,
pp.
15-22,
Osaka,
1987
In
Japanese
Taylor
1953
Taylor,
W.
L.,
Cloze
Procedure:
A
New
Tool
fgr
Measuring
readability
,
Journalism
Quarterly,
Fall
1953
Ushijima
1987
Ushijima,
K.,
Ishida,
M.,
Yoon
J.,
and
Takagi
T.,
A
Simple
Method
to
Extract
Passive
Voices
in
the
Writing
Fools
for
Japanese
Documents
,
Trans.
of
IPSJ,
Vol.28,
No.
8,
1987
In
Japanese
Watmmbe
1983
Watanabe
S.
and
Ogisi
tL,
Zyoho
Syori
no
fozi
to
Yogo
,
Preprint
of
Working
Group
WGJI
10..2,
lnt
rmation
Processing
Society
of
Japan,
1983
In
Japanese
gasumoto
1983
Yasumoto
B.,
Settoku
no
Bunsyo
Gizyutu
,
Kodan-sya,
Tokyo,
1983
In
Japanese
2
The
difi
erenee
il
time
a
subjecl
spent
to
fill
one
blank
in
the
two
types
o
f
texts
is
significant,
by
the
median
test.
These
rcudts
did
not
show
that
RS
is
related
to
the
difficulty
of
the
content
or
the
vocabulary
of
the
texts,
ttowever,
RS
is
related
to
the
stylistic
difiiculty,
that
is,
RS
show
the
vlative
difficulty
of
:mnsfb,mation
from
the
text
itself
to
the
content.
Therefore,
RS
is
judged
useful
to
measure
the
readability
of
texts
in
general.
We
judged
that
the
clozc
score
is
more
relatexl
to
the
difficulty
of
the
content
than
to
the
difficulty
of
the
style.
There-fore
we
introduced
another
measure
tbr
stylistic
difficulty.
A
com.
rnent
fi
om
the
experimental
subject
who
gave
up
the
procedure
coufit
ms
on,
judgement
to
be
reasonable.
Ile
gave
up
he
task
because
he
has
not
enough
knowledge
of
the
m
ea
of
these
texts,
especially,
of
technical
papers.
g.
Concluding
R
e
m
a
r
k
s
We
have:
derived
a
readability
formula
ti
orh
the
multivariate
analysis
on
wtriance
of
surface
characteristics
of
Japanese
technical
documents
intended
for
adult
readers.
The
mean,
file
minimum,
and
the
maximum
v
alue
of
RS
over
the
several
types
of
texts
are
shown
in
table
8-.1.
As
with
all
indices,
RS
can
be
increased
by
revision
which
dues
not
necessarily
enhance
readability.
For
example,
if
a
text
is
written
entirely
in
hiragana
and
the
sentences
are
cut
into
short
shorter
ones,
lh
and
ph
increases
and
Is
decreases.
This
revision
yields
greater
value
of
RS
but
does
not
produce
the
text
easier
to
read.
The
formula
shmdd
be
applied
to
the
texts
written
in
the
common
w
a
y
To
construct
an
index
that
is
sensitive
to
the
unreadability
caused
by
unusually
many
ldraganas0
we
may
neeA
a
qua
dratic
formula
on
hiragana
rim
length
or
hiragana
run
frequencies.
..................................
--rabie4--l
Co-ti
onent
Loadin
s
...............................
r.
f.
r
u
n
f
r
e
x
l
u
e
n
c
y
,
r.
t.
r
n
l
t
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
N
g
V
V
g
d
2
1
q
-f
o;-
fqOL-
i
--
OgY
--NO.-d------F0.-:I----
O
--g--CNO--
--
qo?l--0-
-
0.04
I.-0.22
-0.05
0,R7
1
0.03
1
0.03
-n.
14
I
0.17
-0.39
0.I0
Alpha.
r.
f.
0.10
I
0.22
0.11
-0.93
I
0.19
1
-0.13
0.03
1
-0.03
0,04
0.11
Hira.
r.
f.
0.08
I
-0.08
-0.15
0.92
1
-0.14
1
0.24
0.0
0.18
0.10
0.09
Kanzi
r.
f.
o.121
-o.o4
-oo3
n.m
-0.25
1
-0,85
0.26
-0.II
0.02
-0.33
Kata.
r.
L
0.071
I.13
0.02
4
.72
I
-0.34
I
-0.10
-0.05
,0.04
-0.55
0.16
Sent.
length
o.o61
-0.03
0.
.
0.34
I
-0.3
1
I
0.04
0.75
0.39
00.07
-0.12
Alpha.
r.
L
-0.38
,
-0.0l
-0.02
-
I.63
I
0.54
I
.0.22
0.25
0.02
o0.14
-0.19
Hh
a.
r.
1.
o.
U.O
I
-0.
18
I
0
.
2
5
-0.39
-0.30
0.112
-0.0A
-o.28
i
-0.06
Kanzi
r.
l.
-0.14
I
0.0
-0.02
..0.tI4
I
-0.63
I
-0.53
0.28
0.29
0.:20
0.32
Kata.
r.
1.
l
ooten
r..Kl
en
-t
.4
:3
!
.-Q.54
!
0.36.
0.13.
......
0
.
5
0
-0.03
-
0
.
3
5
0
.9J
0.291
0.13
0.04
3.
c,t
1.9.
I
1.34
0.95
0.65
0.53
0.45
Eigenvalu
2.90
I
1.30
0.40
36.60
I
19.50
I
13.40
9.50
6.50
5.30
4.50
th
oportion
f
t?ym!ative.
36.60
!
56.
o
6
::
0
.....
79.
L
Ls.6e,
99.?o
9
5:4o
28
39
!
29.69
....
10o:oo
length
653
Fig,
4-1.
PrinCipal
Compm
ent
Scores
1000
P
i
1
S
Fig.
5--1.
Frequencies
Of
Passive
800
v
p
P
P
p
-
.
s
12
1
p
P
:
mi
6
10
p
p
400
e
P
P
D
-6
p
E
E
E
E
e
n
200
e
e
s
E
0
................
.1
.
.
.
.
.
.
.
.
.
.
.
i
.............
L
.......
E
4
1
..................
-4
-2
Se
:ond
0
2
-6
-4
-2
0
Second
Component
Score
2
3
.....................
2
1
D
.....
lJ
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
p
P
P
P
p
pp
l
q
fie
,lu
Em
EE
in
LZ
:Y-
-L-T
-:-22L-:
p
ln
liIi
i
E
RS
6
1
.
4
4
5
4
.
5
4
51.86
43.72
3
7
.
8
7
35.64
cloze
66.58163.81
56.28
56.25
64.69
60.46
Itimefolank
I
6
.
8
9
6.19
8218
I
8.75
9
.
0
6
I
g.05
1
L
..
.
k.......
L
....
......
l
........
1.
.
.
I
.
........
.
fl
.1
-2
-3
-4
-6
............
L.
.
.
.
.
.
.
.
.
i
I
Table
7--2.
median
test
on
time
1
P
I.
.
.
.
.
.
.
.
.
.
.
.
.
P.
.
.
.
m
........
long
12
24
I
sho
tl
......
24
12
J
-
Xz
672
,
.p
0
0
5
.o
.
-4
-2
Second
0
2
4
xiT-
---o.W--F--1
.
,
-
-
:
te,x
t
2
text3
tcxt4
0.11
0.41
0.0
7
Difficut
1
di
ato
s
136.7
I
27.5
18.
N
r
hnic
D
.
49.4
L!!
-Y
xt
kTT
T
h
S
-
-
T
9
.
9
I
T55-1
48
51
..............
Senior
High
School
A
58.0
.
9.51
49
2
39.5
h
In
table
8-1,
i
n
d
i
c
a
t
o
r
s
are
the
five
and
the
two
texts
in
the
PCA
samples
included
as
indicators
judged
as
easy
and
as
difficult.
T
e
c
h
n
i
c
a
l
D
o
t
a
n
e
n
t
s
are
the
other
70
samples.
T
e
x
t
b
o
o
k
s
are
the
passages
taken
from
the
school
textbooks
on
natural
science
and
from
the
ones
on
social
science,
five
fro
each.
They
are
included
in
lable
8-1
for
comparison.
IO.6Ol0.87
10.38
I
0
23t
I
0.42
l
0.20
o
51
.0.50
33
4
0.43
,
T-b.
-7--658-7--
-I
dov
I
O.lO
L
.O.2O
l
o.12
.I
Table
5-2.
Correlations
to
the
FrequenCy.
of
P
sive
Forras
................
6
a
A
LARGE-SCALE
SYSTEMIC
FUNCTIONAL
GRAMMAR
OF
GREEK
Aggeliki
Dimitromanolaki,
Ion
Androutsopoulos
and
Vangelis
Karkaletsis
Software
and
Knowledge
Engineering
Laboratory
Institute
of
Informatics
and
Telecommunications
National
Centre
for
Scientific
Research
Demokritos
GR-153
10
Aghia
Paraskevi,
Athens,
Greece
e-mail:
adimit,
ionandr,
vangelis
iit.demokritos.gr
Abstract
This
paper
presents
a
large-scale
computational
grammar
of
Greek,
couched
in
the
framework
of
Systemic
Functional
Linguistics.
The
grammar
is
being
developed
in
the
context
of
M-PIRO,
a
multilingual
natural
language
generation
project,
where
personalized
descriptions
of
museum
exhibits
are
generated
automatically
from
a
single
database
source.
Although
the
grammar
is
still
under
development,
it
already
provides
a
wide
coverage
of
the
Greek
syntax
and
morphology.
Our
long-term
goal
is
to
produce
a
wide-coverage
computational
grammar
of
Greek
suited
to
generation
applications.
1
Introduction
In
the
last
few
decades,
the
evolution
in
computational
linguistics
and
the
development
of
realworld
Natural
Language
Processing
NLP
applications
has
increased
the
need
for
large-scale
computational
grammars.
Natural
Language
Generation
NLG
Reiter
and
Dale
2000
,
the
process
of
producing
texts
from
some
underlying
non-linguistic
representation
of
information,
is
such
an
application.
This
paper
presents
a
large-scale
computational
grammar
of
Greek,
couched
in
the
framework
of
Systemic
Functional
Linguistics
Halliday
1985
.
The
grammar
is
being
developed
in
the
context
of
M-PIRO,
a
multilingual
natural
language
generation
project,
where
descriptions
of
museum
exhibits
are
generated
automatically
in
three
languages
English,
Greek
and
Italian
from
a
single
database
source1.
The
descriptions
are
personalised,
i.e.
reflect
the
interest
of
the
user
as
well
as
certain
educational
goals,
and
are
both
textual
and
spoken
Androutsopoulos
et
al.
2001
.
M-PIRO
builds
upon
the
ILEX
natural
language
generation
system2
Oberlander
et
al.
1998
and
extends
ILEX
s
technology
by
incorporating
improved
multilingual
capabilities,
high-quality
speech
output,
authoring
facilities,
extended
user
modelling
mechanisms,
as
well
as
a
more
modular
core
generation
engine.
NLG
systems
are
usually
decomposed
into
more
specific
modules
Reiter
1994;
Reiter
and
Dale
1997
.
The
M-PIRO
generation
system
consists
of
the
following
modules
Figure
1
:
1
M-PIRO
is
a
project
of
the
Information
Societies
Programme
of
the
European
Union,
running
from
February
2000
to
January
2003.
The
project
s
consortium
consists
of
the
University
of
Edinburgh
UK,
coordinator
,
ITC-irst
Italy
,
NCSR
Demokritos
Greece
,
the
University
of
Athens
Greece
,
the
Foundation
of
the
Hellenic
World
Greece
,
and
System
Simulated
Ltd
UK
.
More
information
about
the
project
is
available
from:
http:
www.ltg.ed.ac.uk
mpiro
.
2
See
http:
www.cstr.ed.ac.uk
cgi-bin
ilex.cgi.
1
Content
selection.
The
system
selects,
from
a
database,
which
facts
to
convey.
Facts
represent
either
relationships
between
the
entities
of
the
domain
e.g.,
X
made
Y
or
attributes
of
entities
e.g.,
name,
dimensions
.
An
example
fact
is
shown
in
the
left
column
of
Table
1.
Document
planning.
The
structure
of
the
text,
i.e.
the
ordering
of
the
facts
and
the
rhetorical
relations
that
hold
between
them
Mann
and
Thompson
1988
,
is
decided.
Micro-planning.
It
specifies
in
abstract
terms
how
each
fact
should
be
expressed
in
each
language;
for
example,
which
verb
to
use,
in
what
voice
and
tense.
An
example
microplanning
specification
is
shown
in
the
right
column
of
Table
13.
Micro-planning
also
includes
processing
steps
that
determine
which
facts
can
be
aggregated
in
a
single
sentence
e.g.,
This
vase
dates
from
approximately
550
BC
and
was
found
in
Attica
rather
than
This
vase
dates
from
approximately
550
BC.
It
was
found
in
Attica.
,
and
what
type
of
referring
expression
should
be
generated
for
each
entity
e.g.,
Doryphorus
,
this
statue
,
or
it
may
be
more
or
less
appropriate
in
the
context
of
previous
sentences
.
Surface
realization.
This
module
uses
a
grammar
of
the
target
language
to
generate
a
text
that
is
syntactically,
morphologically
and
orthographically
correct.
database
and
exhibit
to
describe
content
selection
information
to
be
conveyed
document
planning
document
structure
micro-planning
microdocument
specifications
surface
realization
exhibit
description
Fill-in
final
linguistic
details
Fill
e.g.,
articles,
number
and
case
agreement,
verb
tenses
.
Ordering
of
facts
and
general
document
structure.
How
to
express
each
fact
e.g.
which
verb
to
use,
which
entity
to
express
as
subject
.
Which
facts
to
convey.
Figure
1:
Stages
of
natural
language
generation
in
M-PIRO
Fact:
made-of
exhibit6,silver
Predicate:
made-of
Arg1:
exhibit6
Arg2:
silver
Micro-planning
specification
defexpression
made-of
:language
:greek
:arg1
exhibit6
:arg2
silver
:verb
make-verb
:voice
passive
:aspect
perfect
:arg2-prep
of
Table
1:
Example
fact
and
micro-planning
specification
For
the
last
stage
of
the
generation
process
a
large-scale
computational
grammar
of
each
target
language
is
required.
The
most
popular
approach
to
surface
realization
is
that
of
using
systemic
grammars
Halliday
1985;
Teich
1999
.
These
grammars
are
primarily
concerned
with
the
functions
of
language
and
with
how
these
functions
are
mapped
into
surface
forms
by
making
a
series
of
increasingly
fine-grained
choices
that
determine
the
syntactic
characteristics
of
the
sentence
being
constructed.
The
linguistic
options,
which
are
available
at
certain
specifiable
3
M-PIRO
s
facts
and
micro-planning
specifications
follow
Ilex
s
notation
Mellish
et
al.
1998
.
2
contexts,
are
called
features4.
Features
are
represented
by
system
networks,
with
each
system
being
a
point
of
choice
Figure
2
.
Systemic
grammars
have
been
successfully
employed
in
the
surface
realization
task
of
NLG
as,
for
example,
in
the
projects
PENMAN
Mann
and
Matthiessen
1983
,
TECHDOC
Rosner
and
Stede
1994
,
KOMET
Teich
1992
and
ILEX
Oberlander
et
al.
1998
.
The
prominent
role
of
systemic
grammars
in
surface
realization
is
due
to
the
fact
that
such
grammars
can
treat
the
syntactic,
semantic
and
even
pragmatic
aspects
of
language
in
one
integrated
model
Teich
1999
.
Systemic
grammars
have
also
the
advantage
of
allowing
resource
sharing
between
different
languages
in
multilingual
generation
Bateman
et
al.
1991
.
More
specifically,
resources
for
several
languages
can
be
combined
so
that
commonalities
are
shared
rather
than
re-represented.
This
advantage
is
very
important
to
NLG,
since
the
resources
become
more
economical
and
the
development
time
is
significantly
reduced.
This
functionality
has
been
successfully
deployed
in
KPML
Bateman
1997
,
a
multilingual
generator
that
provides
large-scale
realization
resources
and
a
basic
engine
for
using
such
resources
for
surface
realization.
declarative
indicative
interrogative
clause
subjunctive
imperative
Figure
2:
An
example
system
network
for
the
mood
of
a
clause
The
approach
taken
in
M-PIRO
is
similar
to
that
of
KPML
Bateman
1997
.
The
Greek
grammar
was
not
built
from
scratch;
it
was
constructed
by
taking
the
English
grammar
used
in
Ilex
O
Donnell
1994;
O
Donnell
1996
as
a
starting
point
and
making
the
appropriate
modifications
and
additions
in
order
to
develop
the
Greek
surface
generation
component.
In
this
way,
we
preserve
grammar
commonalities
across
different
languages,
therefore
allowing
for
faster
resource
development
and
easier
maintainance
of
the
system.
The
remainder
of
this
paper
is
organized
as
follows.
Section
2
describes
the
general
organization
of
the
grammar.
Section
3
presents
the
systemic-based
description
of
clauses,
focusing
on
the
problems
of
constituent
order
and
pronominal
subjects.
Section
4
presents
the
approach
that
we
have
adopted
for
dealing
with
the
structure
and
the
properties
of
Greek
noun
phrases
and
Section
5
is
devoted
to
the
words
and
their
morphology.
Section
6
concludes
and
discusses
future
work
plans.
2
Greek
Systemic
Grammar:
General
Organization
The
input
to
the
surface
realization
component
is
a
list
of
micro-planning
specifications
as
the
one
shown
in
Table
1.
The
goal
is
to
transform
such
a
list
into
a
cohesive
and
coherent
Greek
text.
One
of
the
central
properties
of
systemic
grammars
is
the
rank
scale,
which
defines
the
types
of
linguistic
units
used
in
the
grammar
and
serves
two
purposes.
First,
it
provides
an
effective
description
of
the
paradigmatic
relations
pertaining
to
the
grammatical
units
of
a
language,
going
on
the
assumption
that
every
grammatical
unit
has
a
set
of
grammatical
features
that
is
disjoint
from
the
set
of
features
of
the
other
units.
Second,
it
constitutes
a
hypothesis
about
syntagmatic
organization
in
terms
of
constituency
relations5.
The
rank
scale
is
often
represented
as
the
initial
system
of
the
grammar
Figure
3
.
Following
the
English
grammar,
the
Greek
grammar
of
M-PIRO
is
divided
into
four
ranks
Figure
3
.
The
highest
rank
of
the
grammar
is
the
clause-complex
rank,
which
describes
the
4
The
term
feature
is
not
used
as
in
unification
formalisms,
where
feature
refers
to
an
attribute
and
its
value.
The
Systemic
usage
is
closer
to
the
value.
5
In
this
sense,
the
rank
scale
is
comparable
to
the
bar
levels
in
X-bar
syntax;
it
incorporates,
however,
very
different
claims
about
the
actual
shape
of
the
constituent
structure.
3
structure
of
multi-clausal
units,
the
next
lower
one
is
the
clause
rank,
which
describes
the
clause
structure,
the
next
is
the
group
rank,
which
describes
the
structure
of
noun
phrases
NPs
and
prepositional
phrases
PPs
,
and
the
lowest
one
is
the
word
rank,
which
describes
the
grammatical,
syntactic
and
semantic
features
of
words.
The
surface
realization
takes
place
in
a
top-down
manner,
moving
from
the
highest
rank
of
the
grammar
to
the
lowest
one.
At
the
current
state
of
the
Greek
grammar,
the
last
three
of
these
ranks
have
been
developed
and
will
be
presented
in
Sections
3-5.
Regarding
the
clause
complex
rank,
the
corresponding
part
of
the
English
grammar
is
currently
used.
We
are
investigating
if
this
is
sufficient
for
Greek.
It
is
worth
mentioning
that
the
Greek
grammar
uses
182
systems
of
which
62
are
new,
i.e.
they
were
not
inherited
from
its
English
ancestor.
These
new
systems
handle
Greek-specific
phenomena,
which
are
not
present
in
English.
At
the
clause
rank,
such
phenomena
include
the
constituent
order,
the
clause
aspect,
the
pronominal
subjects,
the
case
assignment,
the
pronoun-antecedent
agreement
etc.
At
the
group
rank
new
features
were
introduced
to
deal
with
phenomena
such
as
agreement
and
proximal
deixis,
while
the
new
systems
of
the
word
rank
mainly
involve
information
about
the
morphological
realization
of
words.
Table
2
presents
the
number
of
systems
used
at
each
rank
of
both
grammars.
clause-complex
RANK
entry
to
grammar
clause
group
word
Figure
3:
The
top
system
of
the
grammar,
RANK
RANK
Clause
complex
Clause
Group
Word
ENGLISH
7
37
41
35
GREEK
7
54
65
56
Table
2:
Number
of
systems
at
each
rank
in
English
and
Greek
grammar.
Furthermore,
apart
from
the
grammar,
there
are
two
complementary
resources:
the
lexicon
and
the
morphological
component.
The
lexicon
provides
the
lexical
items
necessary
for
the
realization
of
words,
and
their
features.
The
features
must
be
consistent
with
the
features
defined
in
the
word
rank
of
the
grammar.
The
morphological
component
is
responsible
for
the
morphological
generation;
it
accepts
as
input
the
lexical
item
retrieved
from
the
lexicon
and
the
grammatical
e.g.
noun
declension
and
syntactic
features
e.g.
accusative
singular
of
the
word
to
be
generated
and
returns
a
fully
inflected
word.
3
Greek
Clauses
The
clause
rank
first
defines
the
structure
of
the
clause,
i.e.
the
constituents
that
make
up
a
clause
and
their
possible
orders.
According
to
this
definition,
the
clause
consists
of
the
following
elements,
of
which
only
Subj
and
Pred
are
obligatory
and
the
rest
of
them
are
optional.
These
elements
have
corresponding
slots
that
are
stored
in
the
grammar
and
carry
the
information
required
for
the
surface
realization:
Adjunct1:
A
modifier
adverb
or
prepositional
phrase
that
precedes
the
subject.
E.g.,
...
today
Doryphorus
is
located
in
the
...
.
Subj:
The
subject
of
the
clause,
which
is
always
a
noun
phrase.
Neg:
The
negative
particle.
4
Perf:
An
auxiliary
verb,
e.g.
have
in
the
perfect
tenses.
Pred:
The
main
verb
of
the
clause.
Advmod:
An
adverbial
modifier
typically
placed
before
Pred
or
between
Pred
and
Obj.
E.g.
the
statues
of
kouroi
usually
represented
young
men
.
Obj:
A
general
slot
representing
not
only
the
object
of
the
clause,
but
also
the
subject
complement
in
copula
clauses
or
a
prepositional
phrase
attached
to
the
verb.
Adjunct2:
A
modifier
adverb
or
prepositional
phrase
that
follows
the
object
or
the
predicate
in
case
there
is
no
object
.
E.g.
,
towards
the
end
of
the
archaic
period,
coins
were
used
for
transactions
.
clause-aspect
clause-voice
Figure
4:
The
system
network
for
clause
aspect
and
clause
voice
Once
the
constituents
of
the
clause
have
been
defined,
various
properties
of
the
clause
form
clause
type,
transitivity,
mood,
tense,
aspect
etc.
are
controlled
by
specifying
features
of
the
clause
unit,
which
are
organized
into
systems.
Figure
4
shows
a
system
network
consisting
of
two
systems,
clause
aspect
and
clause
voice6.
Nonrelational
clause
is
the
entry
condition
of
these
systems,
i.e.
the
context
in
which
the
choice
of
aspect
or
voice
must
be
made.
Regarding
aspect,
progressive
clause,
perfect
clause
and
simple
clause
are
the
three
features
options
among
which
a
choice
must
be
made.
Respectively,
the
features
for
voice
are
active-clause
and
passive-clause.
The
box
below
each
feature
contains
the
realization
statements
of
the
feature,
which
encode
the
structural
consequences
of
each
choice,
functioning
as
a
link
between
the
paradigmatic
options
and
the
syntagmatic
forms
axis
of
the
language.
Features
may
also
have
selection
constraints,
i.e.
preconditions
for
the
selection
of
a
feature,
which
are
not
shown
here
for
reasons
of
simplicity.
To
make
all
these
clearer,
let
us
consider
how
the
micro-planning
specification
shown
in
Table
1
will
be
processed
by
the
clause
rank
of
the
grammar.
Some
additional
links
help
the
system
do
the
mapping
between
the
expressions
found
in
the
micro-planning
specification
and
the
grammar.
Arg1
and
arg2
represent
the
Subj
and
Obj
slots
of
the
clause
structure
respectively.
According
to
the
specification,
the
main
verb
of
the
clause
Pred
must
be
a
verb
with
the
id
make-verb.
Following
Ilex
s
grammar,
we
distinguish
between
relational
clauses
whose
main
verb
is
a
copula
and
nonrelational
clauses
any
other
verb
.
This
means
that
the
clause
of
our
example
will
have
the
feature
nonrelational-clause.
The
next
slot
of
the
micro-planning
specification
:voice
passive
leads
to
the
selection
of
the
feature
passive-clause
at
the
system
clause-voice
Figure
4
.
The
realization
statement
of
this
feature
Cat:
passive-verb
adds
the
feature
passive6
The
tool
that
allows
the
graphical
representation
of
systems
and
system
networks
was
developed
by
Mick
O
Donnell
and
is
part
of
the
WAG
system
O
Donnell
1994
.
5
verb
to
the
Pred
slot
of
the
clause
structure.
Similarly,
the
feature
perfect-clause
is
selected
at
the
clause-aspect
system
Figure
4
,
which
adds
the
feature
have-aux
to
the
Perf
slot
and
the
feature
infinitive
to
the
Pred
slot.
The
last
realization
statement
of
perfect-clause
TENSE
PERF
determines
that
Perf
must
also
carry
the
information
about
the
basic
tense
of
the
clause
present,
past
or
future
.
Since
the
micro-planning
specification
does
not
provide
any
information
about
the
tense
of
the
clause,
the
default
tense,
i.e.
present,
is
assumed
default
values
are
determined
in
the
additional
links
connecting
the
grammar
with
the
micro-planning
specification
.
Consequently,
the
feature
present-clause,
which
belongs
to
a
system
called
clause-tense
not
shown
here,
is
selected,
which
causes
Perf
to
have
the
feature
present-verb.
Number
and
person
features
are
selected
by
two
other
systems,
depending
on
the
number
and
person
of
the
subject;
in
this
example
the
features
singular-verb
and
thirdperson
are
added
to
the
Perf
slot.
The
last
slot
of
the
micro-planning
specification
informs
us
that
we
have
to
use
a
preposition
with
the
id
of
in
the
Obj
slot
of
the
clause.
So,
the
clause
structure
of
this
micro-planning
is
given
in
Figure
5.
The
features
nominative-np
and
accusative-np
of
the
Subj
and
Obj
slot
respectively
are
forced
by
the
realization
statements
of
the
clause
feature,
which
is
not
shown
here.
Subj:
exhibit6
nominative-np
Perf:
have-aux
present-verb
singular-verb
thirdperson
Pred:
make-verb
passive-verb
infinitive
Obj:
of
silver
accusative-np
Figure
5:
An
example
output
of
the
clause
rank
CURRENT
COVERAGE
Passive
clauses
Simple,
perfect
and
progressive
aspect
Clause
tenses
S-V-O
and
O-V-S
word
order
Relative
clauses
agreement,
case
assignment
etc.
Pro-drop
Copula
clauses
FUTURE
GOALS
Clitics
Subjunctive
and
imperative
clauses
Non-finite
clauses
Other
possible
word
orders
Other
subordinate
clauses
Table
3:
Phenomena
currently
covered
at
clause
rank
and
future
goals
The
clause
rank
of
the
Greek
grammar
currently
covers
declarative
main
clauses
and
relative
clauses.
Table
3
presents
the
basic
linguistic
phenomena
that
have
been
accounted
for
up
to
now
and
also
the
issues
that
we
plan
to
address
in
the
future.
The
remainder
of
this
section
is
devoted
to
two
issues
that
have
received
particular
attention
in
Greek
linguistics:
constituent
order
and
pro-drop
clauses.
3.1
Constituent
Order
The
problem
of
Greek
word
order
has
been
extensively
studied
Horrocks
1982;
PhilippakiWarburton
1982;
Philippaki-Warburton
1985;
Tsimpli
1990
.
It
is
generally
agreed
that
the
order
of
the
basic
constituents
subject,
verb,
object
of
a
main
clause
is
very
flexible
and
that
all
possible
combinations
yield
grammatical
clauses.
The
difference
between
the
various
constituent
orders
of
the
same
clause
is
not
syntactic;
it
is
rather
pragmatic,
since
the
ordering
of
the
constituents
depends
on
discourse
reasons,
i.e.
which
constituent
is
the
topic
of
the
discourse,
which
constituent
we
want
to
emphasize
etc.
In
generation,
a
clause
is
required
not
only
to
be
grammatical,
but
also
to
be
free
of
false
implicatures
so
as
to
contribute
to
the
overall
discourse
coherence.
The
systemic-based
Greek
grammar
handles
this
problem
as
follows.
Our
methodology
was
to
account
first
for
the
orders
that
are
frequent
in
descriptive
texts
such
as
the
texts
produced
by
M-PIRO.
A
corpus
analysis
indicated
that
the
main
clauses
found
in
descriptive
texts
have
always
one
of
the
two
NPs,
either
the
subject
or
the
object
at
the
first
6
position;
hence,
the
order
is
either
S
subject
-V
verb
-O
object
or
O-V-S.
This
is
easily
explained
by
the
fact
that
each
sentence
in
these
texts
contains
a
topic,
the
entity
to
be
described,
and
some
new
information
about
that
entity;
and
it
is
generally
taken
for
granted
in
linguistics
that
the
topic
is
always
placed
at
the
initial
position
of
the
sentence.
Furthermore,
it
has
been
argued
that
the
subject
is
the
most
natural
topic
of
the
clause
Holton
et
al.
1997
and
this
is
also
evidenced
by
the
dominance
of
the
S-V-O
constituent
order
in
our
corpus.
Hence,
in
our
grammar
the
S-V-O
order
is
assumed
to
be
the
default
basic
constituent
order.
See,
for
example,
the
following
sentence:
1.
.
this
the
portrait
depicts
the
Great
Alexander
This
portrait
depicts
Alexander
the
Great.
However,
in
some
cases
the
topic
is
required
to
be
the
syntactic
object
of
the
clause.
Consider,
for
instance,
the
following
sentences,
which
constitute
different
realizations
of
the
fact
painterof
exhibit1,
painter-of-kleofrades
.
One
of
these
sentences
will
be
generated
next
to
the
text
fragment
given
in
Figure
6,
at
the
position
indicated
by
a
question
mark.
2.
.
the
painter
the-gen
Kleofrades
painted
this
the
amphora
The
painter
of
Kleofrades
painted
this
amphora.
3.
.
this
the
amphora
was-painted
by
the
painter
the-gen
Kleofrades
This
amphora
was
painted
by
the
painter
of
Kleofrades.
4.
.
this
the
amphora
it
painted
the
painter
the-gen
Kleofrades
This
amphora
was
painted
by
the
painter
of
Kleofrades.
.
Martin
von
Wagner
Wurzburg,
.
?
This
exhibit
is
an
amphora.
Today
it
is
located
in
the
Martin
von
Wagner
Museum,
University
of
Wurzburg,
which
is
in
Germany.
?
Figure
6:
Text
fragment
generated
by
M-PIRO
Sentence
2,
S-V-O,
would
be
natural
if
the
subject
the
painter
of
Kleofrades
had
already
been
introduced
in
the
discourse
and
was
functioning
as
the
topic
of
the
utterance.
In
this
case,
however,
the
painter
of
Kleofrades
has
not
been
mentioned
before;
the
text
is
about
the
amphora,
which
is
also
the
topic
of
the
current
utterance,
and
this
is
the
first
mention
of
the
painter.
On
the
other
hand,
although
3
is
both
grammatical
and
consistent
with
the
discourse
parameters,
the
passive
syntax
does
not
look
very
natural
in
Greek
and
most
speakers
seem
to
prefer
4,
i.e.
the
O-V-S
order.
In
order
to
account
for
this
case,
we
defined
a
system
Figure
7
that
differentiates
between
two
types
of
transitive
clauses,
one
that
has
the
subject
and
one
that
has
the
object
as
its
topic
object
topicalisation
.
Following
Ilex
O
Donnell
et
al.
1998
,
the
MPIRO
system
keeps
track
of
pragmatic
information
concerning
the
referring
environment
e.g.
previously
mentioned
entities,
current
topic,
previous
topic,
current
focus,
previous
focus
.
Such
information
is
then
passed
to
the
grammar
Figure
7
.
explicit-object-1
:same
Subj.Sem
topic
S-V-O
transitive-clause
explicit-object-2
:same
Obj.Sem
topic
O-V-S
Figure
7:
The
system
handling
the
constituent
order
of
transitive
clauses
7
In
the
cases
of
object
topicalization,
the
presence
of
the
clitic
pronoun
e.g.
in
sentence
4
is
obligatory
Warburton-Philippaki
1977
.
For
the
time
being,
we
do
not
provide
a
proper
account
of
clitics,
but
we
are
planning
to
handle
this
phenomenon
in
future
versions
of
the
grammar.
Future
versions
will
also
allow
for
other
word
order
variations
in
main
clauses
as
well
as
for
appropriate
ordering
in
subordinate
clauses.
3.2
Pro-drop
As
opposed
to
English,
which
allows
only
overt
pronouns,
Greek
allows
both
overt
and
null
pronouns
in
the
subject
position
Dimitriadis
1995;
Dimitriadis
1996;
Joseph
1994;
PhilippakiWarburton
1987
.
In
order
to
have
as
fluent
texts
as
possible,
we
had
to
study
the
conditions
of
use
of
each
of
these
pronominal
subjects
and
provide
an
account
for
them
in
the
Greek
grammar.
adverbial-modified-subject
:exists
Subj.modifier
not
null-pronoun
Clause
not-adverbial-modified-subject
.
.
Today
this
stater
is
located
in
the
Numismatic
Museum
of
Athens.
It
was
created
during
the
archaic
period
and
is
made
of
silver.
.
.
This
tetradrachm
originates
from
Athens.
It
was
also
created
during
the
archaic
period.
Figure
8:
Pronominal
subjects
depending
on
subject
modifiers
Our
first
observation,
resulted
from
the
corpus
analysis,
was
that
null
subject
pronouns
are
excluded
when
the
subject
of
the
clause
is
modified
by
an
adverb,
e.g.
,
only
this
or
the
emphatic
and
.
In
our
grammar
such
clauses
cannot
have
a
null
pronoun
as
subject.
Figure
8
presents
the
system
handling
this
phenomenon
and
two
text
fragments
containing
null
and
overt
subject
pronouns
respectively.
subject-topic-clause
:same
previous-topic
Subj
null
pronoun
not-adverbial-modified-subject
subject-topic-clause
:same
previous-topic
Subj
overt
pronoun
..
.
.
This
imperial
portrait
dates
from
the
first
century
AD
and
today
is
located
in
the
Archaeological
Museum
of
Corinth.
It
was
created
during
the
roman
period.
.
.
The
painter
of
Kleofrades
also
painted
a
vase
with
scenes
from
Iliou
Persis.
This
is
one
of
his
most
interesting
works.
Figure
9:
Pronominal
subjects
depending
on
discourse
factors
The
second
observation
concerns
the
remaining
clauses,
i.e.
clauses
that
do
not
contain
modified
subjects.
When
the
entity
expressed
as
the
syntactic
subject
of
the
current
clause
is
the
topic
of
the
previous
utterance,
a
null
pronoun
is
preferred.
On
the
other
hand,
if
this
entity
is
not
8
the
topic
of
the
previous
utterance,
an
overt
pronoun
is
needed
in
order
to
assure
correct
interpretation
of
the
sentence
and
coherence
of
the
text.
This
approach
resembles
the
centeringbased
approach
of
Dimitriadis
1996
7.
In
Figure
9
we
summarize
the
system
handling
this
phenomenon
and
provide
two
text
fragments
as
examples
of
the
correct
realization
of
pronouns.
4
Greek
Noun
Phrases
The
realization
of
noun
phrases
NPs
is
guided
by
the
group
rank
of
the
grammar.
The
group
rank
also
guides
the
realization
of
prepositional
phrases
PPs
,
but
space
limitations
do
not
allow
discussing
them
here.
Considering
NPs,
a
basic
distinction
that
is
made
is
between
nominal
containing
a
noun
and
pronominal
containing
a
pronoun
instead
of
the
noun
NPs.
Nominal
NPs
are
further
subdivided
into
common
and
proper
NPs.
As
in
the
case
of
clauses,
we
first
defined
the
structure
of
each
of
these
NP
types.
The
elements
of
a
common
NP
are
presented
below
most
of
them
are
optional
with
only
Noun
being
obligatory
.
These
elements
have
corresponding
slots
that
are
stored
in
the
grammar
and
carry
the
information
required
for
the
surface
realization
of
NPs:
Quantifier:
Words
expressing
quantity,
e.g.
all
,
many
,
few
.
Prox-deictic:
The
demonstratives,
,
,
this
etc.,
when
they
function
as
determiners
and
not
as
pronouns.
Deictic:
The
definite
and
indefinite
article
and
any
other
determiners
that
do
not
fall
to
another
category
quantifier,
proximal-deictic,
numeral
.
Numeral:
The
cardinal
numbers
and
numerals.
Adjectives:
Any
number
of
adjectives
is
allowed.
Noun:
The
noun
of
the
phrase.
Classifier:
An
NP
functioning
as
a
modifier
of
the
current
NP,
e.g.
possessive
pronoun,
genitive-np.
As
pointed
out
in
Holton
et
al.
1997
,
when
all
these
elements
occur
in
the
same
NP,
the
most
natural
word
order
is
as
follows:
Quantifier
-
Prox-deictic
Deictic
Numeral
Adjectives
Noun
Classifier
Consider,
for
example,
the
following
sentence:
all
those
the
five
big
painters
our
All
those
five
great
painters
of
ours
In
contrast,
when
a
smaller
number
of
elements
is
used,
the
linear
order
may
vary
depending
on
semantic
or
pragmatic
factors,
e.g.
,
this
hydria
.
We
have
only
accounted
for
the
order
presented
above,
which,
we
believe,
is
the
basic
the
most
neutral
in
terms
of
context
word
order
of
Greek
NPs,
but
our
future
goals
include
a
treatment
of
all
possible
order
variations.
Subj:
exhibit6
tetradrachm-noun
nonpossessive-deixis
proximal-deixis
close-proximal-deixis
singular-np
neuter-np
nominative-np
Figure
10:
An
example
input
to
the
group
rank
As
in
clauses,
various
aspects
of
the
Greek
NP
form
definiteness,
proximity,
possessiveness,
number,
gender
and
case
agreement
between
the
elements
etc.
are
controlled
by
specifying
features
of
the
NP
unit.
Some
NP
features
are
shown
in
Figures
11-13.
Let
us
now
go
back
to
the
7
As
Dimitriadis
Dimitriadis
1996
points
out
the
Cb
represents
the
immediate
center
of
attention,
and
therefore
constitutes
centering
theory
s
version
of
the
notion
of
topic
.
9
example
considered
in
Section
3.
The
input
to
the
group
rank
is
an
NP
or
a
PP,
such
as
the
Subj
or
Obj,
respectively,
shown
in
Figure
5,
and
also
some
information
provided
by
the
NP
planner,
a
different
module
of
the
system
that
decides
how
to
refer
to
entities,
i.e.
which
type
of
referring
expression
must
be
used
in
a
given
context
O
Donnell
et
al.
1998
.
The
information
provided
by
the
NP
planner
is
represented
by
features
of
the
group
rank.
Figure
10
summarizes
the
information
that
constitutes
the
input
to
the
group
rank
in
the
case
of
our
example
the
features
in
bold
italics
are
provided
by
the
NP
planner
.
Figure
11:
The
system
deixis-possessiveness
Figure
12:
The
system
deixis-proximity
Figure
13:
The
system
network
of
proximal-deixis
As
shown
in
Figure
11,
the
selection
of
nonpossessive-deixis
means
that
the
feature
nonproximal-determiner
must
be
added
to
the
Deictic
slot
of
the
NP
structure.
Similarly,
the
features
proximal-deixis
and
close-proximal-deixis
require
the
Prox-deictic
slot
to
have
the
features
proximal-determiner
Figure
12
and
close-proximal-determiner
Figure
13
,
respectively.
Subsequently,
a
decision
on
the
number
and
gender
of
this
NP
must
be
made.
These
features
also
derive
from
the
NP
planner.
Since
the
NP
is
marked
as
singular-np
and
neuter-np,
the
corresponding
features,
singular-np-pr
and
neuter-prodeixis,
are
selected
in
the
system
network
shown
in
Figure
13
the
selection
of
these
features
is
guided
by
their
selection
constraints
not
shown
here
.
Similarly,
the
feature
nominative-np,
provided
by
the
clause
rank,
10
leads
to
the
selection
of
nominative-pr-nominal-group
Figure
13
.
Therefore,
Prox-deictic
is
enriched
with
the
features
singular-determiner,
neuter-determiner,
nominative-determiner.
In
a
similar
way,
the
corresponding
number,
gender
and
case
features
are
selected
for
the
Deictic
and
the
Noun
slot
of
the
NP
structure.
The
final
output
of
the
group
rank
is
given
in
Figure
14.
Prox-deictic:
proximal-determiner
close-proximal-determiner
singular-determiner
neuter-determiner
nominative-determiner
Deictic:
nonproximal-determiner
singular-determiner
neuter-determiner
nominative-determiner
Noun:
tetradrachm-noun
singular-noun
neuter-noun
nominative-noun
Figure
14:
An
example
output
of
the
group
rank
CURRENT
COVERAGE
Ordering
Determiners
in
proper
NPs
Determiner-noun
agreement
Proximal
deixis
Pronoun-antecedent
agreement
Classifiers
Generic
NPs
FUTURE
GOALS
Possible
ordering
variations
NPs
containing
adjectives
Elliptical
NPs
Table
4:
Phenomena
currently
covered
at
the
group
rank
and
future
goals
The
group
rank
of
the
Greek
grammar
currently
covers
a
wide
range
of
NPs.
Table
4
summarizes
the
basic
linguistic
phenomena
that
have
been
accounted
for
up
to
now
and
also
the
issues
that
we
plan
to
address
in
the
future.
5
Words
and
Morphology
The
word
rank
is
the
part
of
the
grammar,
which,
in
conjunction
with
the
lexicon
and
the
morphological
component,
controls
the
realization
of
words.
The
morphological,
syntactic
and
semantic
features
of
words
need
to
be
specified
in
order
to
guide
morphological
and
syntactic
realization.
Because
of
space
limitations,
we
present
only
the
first
category
of
features,
the
morphological
features,
here.
Figure
15:
The
system
network
of
noun
declension
Figure
15
shows
the
system
network
describing
the
declensions
of
nouns.
We
distinguish
two
main
morphological
categories
of
nouns:
two
ending
nouns
and
three
ending
nouns
.
Two
ending
nouns
are
subdivided
into
same
number
of
syllables
and
different
number
of
syllables
Klairis
and
Babiniotis
1998
.
We
further
divided
into
four
subcategories:
not-ancient-like,
i.e.
masculine
and
feminine
nouns
ending
in
-
in
plural
e.g.
,
,
ancient-like,
i.e.
masculine
and
feminine
nouns
ending
in
-
e.g.
,
in
plural,
neuter1,
i.e.
neuter
nouns
ending
in
or
in
the
nominative
singular
e.g.
,
and
neuter2,
i.e.
neuter
11
nouns
ending
in
in
the
nominative
singular
e.g.
.
Such
declension
features
together
with
the
gender
features
masculine,
feminine,
neuter
are
included
in
the
lexicon
entries
Figure
16
and
in
the
morphological
rules
of
the
grammar
Figure
17
and
are
used
to
generate
the
inflection
types
of
the
words;
note
that
the
case
and
number
features
are
already
known
to
the
system
from
previous
stages
of
processing
Figure
14
.
def-lexical-item
:name
tetradrachm-noun
:spelling
:grammatical-features
noun
common-noun
count-noun
inflected-noun
neuter-noun
neuter1
:concept
tetradrachm
Figure
16:
Lexical
entry
for
tetrradrachm-noun
If
features
singular-noun
inflected-noun
neuter-noun
neuter1
not
genitive-noun
then
add
suffix
If
features
plural-noun
inflected-noun
neuter-noun
neuter1
not
genitive-noun
then
remove
last
character
add
suffix
a
Figure
17:Example
morphological
rules
In
a
similar
way,
we
have
accounted
for
the
morphology
of
verbs.
Figure
18
shows
the
system
network
describing
the
conjugations
of
verbs.
The
features
shown
there
are
combined
with
features
already
known
to
the
system,
e.g.
voice,
tense,
number
and
person
Figure
5
,
for
the
generation
of
the
surface
verb
forms.
The
grammar
also
supports
the
morphological
generation
of
determiners
and
pronouns.
Future
versions
of
the
grammar
will
include
an
account
for
the
morphological
generation
of
adjectives.
Figure
18:
The
system
network
of
verb
conjugations
As
an
example,
consider
the
realization
of
the
Noun
slot
of
Figure
14.
The
system
looks
at
the
Greek
lexicon
and
tries
to
find
a
lexical
item
with
the
id
tetradrachm-noun
Figure
16
.
It
retrieves
the
base
form
marked
as
spelling
of
the
word
and
the
information
related
to
its
declension
neuter1
,
which
is
included
in
the
grammatical
features
of
the
word.
Then,
it
consults
the
morphological
component
trying
to
find
what
changes
in
the
ending
of
words
having
the
declension
feature
neuter1
and
also
the
features
singular-noun,
neuter-noun,
nominative-noun,
provided
by
the
group
rank
of
the
grammar
Figure
14
.
The
retrieved
rule
the
first
rule
in
Figure
17
informs
us
that
no
ending
suffix
is
needed,
so
the
word
is
generated
as
it
is.
If
the
plural
form
of
the
word
was
needed,
i.e.
if
the
Noun
slot
of
Figure
14
had
the
feature
plural-noun
instead
of
singular-noun,
the
second
rule
of
Figure
17
would
be
retrieved.
This
rule
removes
the
last
character
from
the
spelling
of
the
word
and
adds
the
suffix
,
generating,
thus,
the
word
.
12
6
Conclusions
and
Future
Work
In
this
paper
we
presented
a
large-scale
computational
grammar
of
Greek,
used
in
the
M-PIRO
natural
language
generation
project
for
the
surface
realization
of
Greek
texts.
This
grammar
is
couched
in
the
framework
of
Systemic
Functional
Linguistics,
which
is
widely
used
in
generation;
this
framework
allows
resource
sharing
in
multilingual
generation
and,
hence,
guarantees
more
economical
resources
and
reduced
development
time.
Although
the
grammar
is
still
under
development,
it
already
provides
a
wide
coverage
of
the
Greek
language.
Further
work
is
under
way
in
order
to
extend
the
grammar;
more
specifically,
our
future
goals
are
summarized
as
follows:
Treatment
of
clitics.
Subordinate
clauses,
apart
from
the
relative
clauses
that
are
already
dealt
with.
Further
order
variations
in
clauses
and
NPs.
Other
NP
types
e.g.
elliptical
NPs,
adjectival
NPs
.
Morphological
treatment
of
adjectives.
Our
long-term
goal
is
to
produce
a
full
coverage
computational
grammar
of
Greek
that
can
be
exploited
in
several
natural
language
generation
applications.
Acknowledgements
The
authors
would
like
to
thank
Mick
O
Donnell,
former
member
of
the
M-PIRO
team,
for
his
contribution
to
the
integration
of
Greek
resources
into
the
M-PIRO
multilingual
system.
References
Androutsopoulos
et
al.
2001
Androutsopoulos
I.,
Kokkinaki
V.,
Dimitromanolaki
A.,
Calder
J.,
Oberlander
J.
and
Not
E.
Generating
Multilingual
Personalized
Descriptions
of
Museum
Exhibits
The
M-PIRO
Project.
Presented
at
the
29th
Conference
on
Computer
Applications
and
Quantitative
Methods
in
Archaeology,
Gotland,
2001.
Bateman
et
al.
1991
Bateman
J.,
Matthiessen
C.,
Nanri
C.
and
Zeng,
L.
The
Re-use
of
Linguistic
Resources
across
Languages
in
Multilingual
Generation
Components.
In
Proceedings
of
IJCAI
91,
Sydney,
Australia.
1991.
Bateman
1997
Bateman
J.
Enabling
technology
for
multilingual
natural
language
generation:
the
KPML
development
environment.
Natural
Language
Engineering
3
1
:
15-55,
1997.
Dimitriadis
1995
Dimitriadis
A.
When
Pro-Drop
Languages
Don
t:
On
Overt
Pronominal
Subjects
in
Greek.
In
Penn
Working
Papers
in
Linguistics
2:2
Proceedings
of
the
19th
Penn
Linguistics
Colloquium
,
1995.
Dimitriadis
1996
Dimitriadis
A.
When
Pro-Drop
Languages
Don
t:
Overt
Pronominal
Subjects
and
Pragmatic
Inference.
In
Proceedings
of
CLS
32,
1996.
Halliday
1985
Halliday
M.
An
Introduction
to
Functional
Grammar.
Edward
Arnold,
London,
1985.
Holton
et
al.
1997
Holton,
D.,
Mackridge,
P.
and
Philippaki-Warburton,
I.
Greek.
A
Comprehensive
Grammar
of
the
Modern
Language.
Routledge,
London,
1997.
Horrocks
1982
Horrocks,
G.
The
Order
of
Constituents
in
Modern
Greek.
In
Gazdar,
Klein
and
Pullum
eds.
,
Order,
Concord
and
Constituency,
pp.
95-111.
Foris
Publications,
Dordrecht,
1982.
Joseph
1994
Joseph
B.D.
On
Weak
Subjects
and
Pro-Drop
in
Greek.
In
I.
PhilippakiWarburton,
K.
Nicolaidis,
M.
Sifianou
eds
Themes
in
Greek
Linguistics
Papers
from
the
13
First
International
Conference
on
Greek
Linguistics,
Reading,
September
1993
.
Amsterdam:
John
Benjamins
Publishers,
1994,
pp.
21-32.
Klairis
and
Babiniotis
1998
Klairis
C.
and
Babiniotis
G.
:
.
,
1998.
Mann
and
Matthiessen
1983
Mann
W.
C.
and
Matthiessen
C.
M.
Nigel:
A
systemic
grammar
for
text
generation.
Technical
Report
ISI
RR-85-105,
Information
Sciences
Institute,
February
1983.
Mann
and
Thompson
1988
Mann
W.
C.
and
Thompson
S.
Rhetorical
Structure
Theory:
Towards
a
Functional
Theory
of
Text
Organization
.
Text,
3:243
281,
1988.
Mellish
et
al.
1998
Mellish
C.,
O
Donnell
M.,
Oberlander
J.
and
Knott
A.
An
Architecture
for
Opportunistic
Text
Generation
.
In
Proceedings
of
the
9th
International
Workshop
on
Natural
Language
Generation,
Niagara-on-the-Lake,
Ontario,
Canada,
1998.
Oberlander
et
al.
1998
Oberlander
J.,
O
Donnell
M.,
Knott
A.
and
Mellish
C.
Conversation
in
the
museum:
experiments
in
dynamic
hypermedia
with
the
intelligent
labelling
explorer.
New
Review
of
Hypermedia
and
Multimedia,
4,
11--32,
1998.
O
Donnell
1994
O
Donnell
M.
Sentence
Analysis
and
Generation
a
Systemic
Perspective.
PhD
thesis,
Linguistics
Dept.,
University
of
Sydney,
1994.
O
Donnell
1996
O
Donnell
M.
Input
specification
in
the
WAG
sentence
generation
system.
In
Proceedings
of
the
8th
International
Workshop
on
Natural
Language
Generation,
Herstmonceux
Castle,
UK,
June
1996.
O
Donnell
et
al.
1998
O
Donnell
M.,
Cheng
H.
and
Hitzeman
J.
Integrating
Referring
and
Informing
in
NP
Planning.
In
Proceedings
of
the
Coling-ACL
98
Workshop
on
the
Computational
Treatment
of
Nominals,
August
16,
1998,
Universite
de
Montreal,
Canada.
pp.
4655.
Philippaki-Warburton
1982
Philippaki-Warburton,
I.
.
,
1:
99-107,
1982.
Philippaki-Warburton
1985
Philippaki-Warburton,
I.
Word
Order
in
Modern
Greek.
Transanctions
of
the
Philological
Society,
pp.
113-143,
1985.
Philippaki-Warburton
1987
Philippaki-Warburton,
I.
The
theory
of
empty
categories
and
the
pro-drop
parameter
in
Modern
Greek.
Journal
of
Linguistics,
23:
289-318,
1987.
Reiter
1994
Reiter,
E.
Has
a
Consensus
NL
Generation
Architecture
Appeared,
and
is
it
Psycholinguistically
Plausible?
In
Proceedings
of
the
7th
International
Workshop
on
Natural
Language
Generation
INLGW-1994
,
pages
163-170,
1994.
Reiter
and
Dale
1997
Reiter,
E.
and
Dale,
R.
Building
Applied
Natural
Language
Generation
Systems.
Natural
Language
Engineering
3:57-87,
1997.
Reiter
and
Dale
2000
Reiter,
E.
and
Dale,
R.
Building
Applied
Natural
Language
Generation
Systems.
Cambridge
University
Press,
2000.
Rosner
and
Stede
1994
Rosner
D.
and
Stede
M.
Generating
multilingual
documents
from
a
knowledge
base:
the
TECHDOC
project.
In
Proceedings
of
the
15th
International
Conference
on
Computational
Linguistics
COLING-94
.
Volume
I,
pages
339-346.
Kyoto,
Japan,
1994.
Teich
1992
Teich,
E.
KOMET:
Grammar
documentation.
Technical
report,
GMD,
Darmstadt,
Germany,
1992.
Teich
1999
Teich,
E.
Systemic
Functional
Grammar
in
Natural
Language
Generation:
Linguistic
Description
and
Computational
Representation.
Casssell
Academic
Publishers,
London,
1999.
Tsimpli
1990
Tsimpli,
I.
The
Clause
Structure
and
Word
Order
of
Modern
Greek.
In
J.
Harris
ed.
,
UCL
Working
Papers
in
Linguistics,
2:
226-255,
1990.
Warburton-Philippaki
1977
Modern
Greek
clitic
pronouns
and
the
surface
structure
constraints
hypothesis.
Journal
of
Linguistics,
13:259-281,
1977.
14
GOTHENBURG
MONOGRAPHS
IN
LINGUISTICS
14
A
LOGICAL
APPROACH
TO
COMPUTATIONAL
CORPUS
LINGUISTICS
Torbj
rn
Lager
Department
of
Linguistics,
G
teborg
University,
Sweden,
1995
GOTHENBURG
MONOGRAPHS
IN
LINGUISTICS
14
A
LOGICAL
APPROACH
TO
COMPUTATIONAL
CORPUS
LINGUISTICS
Torbj
rn
Lager
Doctoral
Dissertation
publicly
defended
in
Stora
H
rsalen,
Humanisten,
G
teborg
University,
on
December
16th,
1995,
at
10.00
for
the
degree
of
Doctor
of
Philosophy
Department
of
Linguistics,
G
teborg
University,
Sweden,
1995
ABSTRACT
The
purpose
of
this
thesis
is
to
build
a
corpus
theory
development
environment
to
discuss
its
design,
use,
and
implementation.
The
proposed
system
is
based
on
a
logical
approach
to
computational
corpus
linguistics
where
sentences
of
logic
are
used
to
express
statements
about
texts
and
logical
inference
is
used
to
manipulate
these
sentences
in
order
to
analyse
the
texts.
The
thesis
demonstrates
the
remarkable
ease
with
which
the
functionalities
needed
in
a
corpus
system
can
be
implemented
when
based
upon
adequate
means
of
representing,
querying,
and
reasoning.
The
proposed
system
implements
hand
coding,
searching,
concordancing,
parsing,
counting,
tabling,
collocating,
automatic
part-of-speech
tagging,
lemmatizing,
excerpting,
interpreting,
treebanking,
explanation,
and
various
kinds
of
learning.
By
linking
all
this
functionality
into
a
common
representational
framework
characterised
by
high
expressive
power,
declarativity,
and
explicit
reasoning
strategies,
and
by
embedding
the
whole
concept
in
a
particular
philosophical
and
methodological
context,
including
an
ontology
of
text,
an
analysis
of
the
notion
of
theory,
an
explication
of
the
notion
of
truth,
and
other
foundational
issues,
we
arrive
at
an
interactive
system
which
is
multi-functional
and
general,
yet
simple,
consistent,
and
highly
usable.
Apart
from
being
interesting
from
a
practical
point
of
view,
the
development
of
such
a
system
raises
intriguing
philosophical
and
methodological
questions:
What
is
a
corpus
text?
What
is
a
corpus
theory?
What
does
it
mean
to
develop
a
corpus
theory?
What
does
it
mean
for
a
corpus
theory
to
be
true
about
a
corpus
text?
What
is
the
link
between
the
truth
of
such
a
theory
and
its
usefulness
for
natural
language
processing
purposes?
These
and
related
questions
are
discussed
in
the
thesis.
The
system
exists
in
a
prototype
implementation
and
the
thesis
contains
numerous
examples
from
this
implementation
in
action.
KEY
WORDS:
Corpus
linguistics,
Corpus
tools,
Grammar,
Grammar
development,
Logic
programming
Preface
This
is
my
Ph.D.
thesis.
It
is
intended
for
an
audience
consisting
of
computational
corpus
linguists
and
theoretical
computational
linguists.
I
have
put
computational
within
brackets,
to
indicate
that
a
background
and
interest
in
computer
science
is
optional.
The
chapters
covering
the
basics
of
computational
logic
are
written
for
linguists
with
no
such
backgrounds,
as
well
as
for
corpus
linguists
who
have
backgrounds
in
computer
science
but
who
prefer
to
have
s
and
s
rather
than
s
and
s
staring
at
them
from
the
pages
when
they
open
a
book.
Thus,
I
should
let
logic
programming
aficionados
know
that
they
can
safely
skip
these
parts,
and
I
d
better
tell
the
statistics
people
that
there
are
a
couple
of
s
and
s
in
this
book,
so
hang
on.
My
sincere
thanks
and
deeply
felt
gratitude
go
to
the
following
persons:
To
my
supervisor
Jens
Allwood,
whose
enthusiasm
and
generosity,
coupled
with
a
nagging
criticism,
has
kept
me
on
the
track
all
the
way
to
the
end.
To
Joakim
Nivre,
assistant
supervisor,
whom
I
have
often
felt
understood
my
ideas
better
than
I
did
myself
and
whose
sound
advice
has
been
invaluable
to
my
work.
To
assistant
supervisor
Bj
rn
Haglund,
philosopher,
whom
I
have
known
since
I
first
begun
my
studies
at
the
university,
and
whose
deep
and
clear
thinking
has
always
impressed
me.
To
my
thesis
opponent
Martin
Kay,
for
a
very
rewarding
discussion
at
the
public
defence
of
my
thesis.
To
the
members
of
my
thesis
committee
Lars
Ahrenberg,
Christian
Bennet,
and
Robin
Cooper
for
letting
me
become
vi
a
doctor.
To
Lars
and
Robin
also
for
the
many
stimulating
discussions
at
various
points
during
the
development
of
my
work.
To
Bj
rn
Beskow,
inspiring
friend,
with
whom
I
have
discussed
many
of
my
ideas
when
they
were
still
in
their
infancy.
To
Peter
Campbell
who
did
wonders
to
my
English,
proof
reading
my
manuscript,
correcting
my
style.
To
all
colleagues
at
the
department
of
linguistics
in
G
teborg,
for
providing
such
an
enjoyable
atmosphere
to
work
in.
To
my
parents,
for
always
supporting
me
in
all
the
things
I
have
ever
undertaken.
To
August,
Fabian,
Hillevi,
and
Malin,
my
dearest
family
I
hereby
dedicate
this
thesis
to
you.
On
the
cover
of
this
book
is
a
picture
of
Saint
Jerome
Eusebius
Hieronymus
,
c.347-420,
painted
by
Albrecht
D
rer
500
years
ago.
Jerome
was
a
Father
of
the
Church,
whose
great
work
was
the
translation
of
the
Bible
into
Latin.
At
that
time
there
were
no
computers.
In
the
picture,
sitting
in
his
lab,
Jerome
seems
to
be
accessing
his
intuitions,
while
pointing
at
a
corpus.
G
teborg
in
May,
1996
T.L.
Contents
CHAPTER
1
Introduction
1
1.1
1.2
1.3
1.4
1.5
1.6
1.7
Introduction
1
1.1.1
The
dichotomy
is
false
...
4
1.1.2
...
but
still
practised
5
The
State
of
the
Art
of
Corpus
Tools
and
Systems
6
1.2.1
A
functional
view
of
corpus
tools
6
1.2.2
Existing
tools
for
the
corpus
linguist
8
1.2.3
Existing
corpus
tools
a
critical
discussion
12
Related
Tools
and
Systems
19
1.3.1
Grammar
development
environments
19
1.3.2
Knowledge
acquisition
systems
20
The
Aims
of
This
Thesis
21
What
This
Thesis
Contains
24
1.5.1
Chapter
by
chapter
24
1.5.2
Some
recurring
themes
27
A
Short
Note
about
Corpora
28
Summary
and
Conclusion
28
CHAPTER
2
Philosophy
of
Corpus
Linguistics
30
2.1
2.2
2.3
2.4
2.5
2.6
Introduction
30
The
Ontology
of
Text
32
2.2.1
Texts,
facts,
and
states
of
affairs
32
2.2.2
Points,
segments,
properties,
property
values,
and
relations
33
Statements
and
Theories
35
2.3.1
Statements
35
2.3.2
Theories
37
Truth
and
Truthlikeness
37
Corpus
Linguistics
as
an
Activity
39
2.5.1
Three
things
linguists
do
40
2.5.2
The
dynamics
of
theory
development
in
linguistics
41
Summary
and
Conclusion
42
viii
CHAPTER
3
The
Logic
of
TagLog
44
3.1
3.2
3.3
3.4
3.5
3.6
3.7
Introduction
44
Requirements
for
the
TagLog
Language
46
The
Language
of
TagLog
47
3.3.1
TagLog
native
syntax
47
3.3.2
The
grammar
notation
49
3.3.3
Condition-action
rules
50
What
is
a
TagLog
Theory
About?
50
3.4.1
The
interpretation
of
terms
51
3.4.2
The
interpretation
of
formulas
52
3.4.3
The
interpretation
of
theories
53
The
TagLog
Built-In
Predicates
53
3.5.1
Segment
predicates
54
3.5.2
String
predicates
56
Reasoning
in
TagLog
57
3.6.1
Reasoning
57
3.6.2
The
resolution
principle
58
3.6.3
Proof
trees
60
Summary
and
Conclusion
60
CHAPTER
4
TagLog
Theories
62
Introduction
62
Text
Descriptions
63
Situation
Descriptions
68
Taxonomies
69
Definitions
71
Grammars
72
4.6.1
Logic
grammars
72
4.6.2
A
grammar
as
an
axiomatic
system
73
4.6.3
What
is
a
TagLog
grammar
about?
73
4.6.4
Grammars
and
grammar
theories
74
4.6.5
Phrase
structure
grammar
75
4.6.6
Categorial
grammar
76
4.6.7
Unification-based
grammar
76
4.7
Lexica
79
4.8
Syntax,
Semantics,
and
the
Modelling
of
Context
81
4.8.1
A
situation
theory
as
a
content
context
model
82
4.8.2
Integrated
grammar
82
4.8.3
Now,
what
s
the
point
of
all
this?
85
4.9
Dialogue
Grammars
86
4.10
Summary
and
Conclusion
88
4.1
4.2
4.3
4.4
4.5
4.6
ix
CHAPTER
5
TagLog
System
Overview
89
5.1
5.2
5.3
5.4
5.5
5.6
5.7
5.8
5.9
Introduction
89
TagLog
for
Interactive
Theory
Revision
90
5.2.1
TagLog
is
a
theory
revision
system
90
5.2.2
TagLog
is
an
interactive
system
91
5.2.3
TagLog
does
this,
user
does
that
92
Against
Tools!
94
5.3.1
Cooperation
between
tools
97
Macintosh
TagLog
User
Interface
99
5.4.1
Menus
99
5.4.2
Windows
100
The
TagLog
Schema
103
The
Choice
of
Interaction
Styles
104
5.6.1
The
pragmatics
of
human-computer
interaction
105
5.6.2
Talking
with
TagLog
tools
about
texts
and
theories
106
Architecture
108
5.7.1
The
current
system
108
5.7.2
Alternative
backbone
systems
109
5.7.3
Client-server
architecture
110
Efficiency
111
Summary
and
Conclusion
111
CHAPTER
6
Description
113
6.1
6.2
6.3
6.4
6.5
6.6
Introduction
113
The
Importance
of
Hand
Coding
114
The
Elements
of
Hand
Coding
115
6.3.1
Observing
115
6.3.2
Deciding
118
6.3.3
Framing
118
6.3.4
Stating
119
6.3.5
Checking
120
6.3.6
Storing
121
Coding
as
a
Dialog
121
Hand
Coding
in
the
TagLog
System
122
6.5.1
Ways
to
speed
up
coding
122
6.5.2
Feedback
when
coding
123
6.5.3
Undoing
statements
123
6.5.4
Macros
123
Summary
and
Conclusion
124
x
CHAPTER
7
Exploring
Linguistic
Reality
125
Introduction
125
Searching
126
7.2.1
Searching
as
deduction
126
7.2.2
Searching
for
syntax
128
7.2.3
Pointing
to
what
is
found
129
7.2.4
Searching
as
a
dialog
130
7.2.5
Searching
and
prediction
130
7.2.6
Precision
and
recall
130
7.3
The
TagLog
Search
Tool
131
7.3.1
Search
Tool
layout
131
7.3.2
How
to
use
the
Search
Tool
132
7.3.3
A
simplified
notation
132
7.3.4
Linking
to
other
tools
133
7.4
Exemplifying
Searching
133
7.4.1
Searching
for
instances
of
a
basic
string
133
7.4.2
Searching
for
adjacency-pair
segments
134
7.4.3
Searching
for
errors
and
incompleteness
134
7.4.4
Tracing
the
Text-Content
Context
relation
135
7.5
Text
in
Co-text
Concordancing
136
7.5.1
The
logic
of
concordancing
136
7.6
The
Concordance
Tool
136
7.6.1
Concordance
Tool
layout
137
7.6.2
How
to
use
the
Concordance
Tool
138
7.6.3
Linking
with
other
tools
138
7.7
Concordancing
Examples
138
7.7.1
A
concordance
of
word
form
tokens
138
7.7.2
Concordancing
based
on
non-string
properties
139
7.7.3
Using
a
theory
to
build
a
concordance
140
7.7.4
The
Concordance
Tool
Statement
Browser
link
140
7.7.5
Concordances
and
stop
lists
141
7.8
Concordancing
for
Hand
Coding
141
7.9
Concordancing
for
Grammar
Development
142
7.10
The
Treebank
Approach
144
7.11
Summary
and
Conclusion
145
CHAPTER
8
7.1
7.2
Browsing
Theories
146
8.1
8.2
8.3
Introduction
146
Retrieval
by
Deduction
147
The
TagLog
Statement
Browser
148
8.3.1
Inspecting
the
statements
describing
a
segment
150
xi
8.4
8.5
8.6
8.7
8.8
8.9
Parsing
as
Deduction
150
Alternative
Theory
Presentation
Modes
153
8.5.1
Alternative
ways
of
presenting
singular
statements
154
8.5.2
Alternative
ways
of
presenting
general
statements
155
Table-Based
Presentation
156
8.6.1
A
simple
table
in
the
abstract
156
The
TagLog
Table
Tool
157
Using
the
Table
Tool
158
8.8.1
Simple
table
158
8.8.2
Table
with
list-valued
columns
158
Summary
and
Conclusion
159
CHAPTER
9
Quantitative
Analysis
160
9.1
9.2
9.3
9.4
9.5
9.6
9.7
Introduction
160
Counting
in
TagLog
161
9.2.1
Counting
types
or
counting
tokens
161
The
TagLog
Count
Tool
162
Frequency
Tabling
162
9.4.1
Exporting
tables
from
TagLog
to
other
programs
165
9.4.2
A
word
length
graph
166
Descriptive
Statistics
in
TagLog
166
9.5.1
Descriptive
statistics,
an
example
168
Other
Uses
of
Tables
168
9.6.1
Listing
high
frequency
words
168
9.6.2
Collocations
169
Summary
and
Conclusion
170
CHAPTER
10
Logical
Extensions
171
Introduction
171
Hard
to
Say
in
Horn
Clause
Logic
172
Explicit
Negation
174
Meta-Logic
176
10.4.1
The
provable
1
predicate
177
10.4.2
The
unprovable
1
predicate
178
10.4.3
The
predicates
observed
1
and
predicted
1
179
10.4.4
The
consistent
1
predicate
180
10.4.5
The
known
and
the
unknown
181
10.4.6
Meta-logic
summary
181
10.5
Non-Monotonic
Reasoning
182
10.5.1
Describing
communicative
functions
in
TagLog
183
10.1
10.2
10.3
10.4
xii
10.6
10.7
Extensions
Applied
185
Summary
and
Conclusion
186
CHAPTER
11
Automatic
Tagging
187
11.1
11.2
11.3
11.4
11.5
11.6
11.7
11.8
Introduction
187
The
Elements
of
Automatic
Tagging
189
11.2.1
Requirements
189
Statistical
Part-of-Speech
Tagging
190
Transformation-Based
Part-of-Speech
Tagging
194
The
Learning
of
Transformation-Rules
196
Logic-Based
Part-of-Speech
Tagging
199
Constraint-Based
Part-of-Speech
Tagging
202
Summary
and
Conclusion
205
CHAPTER
12
Hypothesis
Testing
206
12.1
12.2
12.3
12.4
12.5
12.6
12.7
12.8
Introduction
206
The
Logic
of
Hypothesis
Testing
207
Measuring
the
Status
of
a
Hypothesis
207
12.3.1
Coverage
208
12.3.2
Truthlikeness
208
12.3.3
Conclusiveness
208
The
TagLog
Hypothesis
Testing
Tool
209
Interpreting
the
Results
of
Hypothesis
Testing
210
12.5.1
The
significance
of
predictive
instances
210
12.5.2
The
significance
of
confirming
instances
211
12.5.3
The
significance
of
falsifying
instances
211
12.5.4
The
significance
of
non-covered
instances
212
12.5.5
Summing
up
significance
213
Testing
a
Simple
Hypothesis
213
Testing
a
Grammar
Rule
219
Summary
and
Conclusion
221
CHAPTER
13
Explanation
222
13.1
13.2
13.3
13.4
Introduction
222
Some
Preliminaries
of
Explanation
223
The
Logic
of
Deductive
Explanations
226
Explanation
and
Presentation
228
13.4.1
Graphical
proof
tree
229
13.4.2
Explanation
dialogue
230
13.4.3
Analysis
tree
231
xiii
13.5
13.6
13.7
13.8
13.9
13.10
13.11
13.12
13.13
13.4.4
General
statement
232
13.4.5
General
grammar
rule
233
Abductive
Explanations
234
The
Logic
of
Abductive
Explanations
235
The
TagLog
Explanation
Tool
237
13.7.1
The
Explanation
Tool
layout
237
13.7.2
How
to
use
the
Explanation
Tool
238
Explaining
Syntax
238
Explanation
and
Logic-Based
Tagging
239
Lemmatization
as
Abduction
241
Interpretation
as
Abduction
Revisited
245
Knowledge
Acquisition
as
Abduction
246
Summary
and
Conclusion
247
CHAPTER
14
Grammar,
Truth,
and
Context
248
14.1
14.2
Introduction
248
The
Problem
of
False
Grammars
249
14.2.1
Grammars
revisited
251
14.3
Errors
Tools
Make
253
14.3.1
Parsing
and
the
problem
of
ambiguity
253
14.3.2
Searching
and
the
problem
of
imprecision
257
14.3.3
Automatic
tagging
and
the
problem
of
tagging
errors
259
14.3.4
Truth
and
Usefulness
259
14.4
How
to
Fix
a
False
Grammar
260
14.4.1
An
integrated
approach
to
context
sensitivity
261
14.4.2
A
non-monotonic
approach
to
co-text
sensitivity
264
14.5
Summary
and
Conclusion
266
CHAPTER
15
Conclusion
267
15.1
15.2
15.3
15.4
15.5
Meet
This
Linguist!
267
What
Does
He
Need?
269
How
Do
We
Give
It
to
Him?
270
What
Does
He
Get?
271
What
More
Could
He
Possibly
Ask
For?
273
15.5.1
More
testing
273
15.5.2
Better
formalism
274
15.5.3
More
speed
275
15.5.4
More
functionality
275
15.5.5
More
clarity
276
xiv
References
APPENDIX
A
278
TagLog
Built-In
Predicates
288
A.1
A.2
A.3
A.4
A.5
A.6
A.7
A.8
A.9
A.10
Introduction
288
Point
and
Segment
Predicates
290
String
Predicates
294
Tree
Predicates
297
Set
and
Bag
Predicates
299
Other
Metalevel
Predicates
299
Arithmetic
301
Schema
Predicates
302
Presentation
Procedures
303
Corpus
Management
Procedures
305
APPENDIX
B
Selected
Code
306
B.1
B.2
Viterbi
Tagging
in
Prolog
306
An
Improved
Error-Driven
Learner
309
APPENDIX
C
An
Overview
of
Corpus
Texts
Used
312
C.1
C.2
C.3
C.4
Introduction
312
The
Brown
Corpus
Text
312
The
Doctor-Patient
Dialogue
Corpus
318
The
PLUS
Corpus
Text
323
List
of
Figures
FIG.1.
FIG.2.
FIG.3.
FIG.4.
FIG.5.
FIG.6.
FIG.7.
FIG.8.
FIG.9.
FIG.10.
FIG.11.
FIG.12.
FIG.13.
FIG.14.
FIG.15.
FIG.16.
FIG.17.
FIG.18.
FIG.19.
FIG.20.
FIG.21.
FIG.22.
FIG.23.
FIG.24.
FIG.25.
FIG.26.
FIG.27.
FIG.28.
FIG.29.
FIG.30.
FIG.31.
FIG.32.
FIG.33.
FIG.34.
FIG.35.
FIG.36.
FIG.37.
FIG.38.
FIG.39.
FIG.40.
FIG.41.
FIG.42.
FIG.43.
A
Brown
corpus
text
32
Some
possible
relations
between
two
segments
55
SLD
resolution
proof
tree
59
Proof
tree
60
A
text
and
its
description
65
Text
and
theory
in
the
TagLog
system
67
A
situation
theory
and
a
situation
69
A
taxonomy
of
word
classes
70
A
text,
a
situation
theory,
and
a
situation
82
Integrated
grammar
84
From
text
to
text
with
traditional
corpus
tools
95
TagLog:
tool,
theory,
text
96
Piping
between
tools
97
Tools
making
use
of
uniform
representation
97
From
text
to
annotated
text
to
concordance
99
The
TagLog
system
s
menu
bar
and
some
of
its
menus
99
The
TagLog
Text
window
100
A
TagLog
Theory
window
101
A
Grammar
theory
window
101
A
TagLog
Situation
Theory
window
102
The
TagLog
Dialog
window
103
Four
ways
of
building
a
TagLog
system
109
TagLog
as
a
client-server
system
110
Hand
coding
in
TagLog
120
The
TagLog
Search
Tool
131
Finding
an
adjacency
pair
134
Tracing
the
text-content
context
relation
135
The
Concordance
Tool
dialog
box
137
A
concordance
of
the
139
A
concordance
of
nouns
139
The
Concordance
Tool
Statement
Browser
link
140
Concordancing
for
hand
coding
142
Traditional
retrieval
of
tags
147
The
TagLog
Statement
Browser
148
The
Statement
Browser
in
action
150
Parsing
as
deduction
with
the
Statement
Browser
151
Ambiguity
in
the
eyes
of
the
Statement
Browser
152
Mapping
theories
to
other
modes
of
presentation
154
Mapping
from
clauses
to
grammar
155
Mapping
from
clauses
to
a
graphically
represented
tree
155
The
TagLog
Table
Tool
157
The
TagLog
Count
Tool
162
Tabling
in
TagLog
165
xvi
List
of
Figures
FIG.44.
FIG.45.
FIG.46.
FIG.47.
FIG.48.
FIG.49.
FIG.50.
FIG.51.
FIG.52.
FIG.53.
FIG.54.
FIG.55.
FIG.56.
FIG.57.
FIG.58.
FIG.59.
FIG.60.
FIG.61.
FIG.62.
FIG.63.
FIG.64.
FIG.65.
FIG.66.
FIG.67.
FIG.68.
FIG.69.
FIG.70.
FIG.71.
FIG.72.
FIG.73.
FIG.74.
FIG.75.
Diagram
produced
by
TagLog
and
a
graphing
program
166
The
TagLog
Statistics
Tool
167
Automatic
tagging
189
A
Hidden
Markov
Model
191
Error-driven
learning
Adapted
from
Brill,
1992
196
The
TagLog
Hypothesis
Testing
Tool
209
Testing
a
simple
hypothesis
214
Testing
a
simple
hypothesis
cont.
215
Testing
a
simple
hypothesis
cont.
216
Testing
a
simple
hypothesis
cont.
217
Testing
a
simple
hypothesis
cont.
217
Testing
a
simple
hypothesis
cont.
218
Testing
a
simple
hypothesis
cont.
218
Testing
a
simple
hypothesis
cont.
219
Grammar-rule
testing
in
TagLog
220
Graphical
proof
trees
229
Explanation
dialogue
230
Explanation
dialogue
cont.
230
Explanation
dialogue
cont.
231
Analysis
tree
231
The
TagLog
Explanation
Tool
237
Explanation
in
TagLog
239
Explanation
and
automatic
tagging
240
Explanation
and
automatic
tagging
cont.
241
Lemmatization
as
abduction
243
Lemmatization
as
abduction
cont.
244
Lemmatization
as
abduction
cont.
244
Interpretation
as
abduction
246
Knowledge
acquisition
as
abduction
247
Three
ways
of
handling
ambiguity
256
A
treebank
factory
258
An
integrated
system
with
a
truthlike
grammar
261
CHAPTER
1
Introduction
1.1
Introduction
The
purpose
of
this
thesis
is
to
build
a
corpus
theory
development
environment
to
discuss
its
design,
its
use,
and
its
implementation.
To
do
this
properly,
I
shall
have
to
ask
a
number
of
important
questions:
What
is
a
corpus
text?1
What
is
a
corpus
theory?
What
does
it
mean
to
develop
a
corpus
theory?
But
first
of
all,
I
shall
have
to
motivate
my
plans
for
the
existence
of
yet
another
environment
or
workbench
.
Is
this
something
that
the
linguist
really
needs,
in
addition
to
or
in
replacement
of
traditional
corpus
tools
and
grammar
development
environments?
I
think
so.
Because
if
you
are
a
theoretical
computational
linguist
your
favourite
grammar
development
environment
will
probably
not
be
sensi1.
Already
at
this
point,
let
me
state
clearly
that
the
term
corpus
in
this
thesis
will
be
used
to
denote
written
texts
or
collections
of
written
texts.
Although
some
of
these
texts
are
transcriptions
of
spoken
language
dialogues,
they
are
still
texts,
according
to
my
notion
of
text.
I
do
not
mean
to
imply
that
corpus
linguistics
cannot
deal
with
spoken
language
directly.
On
the
contrary,
the
work
contained
here
might
even
be
possible
to
extend
in
that
direction,
but
the
present
system
just
does
not
deal
with
it.
2
Introduction
tive
to
corpora
in
any
real
sense.
Or
if
your
are
a
corpus
linguist
your
tools
will
probably
not
let
you
use
current
state
of
the
art
grammar
formalisms
to
describe
and
navigate
your
corpus,
which
is
a
pity
since
notions
like
unification
and
inference
are
really
quite
handy
to
have
for
a
corpus
linguist
too.
There
is
a
gulf
here
between
two
traditions,
a
gulf
that
I
would
like
to
see
closed.
Most
traditional
approaches
to
corpus
tools
are
based
on
mark-up
languages.
I
am
suspicious
of
mark-up
languages.
I
don
t
think
they
are
languages.
They
are
notations
without
formalism,
food
for
brainless
text
crunching
programs.
Not
as
a
matter
of
necessity,
but
as
a
matter
of
fact.
My
own
approach
to
corpus
linguistics
is
based
on
logic,
formal
reasoning,
and
an
ambition
to
re
think
scientific
methodology
in
the
area
of
corpus
linguistics.
On
the
conceptual
logical
level
everything
will
be
much
clearer,
I
will
argue,
if
formal
logic,
reasoning,
and
scientific
method
alone
are
allowed
to
reign.
Although
the
purpose
of
this
thesis
may
not
be
that
important
in
itself,
it
is
set
against
a
background
of
really
important
questions,
and
it
may,
I
believe,
have
the
power
to
make
us
see
that
background
in
a
different
light.
So
let
me
begin
there,
in
this
background,
with
a
quote.
Armchair
linguistics
does
not
have
a
good
name
in
some
linguistics
circles.
A
caricature
of
the
armchair
linguist
is
something
like
this.
He
sits
in
a
deep
soft
comfortable
armchair,
with
his
eyes
closed
and
his
hands
clasped
behind
his
head.
Once
in
a
while
he
opens
his
eyes,
sits
up
abruptly
shouting,
Wow,
what
a
neat
fact!
,
grabs
his
pencil,
and
writes
something
down.
Then
he
paces
around
for
a
few
hours
in
the
excitement
of
having
come
still
closer
to
knowing
what
language
is
really
like.
There
isn
t
anybody
exactly
like
this,
but
there
are
some
approximations.
Corpus
linguistics
does
not
have
a
good
name
in
some
linguistics
circles.
A
caricature
of
the
corpus
linguist
is
something
like
this.
He
has
all
of
the
primary
facts
that
he
needs,
in
the
form
of
approximately
one
zillion
running
words,
and
he
sees
his
job
as
that
of
deriving
secondary
facts
from
his
primary
facts.
At
the
moment
he
is
busy
determining
the
relative
frequencies
of
the
eleven
parts
of
speech
as
the
first
word
of
a
sentence
versus
as
the
second
word
of
a
sentence.
There
isn
t
anybody
exactly
like
this,
but
there
are
some
approximations.
These
two
don
t
speak
to
each
other
very
often,
but
when
they
Introduction
3
do,
the
corpus
linguist
says
to
the
armchair
linguist.
Why
should
I
think
that
what
you
tell
me
is
true?
,
and
the
armchair
linguist
says
to
the
corpus
linguist,
Why
should
I
think
that
what
you
tell
me
is
interesting?
Charles
Fillmore
1992
The
table
below
elaborates
on
Fillmore
s
caricatures,
for
the
purpose
of
making
a
more
systematic
comparison
between
the
two
kinds
of
linguists.
They
too
are
just
cartoon
figures,
obviously,
and
should
not
be
taken
too
seriously.
The
Corpus
Linguist
Focuses
on
Parole
Performance
Process.
Aims
for
a
description
of
the
facts
of
language
and
language
use
as
they
appear
in
a
corpus.
Is
data-driven
in
his
approach
to
linguistic
research.
A
lover
of
long
lists.
Is
satisifed
with
simple
measures,
since
they
are
fast,
even
for
large
texts.
Thinks
that
he
can
live
with
the
noise
created
by
the
simplicity
of
his
methods.
Prefers
quantitative
methods.
Sees
himself
as
part
of
an
empiricist
tradition.
Focuses
on
the
text
as
a
physical
product.
Mainly
interested
in
grammars
of
particular
languages.
Focuses
on
form
only.
Takes
a
global
perspective
of
texts.
Wants
an
overview
of
what
is
there.
Focuses
on
broad
though
possibly
superficial
coverage
of
unrestricted
text.
Prefers
methods
based
on
probability
theory
and
statistics.
Relies
on
observation
of
the
products
of
overt
language
behaviour.
Works
with
authentic
data,
and
with
utterances
in
the
context
of
other
utterances.
Has
a
Baconian
frame
of
mind.
Thinks
that
the
inductive
method
is
the
essence
of
science.
Is
a
firm
believer
in
Discovery
procedures.
If
using
a
computer
for
other
things
than
word
processing,
this
would
be
for
building
concordances,
lemmatizing,
or
counting
statistics.
If
he
does
any
programming
at
all,
it
s
in
Pascal
or
C.
The
Theoretical
Linguist
Focuses
on
Langue
Competence
System.
Aims
for
an
explanation
of
the
facts
of
language
and
language
use.
Is
theory-driven
in
his
approach
to
linguistics.
Prefers
sophistication
over
simplicity,
and
accuracy
over
speed,
even
if
that
means
that
he
has
to
be
satisfied
with
smaller
samples.
Prefers
qualitative
methods.
Sees
himself
as
part
of
a
rationalist
tradition.
Focuses
on
the
text
as
an
abstract
entity.
Aims
for
a
Universal
Grammar.
Focuses
on
form
and
meaning.
Takes
a
local
perspective
of
texts,
i.e.
zooms
in
on
them,
and
looks
at
details.
Performs
deep
analyses
of
artificially
restricted
domains.
Prefers
rule-based
knowledge-based,
inference-based,
logic-based
methods.
Relies
on
intuition
introspection
as
the
main
way
of
accessing
data.
Works
with
toy
examples
,
in
the
form
of
isolated,
decontextualized
sentences.
Thinks
that
the
hypothetico-deductive
method
is
the
essence
of
science.
Believes
in
Justification
Evaluation
procedures.
Works
with
parsers,
generators,
and
theorem
provers,
if
he
s
computationally
minded
at
all.
Prolog
or
Lisp
is
what
counts,
if
he
s
at
all
interested
in
programming.
4
Introduction
1.1.1
The
dichotomy
is
false
...
My
conclusion
is
that
the
two
kinds
of
linguists
need
each
other.
Or
better,
that
the
two
kinds
of
linguists,
wherever
possible,
should
exist
in
the
same
body.
Charles
Fillmore
ibid.
Judging
from
the
literature,
most
linguists
tend
to
agree
with
Fillmore
on
this
point.
Most
linguists
seem
to
think
that
the
dichotomy
is
false,
or
at
least,
that
the
oppositions
are
not
as
strong
as
they
have
been
made
to
look
here.
First
of
all,
everybody
is
likely
to
agree
that
both
theory
and
data
are
necessary
ingredients
of
science,
that
science
without
theory
is
most
likely
to
be
uninteresting,
and
that
science
without
respect
for
data
is
likely
to
come
up
with
theories
that
are
false.
Interesting
and
true
scientific
theories
can
only
spring
from
the
interplay
of
both
theory
and
empirical
data.
As
regards
the
question
of
what
linguistics
is
really
about,
many
linguists
simply
do
not
believe
that
the
gulf
between
Langue
and
Parole
between
Competence
and
Performance,
between
System
and
Process,
or
whatever
pair
of
terms
one
prefers
is
as
large
as
one
is
led
to
think.
It
can
...
be
argued
that
the
putative
gulf
between
competence
and
performance
has
been
overemphasised,
and
that
the
affinity
between
say
the
grammar
of
a
language
as
a
mental
construct
and
the
grammar
of
a
language
as
manifested
in
performance
by
the
native
speaker
must
be
close,
since
the
latter
is
a
product
of
the
former.
Geoffrey
Leech
1992
Concerning
our
ways
of
accessing
data,
observation
and
intuition
introspection
are
usually
thought
to
complement
each
other
except,
perhaps,
by
certain
extremists
in
either
camp
.
Grammars
do
not
grow
magically
from
corpora.
They
require
the
intelligent
analytical
mind
of
a
grammarian
who
draws
on
knowledge
of
previous
studies,
on
his
or
her
own
intuition
as
well
as
on
observations
of
text.
Stig
Johansson
1992
Concerning
method,
there
is,
on
the
one
hand,
the
belief
that
statistical
analysis
ought
not
to
be
an
end
in
itself,
but
only
a
starting
point
for
further,
qualitative
analysis
of
data.
On
the
other
hand,
there
is
the
idea
that
statistical
treatments
presuppose
some
initial
classification.
Introduction
5
There
can
be
no
opposition
between
quantitative
and
qualitative
method,
since
quantity
and
quality
are
mutually
complementary
rather
than
exclusive.
Indeed,
every
quantity
is
either
the
numerosity
of
a
collection
of
items
sharing
a
certain
quality,
or
the
intensity
of
a
quality.
Hence,
in
the
process
of
concept
formation,
quality
precedes
quantity
Mario
Bunge
1995
Also,
theoreticians
argue
that
quantities
can
be
added
to
any
qualitative
model
of
language
anyway,
without
sacrificing
any
of
the
existing
features
of
that
model
one
could
think
here
of
probabilistic
logic
and
probabilistic
phrase
structure
grammar
.
All
in
all,
we
may
note
that
matters
are
not
so
polarized
if
they
ever
were
as
they
may
appear:
many
researchers
recognize
the
value
of
statistically
based
techniques
in
the
rule-based
world,
and
vice
versa.
John
McNaught
1993
Concerning
descriptive
versus
theoretical
linguists:
Both
types
of
linguistics
are
valid
in
their
own
terms,
and
should
be
regarded
as
mutually
contributory.
Descriptive
linguistics
can
be
just
as
answerable
to
theory
as
the
theoretical
linguistics
of
language
universals.
In
fact,
descriptive
linguistics
is
more
amenable
to
theory
construction
and
testing
in
accordance
with
the
tenets
of
scientific
method,
because
the
nature
of
its
data
e.g.
utterances
in
a
particular
language
is
less
abstract
and
more
directly
observable.
Geoffrey
Leech
1992
So
to
summarize,
there
are
reasons
to
believe
that
most
corpus
linguists
and
most
theoretical
linguists
realize,
or
would
realize
if
they
thought
more
deeply
about
it,
that
to
take
an
extremist
position
in
relation
to
this
dichotomy
would
not
be
very
wise.
1.1.2
...
but
still
practised
Having
given
proper
attention
to
some
of
those
who
try
to
bridge
the
gap
of
the
dichotomy,
it
remains
to
be
said
that
not
much
work
has
been
done
in
order
to
actually
reconcile
the
two
views.
The
dichotomy
may
be
seen
as
false,
but
it
is
still
practised.
Theoretical
linguists
usually
do
work
with
artificial
examples,
rather
than
authentic
utterances.
Fair
enough,
they
are
often
able
to
say
quite
interesting
things
about
their
examples,
explaining
some
features
of
them
in
6
Introduction
terms
of
other
features,
or
connecting
them
to
meaning
and
context
in
various
ways.
Unfortunately,
the
artificial
flavour
of
the
examples
often
makes
it
less
interesting
than
it
would
otherwise
have
been.
Corpus
linguists
are
usually
preoccupied
with
the
classification,
description,
and
summarizing
of
data,
and
the
result
of
this
work
can
be
rather
uninteresting
from
a
theoretical
point
of
view.
Yet
the
corpus
linguist
still
sits
there
with
his
concordance
programs
and
his
harddisk
filled
with
multi-megabytes
of
texts.
The
theoretical
linguist
may
have
traded
his
armchair
for
a
computer
and
a
couple
of
parsers,
generators,
and
theorem
provers,
but
he
is
not
using
them
for
analysing
text,
only
for
modelling
the
understanding
and
production
of
artificial
examples.
To
bridge
the
gap
of
the
dichotomy
also
in
practice,
I
believe
that
appropriate
tools
will
have
to
be
developed.
This
is
what
I
will
attempt
in
this
thesis.
I
will
approach
this
task
from
the
corpus
linguist
s
point
of
view.
Let
s
begin
with
a
peek
in
his
toolbox.
1.2
The
State
of
the
Art
of
Corpus
Tools
and
Systems
In
this
section,
I
will
take
a
brief,
critical
look
at
the
current
state
of
the
art
of
corpus
tools
and
systems.
As
my
point
of
departure,
I
will
use
a
set
of
informal
functional
specifications
of
tools
of
the
kind
that
corpus
linguists
often
find
useful
in
their
daily
work
on
language
data.
That
is,
I
will
try
to
characterize
different
types
of
tools
in
terms
of
what
they
can
do
for
the
linguist,
rather
than
in
terms
of
what
data
structures
they
manipulate,
or
how
they
are
manipulated.
I
will
then
look
at
some
existing
tools
and
systems,
and
finally,
on
a
generic
level,
I
will
give
some
criticism
levelled
against
these
tools
and
systems.
1.2.1
A
functional
view
of
corpus
tools
Here
is
a
list
of
informal
specifications
of
operations
relevant
to
corpus
work.
Each
specification
lists
the
required
input
as
well
as
the
resulting
output.
Nothing,
however,
is
implied
about
the
representations,
algorithms,
or
heuristics
used.
The
State
of
the
Art
of
Corpus
Tools
and
Systems
7
Hand
coding
-
given
a
description
of
a
text,
a
pointer
to
a
segment
of
text,
and
a
predicate
,
produces
a
predication
which
is
added
to
the
description.
Searching
-
takes
a
text,
perhaps
together
with
a
description
of
it,
and
a
target
specification
e.g.
a
partial
description
,
and
produces
a
set
of
pointers
to
segments
in
the
text
that
satisfies
the
specification.
Concordancing
-
takes
a
text,
perhaps
a
description
of
the
text,
and
a
target
specification,
and
produces
a
concordance
i.e.
a
list
of
words
and
phrases
in
context
that
satisfies
the
specification.
Parsing
-
takes
as
input
a
segment
of
text,
a
grammar,
and
a
parsing
goal,
and
produces
something
that
satisfies
the
goal,
e.g.
a
set
of
assignments
of
categories
to
segments,
or
a
set
of
parse
trees.
Counting
-
takes
a
text,
perhaps
a
description
of
the
text,
and
a
target
specification,
and
returns
the
number
of
text
segments
that
satisfies
the
specification.
Tabling
-
takes
a
text,
perhaps
together
with
a
description
of
it,
and
a
target
specification,
and
produces
a
table
e.g.
a
frequency
table,
a
table
of
collocations,
etc.
.
Collocating
-
given
a
description,
and
a
target
specification,
produces
a
list
of
collocations,
that
is,
a
list
of
words
that
co-occur
more
often
than
expected
by
chance.
Automatic
part-of-speech
tagging
-
takes
a
text,
a
lexicon
and
perhaps
some
rules,
or
transition
probabilities
,
and
produces
a
description
of
the
text,
at
the
level
of
parts-of-speech.
Lemmatizing
-
takes
a
text,
a
lexicon
and
perhaps
some
kind
of
rules
,
and
produces
a
description
of
the
text
which
specifies
the
lemma
for
each
word
token,
so
as
to
abstract
away
from
the
inflected
form
of
it.
Interpreting
-
takes
a
segment
of
text,
a
grammar
or
some
rules
of
inter-
pretation,
and
finds
the
content
corresponding
to
the
segment.
Excerpting
-
given
a
text,
and
a
target
specification
e.g.
a
partial
description
,
produces
strings
corresponding
to
matching
segments.
Manual
disambiguation
-
given
a
number
of
alternative
descriptions,
and
the
user
s
act
of
choosing
between
them,
returns
one
description.
Automatic
disambiguation
-
given
a
number
of
alternative
descriptions,
and
rules
for
selecting
between
them,
returns
one
description.
8
Introduction
This
list
does
not
aim
to
be
exhaustive.
Anyone
could
probably
easily
think
of
other
functionalities
as
well.
Nor
are
the
operations
necessarily
primitive:
automatic
part-of-speech
tagging
involves
automatic
disambiguation,
concordancing
may
involve
searching,
automatic
part-ofspeech
tagging
is
sometimes
regarded
as
one
step
in
the
process
of
parsing,
etc.
Functional
specifications
are
potentially
very
useful.
First,
they
allow
us
to
categorize
different
existing
tools
and
systems
in
terms
of
which
functionalities
they
do,
or
do
not,
support.
Second,
they
give
us
a
way
to
specify
the
functionality
of
a
new
design;
the
service
provided
by
a
corpus
workbench
could
be
completely
specified
by
operations
such
as
these.
Third,
they
allow
us
to
compare
different
tools
of
the
same
category
with
respect
to
how
and
how
well
they
realize
the
functionalities.
Finally,
they
give
us
a
handle
on
an
important
notion
of
correctness:
given
a
particular
implementation,
does
it
correctly
realize
the
functionalities
that
it
aims
for;
does
it
really
do
what
it
is
supposed
to
do?
1.2.2
Existing
tools
for
the
corpus
linguist
A
wide
range
of
implementations
of
tools
and
systems
realizing
the
behaviour
of
the
operations
listed
above
could
be
imagined.
In
this
section
some
more
or
less
well-known
examples
of
such
tools
will
be
given.
Brill
s
rule-based
tagger
This
is
a
tool
for
automatic
tagging
of
parts-of-speech,
developed
by
Eric
Brill
1992
.
It
is
a
rule
based
tagger
where
tagging
is
done
in
two
stages.
First,
every
word
is
assigned
its
most
likely
tag
in
isolation.
For
that
purpose,
a
lexicon
is
consulted.
Words
not
in
the
lexicon
are
first
assumed
to
be
nouns
proper
nouns
if
capitalised
,
and
then
various
cues
are
used
to
change
the
guess
of
the
most
likely
tag.
Next,
contextually
triggered
transformation-rules
of
the
kind
if
a
word
is
tagged
V,
and
the
preceding
word
is
tagged
DET,
then
switch
the
tag
from
V
to
N
are
used
to
improve
accuracy.
Transformation-rules
can
be
acquired
automatically
from
a
training
corpus.
The
CLAN
system
CLAN
is
a
system
of
tools
for
analysing
spoken
language
transcripts
that
are
part
of
CHILDES
a
large
archive
of
child
language
data
The
State
of
the
Art
of
Corpus
Tools
and
Systems
9
MacWhinney,
1990
.
These
are
some
of
the
tools
available
in
the
CLAN
suite:
CHECK
-
Verifies
data
accuracy
and
correct
use
of
coding
syntax;
COMBO
-
Searches
a
file
for
segments
matching
a
regular
expression;
DIST
-
Lists
average
distance
between
words
and
codes;
FREQ
-
Gives
a
word
frequency
count;
KEYMAP
-
Creates
a
concordance;
MLU
-
Calculates
the
mean
length
of
utterance
in
a
file;
WDLEN
-
Tabulates
word
and
utterance
lengths
and
prints
a
histogram.
Each
tool
in
CLAN
is
a
separate
program.
Output
from
one
CLAN
program
can
be
piped
to
another
CLAN
program
for
further
analysis.
The
Constraint
Grammar
system
The
Constraint
Grammar
system
Karlsson,
1990;
Karlsson
et
al.
1995
is
a
rule-based
or
constraint-based
system
for
automatic
tagging
and
parsing
of
running
text.
Tagging
is
performed
in
two
stages:
first,
a
morphological
module
looks
up
all
the
possible
tags
for
each
word
token
in
a
text;
secondly,
a
morphological
disambiguator,
using
a
set
of
carefully
hand
crafted
constraints,
discards
illegitimate
alternatives,
based
on
the
context.
Tags
that
survive
the
constraints
are
assumed
to
be
the
correct
ones.
The
DagTag
system
DagTag
is
a
system
for
hand
coding
of
dialogues,
developed
at
the
University
of
Link
ping
Ahrenberg
J
nsson,
1989
.
It
supports
searching
and
counting.
Tagging
is
performed
by
assigning
DAGs
Directed
Acyclic
Graphs
to
segments
of
text
of
arbitrary
length,
by
first
marking
the
text
with
the
help
of
a
mouse
and
then
selecting
the
appropriate
tag
structure
from
a
menu.
The
tagged
dialogues
can
be
searched
for
occurrences
of
specific
words
or
string
sequences,
or
occurrences
of
special
constructions
that
match
a
certain
DAG.
The
Penn
treebank
The
Penn
treebank
is
a
4-5
million
word
corpus
that
has
been
annotated
for
part-of-speech
information
and
skeletal
syntactic
structure
Marcus,
Santorini,
Marcinkiewicz,
1993
and
predicate-argument
structure
Marcus
et
al.,1994
.
Various
tools
for
manipulating
and
navigating
the
corpus
have
also
been
developed.
The
PARTS
part-of-speech
tagger
Church,
1988
was
used
to
automatically
tag
the
text.
The
Fidditch
partial
parser
Hindle,
1983
was
used
to
parse
the
corpus
automatically.
10
Introduction
Tools
for
tag-disambiguation
and
correction
of
part-of-speech
tagged
text,
as
well
as
tools
for
correcting
analysis
trees
bracketed
structures
were
developed
in
GNU-Emacs
Santorini,
undated
.
Tgrep
is
an
analysis
tool
for
pattern
matching
against
the
skeletally
parsed
corpus.
The
Stuttgart
Corpus
Workbench
The
Corpus
Query
Processor
CQP
is
the
heart
of
a
corpus
management
system
developed
at
the
university
of
Stuttgart
Christ,
1994
.
Other
tools
of
the
system
include
tools
for
parsing,
collocation
extraction,
clustering,
and
statistics-based
tools
for
automatic
parts-of-speech
tagging.
CQP
is
a
command
language
based
query
interpreter.
Queries
are
entered
by
the
user,
the
result
is
computed
and
presented
to
the
user
in
a
simple
concordance
format.
Queries
can
be
seen
as
a
set
of
constraints
which
must
be
fulfilled
by
a
sequence
of
corpus
entities
,
usually
words.
These
constraints
can
be
expressed
by
several
means:
the
central
construct
is
regular
expressions
over
attribute
expressions
,
which
are
in
turn
boolean
expressions.
The
Tagger
system
Tagger
Meyer
Tenney,
1993
is
a
system
that
supports
hand
coding,
counting
statistics
,
and
excerpting.
The
Tagger
system
is
designed
to
be
useful
to
the
corpus
linguist
who
wishes
to
study
a
particular
linguistic
construction
in
detail
and
to
customise
the
tags
that
he
or
she
develops
to
study
the
constructions.
In
brief,
Tagger
is
a
program
that
allows
a
linguist
to
mark
constructions
in
a
corpus,
create
a
series
of
tags
that
could
be
associated
with
the
marked
constructions,
conduct
statistical
analyses
of
the
tags
that
were
created,
and
print
out
examples
of
constructions.
The
Xerox
Part-of-Speech
Tagger
XPOST
XPOST
is
a
hidden
Markov
model
based
part-of-speech
tagger.
Given
a
sentence,
each
word
token
is
assigned
a
part-of-speech
ambiguity
class
from
a
lexicon
e.g.
package
is
in
the
ambiguity
class
noun,verb
.
Words
not
in
the
lexicon
are
subjected
to
suffix
analysis.
A
probabilistic
model
that
assesses
the
likelihood
of
particular
part-of-speech
assignments
based
on
word
order
is
then
applied
to
disambiguate
the
available
choices.
The
final
output
is
a
sentence
with
each
word
tagged
with
the
most
likely
part-of-speech
tag.
This
paragraph
is
quoted
in
its
entirety
from
DFKI
NL
Software
Registry.
The
State
of
the
Art
of
Corpus
Tools
and
Systems
11
The
Xtract
tool
kit
Xtract
is
a
set
of
mainly
statistics-based
tools
for
the
automatic
extraction
of
collocations
from
text
corpora
Smadja,
1993
.
There
is
a
tool
for
finding
sentences
containing
given
collocations,
tools
for
the
extraction
of
all
n-grams
from
a
text
satisfying
certain
conditions
specified
by
the
user,
tools
for
presentation
of
output,
a
frequency
tabling
tool,
etc.
Xtract
can
make
good
use
of
part-of-speech
tags
in
the
input
texts,
if
they
are
given
in
the
right
format.
Sinclair
s
Strategy
This
is
not
really
an
existing
system
at
least
as
far
as
I
know
.
Rather,
it
is
a
list
of
tools
believed
by
a
distinguished
corpus
linguist
to
be
needed
by
the
corpus
researcher:
a
parser,
a
boundary
marker,
a
tagger,
a
collocator,
a
lemmatizer,
a
compounder,
a
disambiguator,
a
lexical
parser,
a
phrase
finder,
an
exemplifier,
a
setter,
a
classifier,
a
typologizer,
etc.
Sinclair,
1992
.
Although
I
cannot
say
I
understand
the
purpose
of
every
one
of
these
tools,
the
idea
seems
to
be
to
isolate
the
smallest
possible
chunks
of
functionality
and
to
implement
these
as
separate
tools.
Then
complex
routines
can
be
built
up
for
particular
applications
by
organizing
several
tools
together
Sinclair,
ibid.,
p.
396
.
In
the
same
paper,
Sinclair
also
presents
what
he
claims
are
guidelines
for
software
design
that
seem
to
be
appropriate
to
the
nineties
in
language
text
processing
Sinclair,
ibid.,
p.
396
:
Analysis
should
be
restricted
to
what
the
machine
can
do
without
human
checking,
or
intervention.
Analysis
should
be
done
in
real
time.
Operations
should
be
designed
to
cope
with
unlimited
quantities
of
text
material.
Software
will
be
designed
to
operate
at
more
than
one
level
of
discrim-
ination,
so
as
to
bypass
doubtful
decisions.
Speed
of
processing
should
take
precedence
over
ultimate
precision
of
analysis.
Software
should
be
robust.
12
Introduction
1.2.3
Existing
corpus
tools
a
critical
discussion
Typically,
existing
corpus
systems
implement
only
some
of
the
functionality
required
by
the
corpus
linguist.
A
corpus
linguist
hoping
to
find
one
single
system
fully
implementing
all
of
the
functionality
listed
in
Section
1.2.1
will
be
disappointed.
In
fact,
some
functions
are
not
supported
by
any
traditional
corpus
system
that
I
know
of.
For
example,
I
have
not
been
able
to
find
corpus
systems
that
relate
any
interesting
representation
of
the
content
of
a
text
to
its
form,
despite
the
fact
that
many
linguists
see
this
as
important.
A
problem
with
current
tools
is
that
often
they
do
not
fully
realize
their
inherent
functional
potential.
Their
functionality
does
not
generalize
the
way
one
expects
it
to.
There
may
be
restrictions
on
the
length
of
segments
that
can
be
handled
perhaps
only
word-long
segments
can
be
marked,
searched
for,
or
counted.
There
may
be
restrictions
on
the
level
of
descriptions
that
can
be
handled
perhaps
only
one
level
at
a
time.
There
may
be
restrictions
on
the
relation
between
tags
perhaps
only
one
tag
per
segment
is
allowed,
perhaps
overlapping
tags
are
forbidden.
Or
there
may
be
a
gross
incompleteness
of
retrieval
tools
with
respect
to
the
information
actually
encoded
in
marked
up
texts
perhaps
the
query
give
me
all
segments
that
consist
of
a
determiner
which
is
not
followed
by
a
noun
,
is
a
query
the
system
cannot
handle.
Many
existing
systems
suffer
from
what
I
will
call
a
lack
of
connectedness
:
a
bad,
inflexible
integration
of
tools,
that
does
not
let
the
user
exploit
already
existing
knowledge
sources,
or
combine
functionality
in
novel,
as
of
yet
unthought
of
ways.
Typically,
knowledge
used
by
one
tool
cannot
be
used
by
another
tool,
not
even
when
the
knowledge
is
relevant
for
the
other
tool
as
well.
Furthermore,
output
from
one
tool
can
often
not
be
taken
as
input
to
another,
not
even
when
this
would
in
principle
be
possible.
Some
systems
lack
full
relevance
for
researchers
coming
from
a
tradition
different
from
the
one
in
which
the
system
was
developed.
A
linguist
writing
unification-based
grammars,
for
example,
who
wants
to
take
advantage
of
existing
corpora
and
corpus
tools
in
his
own
research,
might
find
the
discrepancy
hard
to
handle,
between,
on
the
one
hand,
his
own
systems
of
concepts
and
terminology,
his
own
goals
and
methods,
and,
on
the
other
hand,
the
concepts,
goals,
and
methods
of
the
tradition
from
The
State
of
the
Art
of
Corpus
Tools
and
Systems
13
which
he
borrows
his
corpus
tools.
The
tools
might
not
let
him
structure
and
organize
the
corpus
in
the
way
that
he
would
like,
and
he
will
have
to
learn
to
continuously
translate
between
two
modes
of
thinking:
he
will
have
to
learn
to
translate
questions
concerning
correctness
and
coverage
of
his
grammar
rules
into
queries
that
the
system
can
handle,
and
he
will
have
to
learn
to
turn
the
answers
that
he
gets
into
actions
modifying
or
extending
the
grammar.
Systems
that
are
interactive
in
any
real
sense
ought
to
support
incrementality,
and
reversibility,
and
they
ought
to
supply
immediate
feedback
cf.
Shneiderman,
1992
.
But
many
traditional
corpus
tools
are
all
at
once
systems,
that
do
not
let
us
add
or
delete
information
incrementally,
or
if
they
do
allow
that,
they
are
not
reversible
,
so
that
if
an
algorithm
has
worked
on
a
text,
and
we
remove
some
of
the
information
that
it
used,
the
effects
are
still
not
undone.
Systems
are
often
built
to
work
in
a
batch
oriented,
non-interactive,
non-cooperative,
style.
So
sometimes,
even
when
the
user
feels
that
he
knows
something
that
would
be
of
help
to
the
system,
he
is
not
able
to
share
this
knowledge
with
the
system,
to
cooperate
with
the
system.
Moreover,
in
those
cases
where
a
system
is
interactive,
it
is
so
in
a
rigid
way,
with
the
system
in
control,
and
the
user
at
its
mercy
as
an
oracle
,
supposed
to
answer
the
questions
the
system
asks.
Systems
are
seldom
cooperative,
in
any
real
sense.
Many
systems
have
inadequate
usability.
For
one
thing,
they
often
lack
a
graphical
user
interface
GUI
,
even
when
they
do
allow
some
interaction.
But
I
am
not
so
much
talking
about
a
lack
of
GUIs.
Given
the
current
state
of
the
art
of
user
interface
technology,
any
program
can
be
provided
with
a
GUI.
What
I
am
after
is
rather
the
fact
that
many
existing
tools
and
systems
lack
a
clear
and
consistent
conceptual
model
the
logic
of
their
functionality
is
unclear.
The
often
cited
goal
of
a
successful
interactive
system
having
the
computer
vanish
as
users
become
completely
absorbed
in
their
task
domain
is
therefore
seldom
achieved.
To
sum
up
this
section
so
far,
I
want
to
argue
that
current
corpus
tools
and
systems
are
lacking
in:
multi-functionality
generality
connectedness
14
Introduction
full
relevance
interactiveness
usability
But
these
are
just
symptoms;
I
would
like
to
offer
the
following
diagnosis.
I
do
not
think
that
the
problem
is
bad
engineering,
or
bad
choice
of
data
structures
or
algorithms.
The
problems
go
deeper
than
that.
I
believe
that
the
most
important
source
of
problems
with
the
state
of
the
art
of
corpus
tools
and
systems
is
a
lack
of
appreciation
of
the
importance
of
adequate
representation
formalisms,
query
languages,
and
explicit
reasoning
strategies.
Let
us
investigate,
albeit
at
a
tentative
level,
the
representational
strategy
underlying
the
tools
and
systems
mentioned
above.
Most
of
the
systems
are
based
on
the
general
idea
of
mark-up,
i.e.
labels
are
attached
to
segments
of
text,
describing
their
properties,
or
relations
between
them.
I
will
refer
to
this
idea
as
Tagging
by
Labelling
.2
In
a
particularly
simple
version
of
the
idea,
each
word
is
followed
immediately
by
a
label.
An
occurrence
of
John
loves
Mary
marked
up
in
this
way
would
look
roughly
as
follows:
1
John
pn
loves
v
Mary
pn
Different
versions
of
the
general
idea
exist,
depending
on
the
spatial
orientation
of
the
text
and
how
the
labels
are
attached.
Each
word
can
be
placed
on
a
separate
line,
followed
by
its
tag:
2
John
pn
loves
v
Mary
pn
Or
labels
may
be
located
below
the
words
that
they
attach
to:
3
John
pn
loves
Mary
v
pn
Regardless
of
version,
these
simple
mark-up
languages
can
be
enhanced
with
a
simple
mechanism
for
extending
the
referential
scope
of
a
label.
2.
It
may
be
interesting
to
note
that
according
to
the
Frequently
Asked
Questions
document
FAQ
of
the
newsgroup
comp.text.sgml
,
mark-up
is
a
term
coming
from
the
publishing
and
printing
business,
where
it
means
the
instructions
for
the
typesetter
that
were
written
on
a
typescript
or
manuscript
copy
by
an
editor.
The
State
of
the
Art
of
Corpus
Tools
and
Systems
15
In
the
case
of
the
linear
format,
a
pair
of
brackets
will
do.
One
may
thus
attach
one
label
to
a
sequence
consisting
of
more
than
one
word,
in
the
following
manner:
4
s
John
loves
Mary
By
allowing
embedding
of
one
such
scoped
tag
within
another,
arbitrarily
deep
tree
structures
can
be
created.
5
s
np
John
pn
vp
loves
v
np
Mary
pn
Up
until
now,
I
have
referred
to
a
system
of
mark-up
labels
as
a
language
.
It
should
be
noted,
however,
that
the
status
of
mark-up
languages
as
languages
is
not
completely
clear.
As
far
as
I
know,
no
comprehensive
effort
has
been
made
to
understand
exactly
what
their
status
is.
If
they
really
are
languages,
it
should
be
possible
to
say
something
about
their
syntax,
their
semantics,
their
expressive
power,
and
how
we
may
reason
by
means
of
them.
But
many
things
are
unclear
with
respect
to
these
properties,
even
on
a
very
basic
level.
Consider
the
example
in
3
again,
given
here
as
6
:
6
John
pn
loves
Mary
v
pn
Does
this
mean
that
these
particular
tokens
of
John
,
loves
and
Mary
are
classified
as
a
proper
noun,
verb,
and
proper
noun,
respectively?
Or
does
it
mean
that
the
word
types
represented
by
these
strings
are
classified
in
this
way?
This
is
not
clear.
Should
the
expressions
in
6
also
be
read
as
claims
about
the
particular
order
of
items
in
the
text?
Do
they
claim
that
each
of
the
word-long
segments
is
a
proper
noun,
a
verb,
and
a
proper
noun,
respectively,
and
that
the
verb
follows
the
first
proper
noun
and
that
the
second
proper
noun
follows
the
verb?
One
can
certainly
imagine
an
application
that
is
capable
of
finding
instances
of
verbs
followed
by
instances
of
proper
nouns,
on
the
basis
of
descriptions
like
the
one
in
6
.
But
do
the
expressions
explicitly
claim
that
the
followed-by
relation
holds
between
these
two
segments,
so
that
the
application
can
derive
this
in
the
language,
or
is
this
something
that
the
application
would
have
to
read
off
the
expressions?
This
is
not
clear.
One
could
still
argue
that
the
language
is
so
simple
that
there
is
no
need
for
an
explicit
semantics
or
theory
of
inference.
If
we
can
keep
the
lan-
16
Introduction
guage
as
simple
as
it
appears
in
6
,
this
would
perhaps
be
true.
But
as
soon
as
we
start
to
enhance
it
in
order
to
get
some
more
expressive
power,
we
get
entangled
in
semantic
problems.
Suppose
we
want
to
extend
the
expressive
power
of
our
language
by
allowing
the
attachment
of
more
than
one
label
to
each
word,
as
in
7
,
for
example.
7
John
pn
saw
v,n
Mary
pn
What
does
this
mean?
Does
it
mean
that
this
particular
token
of
saw
is
classified
as
a
noun
and
a
verb?
Or
should
we
understand
it
disjunctively,
as
noun
or
verb?
And
what
conclusions
is
a
system
using
this
information
entitled
to
draw?
If
we
tried
to
search
for
all
instances
of
nouns,
would
this
token
of
saw
be
found
then?
Or
suppose
we
leave
out
a
label,
as
in
the
following:
8
John
pn
loves
Mary
pn
Is
this
supposed
to
mean
that
the
word
in
question
does
not
belong
to
any
part-of-speech?
Or
is
it
supposed
to
mean
that
we
don
t
know
what
partof-speech
it
belongs
to?
Or
is
it
a
sign
of
something
forgotten?
Or
is
it
supposed
to
express
a
set
of
negative
statements:
that
the
word
is
not
a
verb,
not
a
noun,
not
an
adjective,
etc.
I
am
not
saying
that
such
semantic
issues
cannot
be
fixed
for
a
particular
mark-up
language.
I
am
just
saying
that
usually
they
are
not
fixed,
or
if
they
are
fixed,
they
are
fixed
in
a
different
ways
for
different
mark-up
languages,
or
worse,
in
different
ways
for
different
applications
using
the
same
language,
so
that
the
semantics
of
the
language
is
identified
in
practice
with
what
a
particular
application
can
extract
from
a
tagged
corpus
described
by
means
of
it.
Furthermore,
as
regards
expressive
power,
I
am
not
saying
that
the
expressive
power
of
such
a
language
is
too
weak.
I
am
saying
that
I
do
not
know
what
the
expressive
power
is.
In
fact,
expressive
power
of
a
language
is
a
meaningless
notion
if
we
do
not
know
what
the
expressions
in
the
language
mean.
However,
before
we
can
say
anything
about
the
meaning
of
expressions,
we
ought
to
be
clear
about
their
form.
Though,
not
even
that
is
clear.
Are
we
supposed
to
see
the
labels
in
a
marked
up
text
such
as
6
as
expressions
describing
the
words
above
them?
Or
are
The
State
of
the
Art
of
Corpus
Tools
and
Systems
17
we
supposed
to
see
the
whole
of
6
,
including
the
strings
John
,
loves
,
and
Mary
,
as
one
expression
describing
the
original
text,
John
loves
Mary
?
This
is
not
clear.
Speaking
of
weak
expressive
power,
one
must
not
forget
that
this
is
not
necessarily
a
bad
thing,
as
long
as
there
is
enough
expressive
power
for
the
purpose
at
hand.
But
if
expressive
power
is
too
weak,
tagging
problems
are
bound
to
arise,
that
can
only
be
solved
by
hacking
solutions.
Everybody
will
then
start
to
hack
their
solutions
in
different
ways.
Or
a
standard
is
created,
to
make
people
hack
their
solutions
in
the
same
way.
One
area
where
hacking
might
be
called
for
is
where
formal
grammar
theory
meets
traditional
mark-up
languages.
Traditional
mark-up
languages
are
hard
to
reconcile
with
state
of
the
art
grammar
theory.
The
labels
of
traditional
mark-up
languages
are
usually
atomic,
i.e.
they
lack
internal
structure.
By
contrast,
the
labels
in
formalisms
used
by
theoretical
linguistics
formalisms
such
as
PATR
and
DCG
are
structured
objects.
In
a
feature-structure
based
formalism,
the
structure
of
a
label
is
defined
by
its
attributes
and
their
values.
The
value
of
an
attribute
may
be
any
element,
either
atomic
or
complex.
In
a
term-based
formalism,
the
structure
of
a
label
is
determined
by
the
predicate
symbol,
the
arguments,
and
the
order
of
the
arguments.
In
either
case,
the
structure
can
be
used
to
build
an
hierarchy
of
labels
based
on
the
subsumption
relation,
it
can
be
used
to
express
agreement
restrictions,
etc.
This
cannot
be
done
in
a
traditional
mark-up
language.
Traditional
mark-up
languages
have
no
means
of
expressing
generalizations.
Thus
there
are
no
ways
to
write
grammars
in
these
languages;
there
are
no
ways
to
explicitly
account
for
the
relation
between
a
corpus
and
a
lexicon;
there
are
no
ways
to
express
generalizations
to
submit
to
a
hypothesis
testing
procedure;
and
there
are
no
ways
to
express
the
result
of
a
learning
procedure.
Traditional
approaches
to
tools
for
corpus
linguistics
lack
well-defined
query
languages.
This
is
a
source
of
incompleteness:
information
may
be
there,
implicitly
encoded
in
the
pattern
of
labels,
or
encoded
in
some
more
active
data
structure,
but
if
the
query
languages
lack
the
necessary
expressive
power,
we
may
not
be
able
to
retrieve
it.
In
the
above,
I
have
dealt
with
the
lack
of
appreciation
of
the
importance
of
representation
formalisms,
query
formalisms,
and
explicit
reasoning
18
Introduction
strategies.
Another
albeit
related
problem
with
the
state
of
the
art
of
corpus
tools
and
systems
is
a
lack
of
concern
for
matters
of
foundation
a
tendency
for
philosophical
naivety
or
lack
of
interest.
Important
distinctions
are
neglected.
Important
notions
are
not
defined.
The
logic
of
important
knowledge
structures
is
not
analysed.
Ontological
or
epistemological
questions
are
seldom
discussed.
In
the
long
run,
I
believe
that
a
lack
of
discussion
of
first
principles
is
bound
to
affect
corpus
linguistics
and
its
tools
in
a
negative
way.
Yet,
another
and
again
related
problem
is
that
the
purpose
of
some
of
these
corpus
systems
is
somewhat
unclear.
Technology
aims
at
practical
ends,
pure
science
aims
at
understanding,
and
in
between
there
are
all
sorts
of
grey
areas:
technology
more
or
less
principled,
science
more
or
less
applied.
So
what
about
these
tools?
Are
they
supposed
to
form
modules
in
NLP
systems
robust,
error-free
and
efficient
black-box
components
from
which
industrial
applications
can
be
built?
Are
they
supposed
to
support
acquisition
of
knowledge
for
such
NLP
system
modules
to
overcome
the
knowledge
acquisition
bottleneck
of
that
particular
system?
Or
are
they
supposed
to
be
tools
for
the
scientific
investigation
of
text
research
instruments
and
workbenches
to
support
development
of
theories
about
texts?
These
are
three
different
purposes
that
such
systems
can
have.
It
seems
to
me
that
many
designers
of
traditional
tools
try
to
remain
neutral
with
respect
to
these
aims,
with
the
idea
that
their
system
may
serve
all
three
purposes
at
the
same
time.
However,
it
is
easy
to
see
that
the
measure
of
success
is
different
depending
on
the
purpose.
An
NLP
software
module
might
be
regarded
as
successful
if
it
is
modular,
efficient,
robust,
well
engineered,
etc.,
and
if
it
works
correctly
for
input
within
its
intended
range
of
application.
It
does
not
necessarily
have
to
work
with
explicit
representations
that
are
meaningful
to
the
human
eye,
it
doesn
t
have
to
be
able
to
explain
its
behaviour
to
the
user,
and
it
doesn
t
have
to
work
for
unexpected
input.
A
knowledge
acquisition
tool
or
method
for
a
particular
target
application,
on
the
other
hand,
is
regarded
as
successful
if
it
can
help
to
provide
the
NLP
application
with
the
rules,
representations,
databases,
or
data
structures,
that
will
make
it
work
as
intended.
However,
if
corpus
linguistics
is
a
science,
it
ought
to
set
its
goals
higher
than
this.
It
is
here,
I
think,
that
traditional
measures
of
success
of
scientific
theories
comes
in:
truth,
interestingness,
coverage,
simplicity,
and
so
on.
Related
Tools
and
Systems
19
1.3
Related
Tools
and
Systems
In
this
section,
I
present
two
kinds
of
systems
that
are
more
or
less
related
to
corpus
systems.
At
least
they
are
the
kind
of
systems
that
have
inspired
the
design
of
the
corpus
system
proposed
in
this
thesis.
I
am
thinking
here
of
grammar
development
environments
and
knowledge
acquisition
systems.
1.3.1
Grammar
development
environments
A
grammar
development
system
or
environment
is
a
computational
tool
designed
to
assist
the
linguist
in
the
development
of
grammars
for
natural
languages.
Usually,
at
least
the
following
functionality
is
provided:
Parsing
-
takes
a
sentence,
a
grammar,
and
a
parsing
goal,
and
produces
something
that
satisfies
the
goal,
i.e.
usually
a
set
of
parse
trees,
logical
forms,
etc.,
that
reflects
syntactic
and
semantic
properties
of
the
sentence.
Generation
-
takes
a
parse
tree,
and
or
a
logical
form,
a
grammar,
and
a
generation
goal,
and
produces
a
string
of
words
that
satisfies
the
goal.
Typical
systems
also
provide
facilities
for
maintaining
grammars
modifying,
adding,
and
deleting
grammar
rules,
checking
well-formedness
and
consistency,
etc.
and
debugging
grammars
monitoring
or
interacting
with
a
parser,
analysing
failure
to
parse,
etc.
.
Here,
I
will
give
just
one
representative
example
of
such
a
grammar
development
environment:
the
Pleuk
system.3
The
Pleuk
system
Pleuk
Calder
Humphreys,
1994
represents
an
attempt
to
provide
a
shell
within
which
to
situate
implementations
of
a
variety
of
grammar
3.
But
let
me
also,
since
I
am
one
of
the
authors,
briefly
mention
Linguistic
Instruments
Grammar
Laboratories.
First
and
foremost,
this
is
a
set
of
easy-to-use
grammar
development
tools
for
educational
purposes,
but
can
also
be
used
as
lightweight
research
tools.
Each
tool
contains
a
parser
and
a
generator,
facilities
for
printing
analysis
trees,
etc.
Currently
there
exist
versions
for
phrase
structure
grammar,
definite
clause
grammar,
PATR
grammar,
and
categorial
grammar.
20
Introduction
formalisms.
The
system
offers
a
way
of
describing
the
various
modules
of
a
grammar
formalism
and
defining
operations
over
files
and
the
objects
described
in
them,
such
as
compilation,
editing
and
display.
ibid.
.
The
latest
version
of
Pleuk
has
some
facilities
for
testing
against
a
corpus
of
sentences
represented
as
lists
of
words
annotated
with
descriptions
of
what
results
should
be
found.
During
testing
of
a
grammar
against
such
a
corpus,
Pleuk
is
able
to
generate
a
report
of
which
analyses
match
or
fail
to
match
their
expected
results.
1.3.2
Knowledge
acquisition
systems
A
knowledge
acquisition
system
is
a
system
designed
to
assist
a
knowledge
engineer
in
the
development
of
theories
or
models
of
some
application
domain.
Here
is
a
list
of
functionalities
that
a
knowledge
acquisition
system
might
be
able
to
provide:
Generalization
-
given
a
description
of
a
certain
subset
of
a
domain,
generates
a
description
that
characterizes
or
refers
to
a
larger
part
of
the
domain.
Specialization
-
given
a
description
of
a
certain
subset
of
a
domain,
generates
a
description
that
characterizes
or
refers
to
a
smaller
part
of
the
domain.
Explanation
-
given
a
statement,
a
description
of
data,
and
a
back-
ground
theory,
produces
an
explanation
of
the
statement.
Prediction
-
derives
consequences
of
given
knowledge.
Hypothesis
testing
-
given
a
hypothesis,
a
description
of
data,
and
a
background
theory,
produces
some
measure
of
the
empirical
support
for
the
hypothesis.
Consistency
checking
-
given
a
theory,
checks
its
logical
consistency.
I
am
particularly
interested
in
the
kinds
of
systems
that
are
categorized
as
theory
revision
systems.
The
problem
that
a
theory
revision
system
is
designed
to
solve
is
to
revise
an
imperfect
domain
theory
obtained
from
a
textbook,
from
interviews
with
an
expert,
from
the
users
own
intuitions
about
the
domain,
etc.
to
make
it
consistent
with
empirical
data.
Theory
revision
systems
might
be
further
classified
as
interactive
systems
or
balanced
cooperative
systems
Morik,
1994
.
In
an
interactive
theory
The
Aims
of
This
Thesis
21
revision
system,
the
user
is
involved,
but
only
as
an
oracle
;
he
adds
knowledge
to
the
process
when
the
systems
ask
him
for
it,
but
the
system
is
always
in
control.
Clint
DeRaedt,
1992
is
an
example
of
such
a
system.4
In
a
balanced
cooperative
theory
revision
system,
either
the
user
or
the
system
can
perform
a
task
on
the
evolving
theory.
The
control
is
flexible,
and
the
user
is
free
to
manually
add
or
delete
a
fact
or
a
rule
at
any
time,
and
the
system
uses
it.
Systems
in
this
category
include
MOBAL
Morik
et
al.,
1992
.
Here
I
will
only
briefly
describe
one
theory
revision
system
the
balanced
cooperative
system
MOBAL.
MOBAL
MOBAL
Morik
et
al.,
ibid.
is
an
integrated
knowledge
acquisition
and
machine
learning
system.
By
using
MOBAL
s
knowledge
acquisition
environment,
a
user
can
incrementally
develop
a
model
of
an
application
domain
in
terms
of
logical
facts
and
rules.
The
knowledge
entered
can
be
viewed
and
inspected
in
text
or
graphics
windows,
augmented,
or
changed
at
any
time.
The
built
in
inference-engine
can
immediately
execute
the
rules
that
have
been
entered
to
show
the
consequences
of
existing
inputs,
or
answer
queries
about
the
current
knowledge.
Machine
learning
methods
can
be
used
to
automatically
discover
rules
based
on
the
facts
that
have
been
entered,
or
to
form
new
concepts.
If
there
are
contradictions
in
the
knowledge
base
due
to
incorrect
rules
or
facts,
there
is
a
knowledge
revision
tool
to
help
locate
the
problem
and
fix
it
by
recording
exceptions
or
modifying
rules
cf.
Morik
et
al.,
ibid.
.
1.4
The
Aims
of
This
Thesis
Even
if
the
corpus
linguist
and
the
theoretical
linguist
are
reconciled
on
a
philosophical
and
political
level
and
I
think
they
are
it
still
remains
to
set
up
goals,
and
to
develop
tools
and
methods,
that
overcome
the
dichotomy
on
the
practical
level.
This
is
what
I
will
attempt
in
this
thesis.
4.
Note
that
the
term
interactive
is
used
in
a
more
narrow
sense
here.
In
this
thesis,
I
will
use
the
wider
sense
of
this
term,
unless
otherwise
stated.
22
Introduction
At
the
centre
of
our
attention
stands
the
design,
use,
and
implementation
of
a
system
for
the
description
and
analysis
of
corpora,
more
powerful
and
versatile
and
flexible
than
tools
already
existing.
I
have
called
this
system
TagLog.5
The
title
of
the
thesis
reveals
what
I
believe
is
the
key
to
success
in
this
enterprise:
a
logical
approach
to
corpus
linguistics.
This
approach
can
be
summarized
as
follows:
the
use
of
sentences
of
logic
to
express
statements
about
texts;
the
use
of
inference
to
manipulate
these
sentences
in
order
to
analyse
the
texts.
I
intend
to
investigate
the
possibilities
inherent
in
the
approach,
as
well
as
some
of
its
consequences.
I
will
try
to
show
that
logic
may
indeed
play
a
significant
role
in
the
design
of
corpus
tools
and
systems.
A
logical
language
may
be
used
to
logically
reconstruct
traditional
corpus
tools
and
methods.
A
logical
reconstruction
of
a
particular
tool
or
method
is
simply
a
logical
theory
that
together
with
a
certain
reasoning
strategy
is
able
to
explicitly
perform
the
same
reasoning
as
the
tool
is
doing
implicitly,
and
thus,
computationally
interpreted,
realize
the
same
functionality.
To
take
one
simple
example,
parsing
as
deduction
is
a
logical
reconstruction
of
parsing,
in
exactly
this
sense.
What
will
be
demonstrated
is
the
remarkable
ease
with
which
the
functionalities
needed
in
a
corpus
system
are
implemented
when
based
upon
adequate
means
of
representing,
querying,
and
reasoning.
In
fact,
the
proposed
system
implements
all
the
functionality
listed
in
Section
1.2.1
hand
coding,
searching,
concordancing,
parsing,
counting,
tabling,
collocating,
automatic
part-of-speech
tagging,
lemmatizing,
excerpting,
disambiguating,
interpreting
plus
some
of
the
functionality
provided
by
grammar
development
systems
generation
and
presentation
of
analysis
trees,
parse
failure
analysis,
etc.
,
plus
a
a
great
deal
of
the
functionality
provided
by
theory
revision
tools
hypothesis
testing,
explanation,
and
various
kinds
of
learning
.
Thus,
the
need
for
multi-functionality
is
well
catered
for.
5.
TagLog
stands
for
Tagging
in
Logic
just
like
Prolog
stands
for
Programming
in
Logic.
This
name
was
given
to
it
long
ago,
and
it
has
stuck.
But
since
I
no
longer
regard
the
TagLog
system
as
yet
another
tagging
system
,
but
prefer
to
see
it
as
a
corpus
theory
development
environment,
this
name
is
no
longer
very
fitting.
The
Aims
of
This
Thesis
23
Furthermore,
I
will
argue
that
an
increase
in
generality
is
something
that
follows
naturally,
if
we
concentrate
on
reconstructing
the
functionalities
needed
using
a
sufficiently
expressive
logic.
Also,
I
will
try
to
show
that
connectedness
can
be
a
matter
of
using
a
uniform
underlying
representation
format
and
that,
to
a
large
extent,
composing
functionalities
can
be
a
matter
of
logically
interfacing
different
sub-theories
,
or
a
matter
of
composing
expressions
in
the
query
language.
Finally,
I
will
argue
that
in
the
TagLog
system
too,
interactiveness
is
best
supported
by
a
welldesigned
conceptual
model
and
by
the
incrementality
allowed
by
a
closed-loop
system,6
the
reversibility
allowed
by
certain
truth
maintenance
methods,
and
the
feedback
allowed
by
the
conversational
view
of
human-computer
interaction.
Moreover,
the
proposal
made
in
this
thesis
should
not
only
be
regarded
as
a
proposal
for
yet
another
corpus
system
a
contender
of
the
systems
described
in
Section
1.2.2.
I
believe
it
is
a
better
way
of
making
corpus
systems,
but
at
the
same
time,
it
can
be
described
as
a
way
of
avoiding
making
corpus
systems,
and
as
a
way
of
liberating
corpus
linguistics
from
what
I
believe
is
a
rather
unhealthy
reliance
on
tools,
and
make
it
focus
instead
on
theories.
So
when
Sinclair
1992
claims
that
it
is
necessary
to
mount
a
regular
critique
of
any
analytic
software,
to
explore
its
limitation,
check
its
result
and
search
for
improvements.
,
I
think
that
s
fine,
except
that
the
term
analytic
software
should
be
replaced
with
the
term
theory
.
This
is
my
war
cry:
against
tools!
I
will
be
very
explicit
about
the
goal
of
the
approach
to
corpus
linguistics
advocated
in
this
work.
I
would
like
to
rekindle
a
very
old
idea
of
what
science
is
all
about,
and
relate
this
idea
to
corpus
linguistics.
According
to
this
view,
science
is
a
quest
for
true
theories
of
the
world.
I
think
this
view
is
still
valid
despite
the
frequent
instrumentalist
attacks
against
it
,
and
that
it
is
valid
for
corpus
linguistics
too,
and
consequently,
I
would
like
to
regard
corpus
linguistics
as
the
business
of
developing
true
theories
about
texts.
In
fact,
I
would
like
to
elaborate
on
this
a
bit,
and
regard
the
purpose
of
corpus
linguistics
as
the
development
of
true,
wide-coverage,
general,
explicit
theories
about
syntactic,
semantic
and
pragmatic
aspects
of
authentic
texts.
6.
A
closed-loop
learning
system
is
a
learning
system
that
is
able
to
take
the
knowledge
learned
as
an
input
to
another
learning
process
cf.
Michalski,
1994
.
24
Introduction
Thus,
as
far
as
a
methodology
is
proposed
in
this
thesis,
it
will
be
a
methodology
for
the
development
of
such
theories.
As
far
as
tools
are
designed
and
implemented,
they
will
be
tools
to
support
this
methodology,
to
store
and
maintain
general
and
explicit
corpus
theories,
to
allow
testing
of
their
truth
and
coverage,
etc.
In
fact,
the
focus
on
development
of
explicitly
represented
bodies
of
knowledge,
suggests
that
we
regard
the
proposed
system
as
a
corpus
theory
development
environment
analogous
to
a
grammar
development
environment
,
or
as
a
theory
revision
system
tailored
to
the
particular
domain
of
text.
1.5
What
This
Thesis
Contains
In
this
section,
I
provide
an
overview
of
the
content
and
structure
of
this
thesis.
First,
a
chapter-by-chapter
overview
is
given,
then
some
of
the
themes
that
cross
the
chapter
boundaries
are
presented.
1.5.1
Chapter
by
chapter
This
chapter
has
set
the
scene
for
the
whole
of
this
work.
The
rest
of
the
chapters
are
devoted
to
the
articulation
of
the
logic-based
approach
to
corpus
linguistics,
and
to
the
design
and
use
of
the
TagLog
system.
Chapter
2
presents
and
discusses,
under
the
heading
of
Philosophy
of
Corpus
Linguistics
,
some
more
or
less
philosophical
assumptions
concerning
the
nature
of
scientific
theories,
about
reality
linguistic
reality
in
particular
and
about
the
relation
between
theories
and
reality.
Also,
a
picture
of
the
process
of
developing
corpus
linguistic
theories
will
be
sketched,
inspired
by
philosophy
of
science,
and
in
particular
by
the
work
of
Karl
Popper.
In
Chapter
3,
a
notion
of
formal
theory
is
developed,
that
will
allow
us
to
some
extent
capture
the
informal
view
of
a
theory
explained
in
Chapter
2.
Some
important
requirements
for
a
language
for
description
and
analysis
of
text
are
proposed.
Syntax
and
semantics
for
the
formalism
of
choice
Horn
clause
logic
is
given,
and
the
only
rule
of
derivation
that
is
really
needed
the
resolution
principle
is
explained.
What
This
Thesis
Contains
25
In
Chapter
4
we
explore
the
expressive
power
of
the
TagLog
formalism,
and
use
this
power
to
develop
various
example
theories
about
different
aspects
of
text:
data
descriptions,
grammars,
and
lexica,
for
example.
In
Chapter
5,
a
presentation
of
a
Macintosh
implementation
of
the
TagLog
system
is
given.
This
is
just
an
overview,
however,
and
more
detailed
descriptions
of
the
TagLog
analysis
tools
are
given
throughout
the
thesis,
in
connection
with
the
sections
that
describe
the
theory
behind
the
tools,
and
give
examples
of
their
use.
Chapter
6
contains
a
discussion
of
the
process
of
description,
something
that
in
the
TagLog
framework
is
regarded
as
the
process
of
updating
a
theory.
This
chapter
is
concerned
mainly
with
manual
description,
or
hand
coding.
Automatic
tagging
will
be
the
topic
of
Chapter
11.
The
connections
between
the
notion
of
hand
coding
and
the
notions
of
observation
and
pointing
are
accounted
for.
Chapter
7
covers
two
important
aspects
of
data-exploration,
namely
navigation
and
presentation.
It
is
shown
that
the
TagLog
system,
through
an
implementation
of
the
notion
of
searching
as
deduction,
and
by
means
of
a
built-in
predicate
that
implements
a
kind
of
deixis,
is
able
to
find
segments
satisfying
partial
descriptions
provided
by
the
user
and
subsequently
point
them
out
to
him.
It
is
also
demonstrated
how
the
TagLog
concordance
tool
works.
Finally,
it
is
shown
how
navigation
and
presentation
in
this
sense
is
linked
to
observation
and
description
in
the
sense
of
Chapter
6.
In
Chapter
8,
the
browsing
of
theories
is
discussed.
Among
other
things,
the
notion
of
parsing
as
deduction
is
explained.
The
TagLog
Statement
Browser
is
presented
a
tool
designed
to
support
inspection
and
manipulation
of
the
current
description
of
a
particular
segment
of
text.
Whereas
the
Statement
Browser
always
presents
descriptions
in
the
form
of
sets
of
logical
formulas,
it
is
also
argued
that
a
logic-based
system
for
corpus
analysis
is
not
confined
to
looking
at
the
result
of
analyses
in
the
form
of
sets
of
logical
formulas.
Often,
systematic
mappings
between
TagLog
formulas
and
visual
forms
that
convey
the
same
information
exist.
Chapter
9
is
devoted
to
the
quantitative
summary
of
data:
counting,
descriptive
statistics
and
frequency
tabling.
The
TagLog
Table
Tool
is
used
to
generate
listings
of
high
frequency
words
and
collocations.
26
Introduction
In
Chapter
10,
by
the
addition
of
a
restricted
form
of
classical
negation,
and
some
meta-logical
capabilities,
the
expressive
power
of
the
logic
presented
in
Chapter
3
is
significantly
raised
with
very
little
loss
in
terms
of
efficiency.
The
main
motivation
for
the
proposed
extension
is
the
perceived
need
for
dealing
with
negative,
incomplete
and
inconsistent
information.
In
Chapter
11,
the
focus
is
set
on
methods
for
automatic
tagging.
Some
traditional
methods
are
reviewed,
and
logical
reconstructions
of
some
of
them
are
proposed
reconstructions
that
shed
light
on
how
these
methods
work
and
how
they
can
be
generalized.
In
Chapter
12,
the
TagLog
Hypothesis
Testing
Tool
is
presented
a
tool
designed
to
provide
the
researcher
with
a
measure
of
the
degree
of
success
with
which
a
general
hypothesis
describe
the
relevant
facts
of
a
text.
The
significance
of
confirming,
falsifying,
predictive
and
non-covered
instances
of
a
hypothesis
expressed
as
a
universal
statement
is
explained,
and
related
to
the
notions
of
checking,
confirmation,
disconfirmation,
and
modification
of
hypotheses.
In
Chapter
13,
an
explication
of
a
very
important
concept
from
the
philosophy
of
science,
namely
explanation,
will
be
made.
Sentences
expressing
universal
statements
give
rise
to
the
need
to
be
able
to
explain
conclusions,
in
terms
of
the
application
of
such
sentences.
I
deal
with
straightforward
deductive
explanations
as
well
as
with
abductive
or
hypothetico-deductive
explanations.
I
make
an
attempt
to
separate
the
issue
of
the
semantic
content
of
explanations,
from
their
syntax
,
i.e.
how
this
content
is
actually
presented
as
a
proof
tree
in
one
of
its
many
variations,
as
an
ordered
set
of
rule
instantiations,
or
as
a
general
rule
.
In
Chapter
14,
I
will
show
that
given
the
particular
view
of
what
grammar
rules
mean
that
is
favoured
in
this
thesis,
many
rules
in
traditional
grammars
are
strictly
speaking
false,
so
that
they
therefore
make
tools
that
depend
on
them
less
useful
than
they
would
have
been
if
the
rules
were
true.
In
particular,
notions
like
ambiguity
and
imprecision
are
related
to
the
truth
values
of
grammar
rules.
Various
ways
of
correcting
grammars
are
discussed.
In
Chapter
15
the
logical
approach
to
corpus
linguistics
and
its
realization
in
the
TagLog
system
is
assessed
with
respect
to
the
overall
picture
given
in
the
first
chapter.
In
what
sense
can
they
be
said
to
have
bridged
the
gap
What
This
Thesis
Contains
27
of
the
dichotomy
presented
in
the
introduction?
More
generally,
in
what
sense
can
the
advancement
of
our
understanding
of
language
and
language
use
benefit
from
a
logic-based
approach,
and
a
system
like
TagLog?
Finally,
the
thesis
contains
three
appendices:
a
listing
of
a
selected
number
of
TagLog
built-in
predicates,
a
selection
of
various
Prolog
programs
discussed
in
the
thesis,
and
an
overview
of
the
different
corpora
used
to
exemplify
the
logical
approach
to
computational
corpus
linguistics.
1.5.2
Some
recurring
themes
Some
recurring
themes
in
this
thesis,
partly
reflected
in
the
layout
of
the
individual
chapters,
are:
Philosophy.
In
sections
of
this
kind,
I
present
assumptions
concerning
the
subject
matter
and
methods
of
corpus
linguistics.
The
assumptions
are
more
or
less
philosophical
in
nature
they
concern
ontology,
epistemology,
and
philosophy
of
science.
Technical
matters.
In
these
sections,
I
will
specify,
often
in
logical
and
set-theoretical
terms,
the
basic
ideas
and
techniques
underlying
the
design
of
each
of
the
TagLog
tools,
and
the
functionalities
that
they
provide.
The
sections
have
names
like
Tagging
with
a
Theory
,
Searching
as
Deduction
,
Parsing
as
Deduction
,
Lemmatizing
as
Abduction
,
etc.
Often,
the
logical
way
of
doing
things
is
compared
with
more
traditional
techniques.
Manual
extracts.
These
sections
can
be
thought
of
as
extracts
from
the
TagLog
user
manual.
For
each
tool,
they
explain
the
roles
of
the
buttons,
text
entry
areas,
pop-up
menus,
etc.
that
populate
it.
Typically,
a
picture
is
given
that
shows
the
layout
of
the
tool.
These
sections
also
explain,
step
by
step,
how
to
use
the
tools.
The
idea
is
to
exemplify,
and
to
make
more
concrete,
the
material
presented
in
the
technical
sections.
Examples.
These
sections
explain
how
to
solve
particular
problems
in
TagLog,
either
representation
problems
how
can
I
describe
the
linguistic
phenomenon
X
in
TagLog?;
how
can
I
capture
the
relation
of
X
to
Y?
or
analysis
problems
how
can
I
write
a
formula
that
will
allow
me
to
find
out
X?
.
They
do
so
by
presenting
abstract
solutions
in
the
form
of
expressions
in
the
TagLog
formalism,
together
with
naturallanguage
paraphrases
that
explain
what
the
expressions
mean,
and
28
Introduction
sometimes
they
even
give
concrete
instructions
for
how
to
use
the
expressions
in
the
environment
of
the
appropriate
tool
described
in
a
manual
section
in
order
to
solve
a
problem.
In
the
latter
case,
a
picture
of
the
full
screen
is
often
given
that
shows
the
tool
in
action.
Discussions.
In
these
sections,
points
are
brought
up
to
discussion
that
summarize,
conclude,
try
to
make
a
point,
link
different
ideas,
assess,
compare
with
traditional
tools,
relate
to
the
background,
etc.
1.6
A
Short
Note
about
Corpora
Before
closing
this
chapter,
let
me
briefly
introduce
some
of
the
corpus
texts
that
I
will
use
in
this
work.
I
will
use
one
of
the
texts
from
the
now
classic
Brown
corpus
of
written
American
English
Francis
Kucera,
1982
,
I
will
use
a
text-based
dialogue
between
a
user
and
a
system
that
was
recorded
in
a
so
called
Wizard
of
Oz
setup
in
the
ESPRIT
project
PLUS
Nivre,
1992
,
and
I
will
use
a
doctor-patient
dialogue
transcribed
by
K
s-Dienes
forthcoming
in
the
course
of
her
thesis
work.
A
more
detailed
presentation
of
these
texts
can
be
found
in
Appendix
C.
It
must
also
be
said
that
I
myself
will
not
use
these
texts
in
the
manner
that
I
think
a
corpus
linguist
should
use
such
texts:
to
develop
better
theories
about
texts.
Rather,
I
will
use
them
to
discuss
my
ideas
about
what
corpus
linguistics
is
or
ought
to
be,
and
to
demonstrate
the
capabilities
of
my
system.
I
am
not
aiming
at
saying
anything
particularly
interesting
about
the
texts
as
such,
and
should
that
happen
anyway,
it
will
be
by
accident.
1.7
Summary
and
Conclusion
Let
me
end
this
introduction
by
summarizing
the
foci
of
this
thesis.
What
I
am
aiming
for
is
a
corpus
theory
development
environment,
a
system
that
scores
high
in
terms
of:
multi-functionality
generality
Summary
and
Conclusion
29
connectedness
full
relevance
interactiveness
usability
Such
an
environment
can
be
built,
I
think,
by
designing
a
representation
strategy
which
is
expressive
enough
for
the
purpose
declarative
computationally
tractable
and
by
embedding
the
whole
concept
in
a
convincingly
told
philosophical
and
methodological
story,
including
an
ontology
of
text,
an
analysis
of
the
notion
of
theory,
an
explication
of
the
notion
of
truth,
and
other
foundational
issues.
CHAPTER
2
Philosophy
of
Corpus
Linguistics
2.1
Introduction
The
aim
of
this
chapter
is
to
present
and
discuss
some
more
or
less
philosophical
assumptions
about
the
nature
of
scientific
theories,
about
reality
in
particular
linguistic
reality
and
about
the
relation
between
theories
and
reality.
Also,
a
picture
of
the
process
of
developing
corpus
linguistic
theories
will
be
sketched,
inspired
by
philosophy
of
science,1
and
in
particular
by
the
work
of
Karl
Popper.
One
does
not
commonly
find
a
chapter
on
philosophy
and
philosophy
of
science
in
a
text
about
corpus
linguistics.
But
I
have
several
good
reasons
for
including
such
a
chapter
in
this
thesis.
1.
I
take
philosophy
of
science
to
be
the
branch
of
philosophy
which
tries
to
analyse,
define
and
explicate
concepts
like
theory,
explanation,
prediction,
falsification,
verification,
law,
descriptive
generalization,
and
other
important
concepts
relevant
to
science.
Some
philosophers
of
science
deal
specifically
with
particular
sciences
such
as
physics
and
psychology,
and
some
but
fewer
with
the
application
of
philosophy
of
science
to
linguistics
Botha,
1989;
Carr,
1990;
Dyvik,
1992;
Itkonen,
1978;
Yngve,
1986
.
Introduction
31
First,
whether
the
view
of
corpus
linguistics
as
the
business
of
developing
true
theories
about
text
is
a
correct
and
fruitful
view,
does,
of
course,
depend
on
what
notions
of
theory,
text,
truth,
and
theory
development,
we
employ.
If
truth
is
a
matter
of
the
statements
of
a
theory
fitting
the
facts
of
linguistic
reality,
then
the
notions
of
theory
,
statement
,
linguistic
reality
,
and
fact
,
are
indeed
important.
Secondly,
from
a
slightly
more
narrow
point
of
view,
the
logical
approach
to
corpus
linguistics,
and
the
tool
designed
to
support
this
approach,
involves
a
logical
language
in
which
to
form
descriptions
of
texts.
For
such
a
language
to
be
meaningful,
its
symbols
must
be
associated
with
elements
of
text,
in
a
clear
and
systematic
way.
Hence
we
need
to
know:
what
are
the
elements
of
text?
Thirdly,
the
designer
of
a
computer
system
any
computer
system
ought
to
be
guided
by
a
clear
view
of
the
entities
that
are
to
be
manipulated
by
the
system.
According
to
Shneiderman,
1992
the
secret
behind
a
successful
interactive
system
is
to
come
up
with
an
appropriate
representation
of
reality.
Only
then
will
the
designer
be
able
to
create
a
system
that
follows
a
consistent,
coherent
conceptualization.
Also,
the
users
will
form
some
kind
of
conceptual
model
of
the
system
and
its
workings
users
always
do
and
it
is
the
duty
of
the
designer
to
help
the
users
form
a
good
such
model.
In
fact,
corpus
tools
have
been
criticized
for
lack
of
conceptual
clarity
in
their
user
interfaces
cf.
Davies,
1992
.
A
likely
source
of
that
problem
is
an
inadequate
user
s
conceptual
model.
Thus,
this
chapter
has
a
distinct
philosophical
flavour.
I
can
only
hope
that
it
is
sufficiently
clear
that
my
ambitions
in
this
respect
are
fairly
limited.
Let
me
sum
up
the
limitations
as
follows.
The
task
of
providing
a
philosophy
of
corpus
linguistics
is
completely
subsumed
under
the
tasks
of
developing
a
logical
approach
to
corpus
linguistics
and
the
task
of
designing
a
tool
for
corpus
linguistics,
based
on
this
approach.
Also,
I
do
not
claim
that
I
have
provided
the
philosophy
of
corpus
linguistics.
I
think
that
many
views
exist
that
are
compatible
with
the
methodology
and
the
tool
proposed
in
this
thesis.
It
is
for
the
promotion
of
clarity
that
at
least
one
such
view
is
presented,
and
it
is
to
avoid
confusion
that
only
one
such
view
will
be
discussed.
32
Philosophy
of
Corpus
Linguistics
2.2
The
Ontology
of
Text
2.2.1
Texts,
facts,
and
states
of
affairs
What
is
a
text?
Consider
the
following
concrete
example:
9
John
loves
Mary
This
thesis
assumes
an
ontology
according
to
which
a
particular
text
is
a
collection
of
facts.2
One
fact
holding
in
the
example
text
above
is
that
the
first
segment
is
an
instance
of
the
string
John
.
Another
fact
is
that
the
very
same
segment
is
a
noun
phrase
denoting
a
certain
individual.
A
third
fact
is
that
part
of
the
content
or
the
context
of
the
text
states
that
the
individual
is
named
John
,
and
so
on.
Now,
9
certainly
is
not
much
of
a
text.
The
text
shown
in
Figure
1
a
text
from
the
Brown
corpus
is
much
more
representative.
FIGURE
1.
A
Brown
corpus
text
Even
though
this
text
is
not
shown
in
its
entirety
it
consists
of
more
than
two
thousand
words
it
is
easy
to
see
that
it
is
a
much
more
complicated
object
than
the
text
in
9
.
It
involves
far
more
facts,
and
its
syntactic,
semantic,
and
pragmatic
aspects
are
interrelated
in
much
more
complicated
ways.
Still,
that
ought
to
be
a
matter
of
degree,
and
some
parts
of
it
2.
Should
I
ever
get
around
to
writing
my
Tractatus
Taglogico-Philosophicus
,
it
would
surely
begin:
1.
A
text
is
a
totality
of
facts,
not
of
strings.
The
Ontology
of
Text
33
are
not
harder
to
describe
than
the
text
in
9
:
the
segment
between
positions
twenty-two
and
twenty-four
the
segment
highlighted
in
the
text
is
a
noun
phrase,
for
example.
In
this
thesis
we
will
concentrate
on
the
facts
of
linguistic
reality.
However,
texts
are
not
the
only
things
consisting
of
facts.
People,
flowers,
computers,
Christmas
Eves,
wars,
and
hotel
rooms
are
also
collections
of
facts.
To
a
certain
extent,
such
facts
too
are
a
matter
of
concern
for
us,
not
only
by
way
of
comparison
with
linguistic
facts,
but
also
on
their
own,
since
non-linguistic
facts
create
a
context
for
linguistic
fact.
Texts
are
parts
of
the
world;
they
are
in
the
world,
and
they
stand
in
relations
to
lots
of
other
things
that
are
in
the
world.
Texts
have
contents
or,
with
another
term,
interpretations
.
Thus,
not
only
do
the
facts
of
linguistic
reality
involve
segments,
strings,
and
grammatical
categories,
they
also
involve
content.
The
content
can
be
thought
of
as
statements
describing
states
of
affairs
forming
a
situation
that,
in
turn,
involve
individuals,
their
properties,
and
relations
between
them.
The
notion
of
statement
will
soon
be
defined.
Typically,
knowledge
of
the
content
of
a
text
is
gained
by
reading
the
text.
Perhaps
less
evident,
knowledge
of
the
context
of
a
text
can
also
be
gained
by
reading
the
text,
while
trying
to
figure
out
what
one
has
to
know
in
order
to
be
able
to
get
at
the
content
of
the
text,
if
one
did
not
already
know
it.
We
can
also
learn
about
the
described
and
presupposed
situation
and
thus
the
content
and
context
in
other
ways:
by
observing
them
directly,
or
by
reading
other
texts
about
them,
for
example.
Let
us
also
make
a
distinction
between
facts
and
states
of
affairs.
Facts
are
states
of
affairs
that
hold.
States
of
affairs
that
do
not
hold
are
not
facts.
States
of
affairs
are
possible
facts.
As
an
example,
consider
the
state
of
affairs
in
which
the
first
segment
of
our
example
text
in
9
is
an
instance
of
the
lexical
category
verb.
This
does
not
hold,
and
thus,
even
though
it
is
a
state
of
affairs,
it
is
not
a
fact.
2.2.2
Points,
segments,
properties,
property
values,
and
relations
States
of
affairs
are
by
no
means
atomic.
Linguistic
states
of
affairs
of
the
kind
we
are
discussing
here
typically
involve
text
segments
which
in
turn
involve
points
,
properties
and
property
values
of
text
segments,
relations
between
text
segments,
or
between
text
segments
and
entities
in
the
extra-
34
Philosophy
of
Corpus
Linguistics
linguistic
reality.
Extra-linguistic
states
of
affairs
may
involve
other
kinds
of
things,
for
example
objects
things
,
events,
states,
and
statements.
The
notions
of
point
and
segment
can
be
introduced
simultaneously.
A
point
in
a
text
is
simply
that
which
delimits
two
adjacent
segments.
A
segment
of
text
is
that
which
is
delimited
by
two
points
in
the
text.
Segments
have
a
very
special
status
in
the
picture
that
we
are
developing
in
this
thesis.
Segments
make
up
the
substance
of
a
text.
Here,
substance
refers
to
the
old
Aristotelian
idea
of
a
bearer
of
property
,
rather
than
to
Saussure
s
concept
of
substance.
I
will
use
the
term
segment
,
in
this
Aristotelian
fashion,
to
refer
to
the
bearer
of
a
property
relevant
to
text.
Another
very
important
thing
about
segments
is
that
they
are
located
in
time
and
space.
From
this
it
follows
that
they
can
be
observed,
and
that
they
can
be
pointed
at.
The
importance
of
observation
and
pointing
will
become
apparent
as
we
go
along.
Moreover,
as
will
be
seen
in
Chapter
3,
segments
can
be
given
unique
names,
and
these
names
can
be
used
in
order
to
say
things
about
the
segments.
Note
that
segments
and
strings
are
not
the
same.
Rather,
segments
are
instances
of
strings,
and
two
segments
may
very
well
be
instances
of
the
same
string
note
that
in
this
sentence,
the
string
instances
has
four
instances!
.
Of
course,
this
is
just
an
example
of
the
well-known
distinction
between
types
and
tokens.
Strings
are
abstract
entities
types
rather
than
tokens
whereas
segments
are
tokens.
A
final
important
fact
about
segments
is
that
they
are
always
immersed,
as
it
were,
in
context.
It
is
not
common
to
conceive
of
strings,
or
sentences,
in
this
way;
they
are
usually
regarded
as
decontextualized
entities.
Segments,
by
contrast,
are
always
surrounded
by
context.
Usually
a
distinction
between
two
kinds
of
context
is
made:
co-text
and
situational
context.
I
would
like
to
characterize
the
difference
as
follows:
The
distinction
between
co-text
and
situational
context
is
a
matter
of
underlying
substance
.
Co-text
consists
of
text
segments,
and
situational
context
consists
of
states
of
affairs
pertaining
to
the
non-linguistic
world.
Let
us
also
consider
the
distinction
between
a
property
and
a
value
of
a
property.
This
distinction
is
not
controversial.
To
take
an
example
not
relevant
to
text,
colour
would
be
a
property,
while
red,
yellow,
green
etc.,
Statements
and
Theories
35
would
be
possible
values
of
this
property.
Here
s
another
example,
this
time
relevant
to
text:
syncat
short
for
syntactic
category
is
a
property
while
noun,
adverb,
and
noun
phrase
etc.
are
possible
values
of
this
property.
2.3
Statements
and
Theories
In
science,
theories
are
of
great
importance.
In
TagLog
too,
theories
are
important.
However,
there
are
at
least
two
notions
of
theory
around.
A
theory
can
be
regarded
as
a
set
of
sentences,
defined
by
an
axiomatic
system.
This
is
a
syntactic
notion
of
a
theory.
Alternatively,
a
theory
can
be
regarded
as
a
system
of
universal
statements.
This
is
a
semantic
notion
of
a
theory.
These
notions
are
related
in
so
far
as
utterances
of
sentences
express
statements.
I
will
discuss
and
make
use
of
both
notions.
In
this
chapter,
however,
the
focus
will
be
on
the
latter.
As
a
starting
point
an
analysis
of
the
notion
of
statement
is
given.
2.3.1
Statements
Statements
are,
to
put
it
simply,
what
declarative
sentences
assert
to
be
the
case
.
Thus,
statements
fall
into
a
semantic
category.
Statements
can
be
expressed
by
different
means.
For
example,
statements
can
be
expressed
by
utterances
of
sentences,
if
by
sentences
we
mean
natural
language
sentences,
or
formulas
in
a
formal
language.
Statements
can
also
be
expressed
by
non-verbal
means
gestures,
pictures,
and
so
on
or
by
a
combination
of
verbal
and
non-verbal
means.3
Note
also
that
one
and
the
same
statement
can
be
expressed
by
different
means,
e.g.
by
two
different
sentences,
perhaps
in
two
different
languages.
To
illustrate,
consider
the
segment
highlighted
in
the
text
in
Figure
1
on
page
32.
Here
are
a
couple
of
statements
about
that
segment
of
text,
expressed
in
ordinary
English:
10
The
segment
between
position
twenty-two
and
twenty-three
is
a
determiner.
The
segment
between
position
twenty-three
and
twenty-four
is
a
noun.
The
segment
between
position
twenty-two
and
twenty-four
is
a
noun
phrase.
3.
The
making
of
a
statement
by
means
of
a
combination
of
verbal
and
non-verbal
means
will
turn
out
to
be
important.
More
on
this
in
Chapter
6.
36
Philosophy
of
Corpus
Linguistics
A
distinction
is
often
made
between
singular
statements
particular
statements
and
universal
statements.
Singular
statements
are
statements
that
describe
singular
facts.
A
singular
statement
has
the
form
x
is
A
or,
It
is
not
the
case
that
x
is
A
.
An
example
would
be
an
utterance
of
Socrates
is
a
human
,
or
an
utterance
of
This
is
not
a
swan
while
pointing
to
something.
Note
that
the
statements
in
10
are
singular
statements.
Sometimes,
I
will
use
the
term
observation
statement
when
I
want
to
stress
the
connection
to
actual
observation.
Universal
statements
have
the
form
All
A
s
are
B
s
,
or
For
any
x,
if
x
is
A,
then
x
is
B
,
where
A
and
B
are
descriptive
terms.
An
example
would
be
an
utterance
of
Every
human
is
mortal
.
Another,
more
grammar-like
example
is
given
in
11
.
11
For
any
positions
x,y
and
z,
if
the
segment
between
x
and
y
is
a
determiner,
and
if
the
segment
between
y
and
z
is
a
noun,
then
the
segment
between
x
and
z
is
a
noun
phrase.
Thus,
instead
of
saying
about
a
particular
segment,
consisting
of
a
determiner
followed
by
a
noun,
that
it
is
a
noun
phrase,
we
say
about
all
segments
consisting
of
a
determiner
followed
by
a
noun,
that
they
are
noun
phrases.
Not
only
does
the
general
statement
in
11
summarize
the
observations
in
10
.
It
also
transcends
these
observations
and
points
to
other
texts
of
which
it
predicts
that
determiners
followed
by
nouns
will
be
noun
phrases.
This
way
of
transcending
experience
is
a
hallmark
of
a
scientific
descriptive
generalization.
Statements
relate
to
the
rest
of
the
world,
in
that
they
describe
states
of
affairs.
This
is
indeed
an
important
property
of
statements,
and
it
will
be
the
topic
of
Section
2.4.
But
statements
relate
to
each
other
in
various
ways
as
well.
For
our
purposes,
the
most
important
relation
between
statements
is
entailment:
the
fact
that
certain
statements
follow
logically
from
others.
Another
important
notion
is
consistency:
statements
can
be
logically
compatible
or
incompatible.
Relating
to
entailment,
there
are
a
couple
of
important
roles
played
by
statements
in
science
that
deserve
to
be
introduced
at
this
point:
statements
enable
explanations
to
be
made
of
specific
matters
of
facts,
and
they
enable
predictions.4
The
first
two
statements
in
10
,
together
with
4.
As
has
often
been
remarked,
the
logical
structure
of
prediction
and
explanation
is
identical
cf.
Hempel
Oppenheim,
1948
.
Truth
and
Truthlikeness
37
the
statement
in
11
,
for
example,
both
explain
and
predict
the
following
statement:
12
The
segment
between
position
twenty-two
and
twenty-four
is
a
noun
phrase.
Note,
finally,
that
we
need
not
discuss
the
deeply
philosophical
problem:
What
exactly
is
a
statement?
Usually,
they
are
regarded
as
abstract,
but
nothing
really
hinges
on
the
position
that
we
take
on
this
question
here.
2.3.2
Theories
I
will
use
the
term
theory,
without
qualification,
when
I
mean
to
refer
to
a
set
of
statements,
and
leave
unspecified
whether
the
statements
are
universal,
or
singular,
or
both.
But
in
many
circumstances,
it
turns
out
to
be
important
to
distinguish
between
two
kinds
of
theories.
On
the
one
hand,
we
have
universal
theories,
consisting
of
universal
statements
only.
These
are
Theories
with
a
capital
T.
When
we
speak
of
scientific
theories,
these
are
the
kind
of
theories
that
we
have
in
mind.
On
the
other
hand,
sets
of
singular
statements
are
also
theories.
In
the
particular
context
of
corpus
linguistics,
they
are
often
used
to
describe
the
data
of
corpus
linguistics.
When
I
can
do
so
without
causing
confusion,
I
will
allow
myself
to
refer
to
this
kind
of
theories
as
data
descriptions,
or
simply
as
descriptions.
2.4
Truth
and
Truthlikeness
As
mentioned
above,
statements
relate
to
the
rest
of
the
world
in
that
they
describe
states
of
affairs.
In
doing
this,
statements
can
be
true
or
false,
or
their
truth
value
can
be
unknown.
If
the
states
of
affairs
described
by
a
statement
hold
i.e.
if
they
are
facts
the
statement
is
said
to
be
true
otherwise
false
or
unknown
.
For
example,
the
singular
statements
expressed
in
10
are
all
true,
since
the
segment
highlighted
in
Figure
1
on
page
32
is
a
noun
phrase,
and
since
the
first
and
second
part
of
this
segment
is
a
determiner
and
a
noun,
respectively.
On
the
other
hand,
if
the
statement
in
13
is
taken
to
be
about
the
second
part
of
that
segment,
it
is
false,
since
this
is
not
a
verb.
13
The
segment
between
position
twenty-three
and
twenty-four
is
a
verb.
38
Philosophy
of
Corpus
Linguistics
A
universal
statement
of
the
form
For
any
x,
if
x
is
A,
then
x
is
B
is
false
only
if
there
is
an
x
such
that
x
is
A
is
true
but
x
is
B
is
false.
It
is
true
otherwise.
A
true
theory,
then,
would
be
a
theory
in
which
every
statement
is
true.
Truth
is
an
important
property
of
theories;
maybe,
along
with
interestingness
and
coverage,
the
most
important
of
their
properties.
But
truth
is
in
many
ways
a
problematic
notion.
First,
there
are,
some
philosophers
say,
no
criteria
for
truth.
So
there
is
no
way
to
tell
a
true
statement
from
a
false
one,
they
say.
This
claim,
however,
should
be
evaluated
against
the
examples
in
10
and
13
.
Clearly
these
sentences
differ
in
exactly
this
respect:
one
is
true,
and
one
is
false.
It
may
be
that
when
philosophers
say
there
are
no
criteria
for
truth,
they
are
thinking
of
some
kind
of
absolute
truth.
But
absolute
truth
whatever
that
is
need
not
concern
us
here.
We
are
perfectly
happy
with
criteria
based
on
observation
or
whatever
that
let
us
tell
10
and
13
apart.
When
it
comes
to
universal
statements,
matters
are
different.
It
is
characteristic
of
scientific
universal
statements
that
they
claim
something
about
a
very
large
sometimes
infinite
number
of
things,
and
that
what
they
claim
concerns
the
future,
and
the
past,
as
well
as
the
present.
This
means,
as
Karl
Popper
has
repeatedly
stressed,
that
such
a
statement
cannot
be
conclusively
verified.
It
can
only
be
falsified,
since
to
falsify
a
universal
statement,
it
is
enough,
in
principle,
to
find
one
falsifying
instance
of
it.
This
gives
another
meaning
to
the
claim
that
there
are
no
criteria
for
truth,
and
it
means,
indeed,
that
falsity
has
an
important
role
to
play
in
science.
The
quest
for
truth
is
important,
but
notions
of
falsity,
inconsistency,
conflict,
and
error
are
of
no
less
importance,
in
that
they
suggest
points
where
theory
can
be
improved.
Improvement
is
error-driven
,
so
to
speak.
Another
problem,
from
our
point
of
view,
is
that
truth
is
an
all-or-nothing
notion.
One
falsifying
instance
of
a
universal
statement
makes
it
just
as
false
as
a
million
falsifying
instances
would
do.
But
in
practice,
finding
one
falsifying
instance
of
a
statement
is
not
likely
to
make
us
abandon
it,
especially
not
if
we
lack
a
better
alternative.
Furthermore,
given
two
false
statements
or
theories,
one
of
them
might
be
better
than
the
other.
It
seems
that
we
need
a
way
to
speak
of
degrees
of
truth.
Corpus
Linguistics
as
an
Activity
39
I
will
therefore
once
again
borrowing
from
Popper
introduce
a
notion
of
truthlikeness.5
We
have
even
good
reason
to
think
that
most
of
our
theories
even
our
best
theories
are,
strictly
speaking,
false;
for
they
oversimplify
or
idealize
the
facts.
Yet
a
false
conjecture
can
be
nearer
or
less
near
to
the
truth.
Thus
we
arrive
at
the
idea
of
nearness
to
the
truth,
or
of
a
better
or
less
good
approximation
to
the
truth;
that
is,
at
the
idea
of
verisimilitude
.
Popper
1972,
p.
318
This
criterion,
this
measure
of
truthlikeness,
is
going
to
look
roughly
as
follows.
The
higher
the
proportion
between
the
number
of
confirming
instances
and
the
number
of
falsifying
instances
plus
the
number
of
confirming
instances,
the
more
truthlike
is
the
theory.
Hence,
the
degree
of
truthlikeness
of
a
statement,
contrary
to
the
truth
of
a
statement,
can
be
measured
from
what
instances
of
it
we
have
seen
so
far.
So
what
is
to
become
of
truth
in
itself?
I
wish
to
advocate
truth
as
a
regulative
principle
for
TagLog
theory
development.
Even
though
we
may
never
be
completely
sure
if
and
when
we
have
found
truth
for
reasons
given
above
,
truth
is
nevertheless
something
we
ought
to
strive
for.
The
principle
is
simply
this:
think
about
truth.
Do
not
allow
yourself
to
be
satisfied
with
a
theory
just
because
it
works
for
a
certain
purpose.
If
it
works
for
a
certain
purpose,
it
is
presumably
not
totally
wrong,
but
it
may
still
be
possible
to
improve
it
may
still
be
made
more
truthlike
and
may
then
become
useful
for
other
purposes
as
well.
I
believe
that
in
particular
the
results
reported
in
Chapter
14
prove
that
this
is
a
fruitful
methodological
principle
to
keep
in
mind
during
theory
development
work
in
TagLog.
2.5
Corpus
Linguistics
as
an
Activity
An
analysis
of
the
scientific
activity
that
is
to
be
aided
by
the
corpus
tool
proposed
in
this
thesis
must
start
from
some
idea
of
what
the
goals
of
this
activity
are.
I
take
the
main
long-term
goal
of
the
science
of
corpus
lin5.
It
should
be
noted,
however,
that
the
notion
of
truthlikeness
defined
in
this
thesis,
is
not
identical
to
Popper
s
notion.
40
Philosophy
of
Corpus
Linguistics
guistics
to
be
true
and
wide-coverage
and
explanatory
theories
of
texts.
The
goal
is
to
be
realized
through
the
pursuit
of
scientific
activity
in
the
domain.
2.5.1
Three
things
linguists
do
We
will
conceive
of
the
enterprise
of
scientific
corpus
linguistics
in
general
as
being
made
up
of
three
main
kinds
of
interrelated
activities:
1.
Development
of
descriptions
i.e.
sets
of
observation
statements
of
particular
texts
usually
referred
to
as
tagging
or
coding
.
2.
Development
of
descriptive
universal
theories
of
sets
of
texts
e.g.
grammars
.
3.
Development
of
theoretical
principles
e.g.
grammar
theory
.
The
relation
between
a
description
of
a
particular
text
and
a
general
descriptive
theory
must
be
roughly
as
follows:
The
description
of
data
must
fit
be
syntactically
compatible
with
the
statements
of
the
descriptive
theory.
Only
then
is
it
possible
to
test
the
latter
statements
with
respect
to
data,
and
only
then
do
configurations
of
data
suggest
statements
to
be
included
in
the
descriptive
theory.
The
relation
between
a
descriptive
general
theory
and
theoretical
principles
is
roughly
a
follows.
Theoretical
principles
determine
the
structure
of
categories
and
e.g.
grammar
rules.
But
descriptive
theories
do
not
follow
from
theoretical
principles.
Some
properties
follow,
but
essentially,
descriptive
theories
must
be
developed
independently,
in
close
contact
with
data.
It
is
in
the
second
activity
the
work
of
developing
descriptive
general
theories
that
corpus
linguists
and
theoretical
linguists
meet,
or
should
meet.
Yet,
as
I
have
argued,
traditional
computational
tools
used
by
theoretical
linguists
e.g.
grammar
development
environments
do
not
usually
support
access
to
corpora,
while
traditional
corpus
tools
do
not
usually
support
formalisms
in
which
categories
and
grammar
rules
derived
from
theoretical
principles
can
be
formulated.
TagLog
can
be
seen
as
an
attempt
to
fill
this
need.
Corpus
Linguistics
as
an
Activity
41
2.5.2
The
dynamics
of
theory
development
in
linguistics
Popper
1972
proposes
that
the
following
schema
captures
the
dynamics
of
scientific
theory
development.
14
P1
TT
EE
P2
A
problem
P1
is
formulated;
a
tentative
theory
TT
is
proposed;
error
elimination
EE
is
attempted;
and
if
errors
were
found
they
give
rise
to
further
problems
P2,
and
so
on.6
This
is
how
we
might
conceive
of
the
process
of
developing
descriptive
corpus
theories
as
an
instance
of
Popper
s
schema:
1.
Problems
are
detected,
either
in
the
form
of
facts
not
accounted
for
by
the
current
theory,
or
in
the
form
or
contradictions
or
other
kinds
of
errors
that
appear
in
the
current
theory.
2.
Updates
additions
or
deletions
or
modifications
of
statements
to
the
current
theory,
accounting
for
the
new
facts,
or
correcting
the
errors,
are
proposed
tentatively
.
3.
The
relevant
part
of
the
current
theory
is
tested
again
,
that
is,
an
attempt
to
eliminate
errors
is
made.
4.
Often,
new
problems
arise,
concerning
how
to
correct
errors
detected
in
3.
However,
Popper
s
schema
applies
to
the
development
of
theoretical
principles
as
well.
1.
A
big
problem
is
detected,
perhaps
a
source
of
systematic
descriptive
errors.
Something
is
fundamentally
wrong.
2.
A
way
to
restructure
the
whole
theory
is
proposed
tentatively
.
Novel
categories,
new
ways
of
writing
grammar
rules,
etc.
are
introduced.
3.
Testing:
New
descriptive
theories,
formulated
using
the
strategy
proposed
in
2
are
developed
using
the
low-level
schema
.
These
statements
are
tested,
and
errors
that
arise
as
a
consequence
of
them
are
detected.
4.
New
big
problems
may
arise
as
a
consequence
of
performing
3.
6.
This
schema,
or
method,
is
referred
to
by
Popper
alternatively
as
the
method
of
conjecture
and
refutation,
the
critical
method,
or
the
method
of
trial
and
error.
42
Philosophy
of
Corpus
Linguistics
Theoretical
principles
can
be
seen
as
means
of
developing
descriptive
theories.
But
it
is
also
the
case
that
development
of
descriptive
theories
can
be
seen
as
a
way
of
testing
theoretical
principles.
Theoretical
principles
are
testable
with
respect
to
descriptive
theories
in
the
sense
that
a
particular
set
of
theoretical
principles
passes
the
test
only
if
it
allows
the
formulation
of
true
descriptive
theories.
We
will
see
the
radical
revision
of
theoretical
principles
happen
a
couple
of
times
in
the
course
of
this
dissertation.
We
will
place
phenomena
into
novel
theoretical
frameworks,
rather
than
modifying
old
ones
by
piecemeal
changes.
We
will
see
different
grammar
theories,
different
ways
of
looking
at
a
lexicon,
and
different
ways
of
making
grammars
sensitive
to
context.
2.6
Summary
and
Conclusion
In
this
chapter,
matters
of
foundation
were
discussed.
Corpus
linguistics
was
characterized
in
terms
of
its
object
of
study
texts
and
in
terms
of
its
long-term
goal
true,
wide-coverage
theories
about
texts.
Text
was
subjected
to
an
analysis
in
terms
of
facts
and
states
of
affairs.
The
facts
and
states
of
affairs
of
linguistic
reality
involve
segments,
properties,
and
relations.
Segments
in
contrast
to
strings
are
immersed
in
context,
they
can
be
observed,
and
they
can
be
pointed
at.
Segments
can
also
be
named.
A
fairly
standard
view
of
theories
as
systems
of
universal
statements
was
presented,
and
universal
statements
were
contrasted
with
singular
statements.
Statements
were
said
to
describe
states
of
affairs,
and
were
said
to
be
true
if
the
states
of
affairs
described
hold,
but
false
otherwise.
Furthermore,
the
relation
of
entailment
between
statements,
and
the
property
of
consistency
of
a
system
of
statements,
were
introduced.
The
role
of
the
notions
of
entailment
in
prediction
and
explanation
was
anticipated.
Finally,
I
have
argued
for
a
simple,
de-mystified
view
of
truth.
Truth,
not
as
a
matter
of
metaphysics,
but
as
a
methodological
principle.
Also
in
this
chapter,
corpus
linguistics
was
characterized
as
a
particular
kind
of
activity.
The
approach
to
corpus
linguistics
advocated
here
does
not
treat
the
logic
of
corpus
linguistics
in
isolation
from
the
activity
of
Summary
and
Conclusion
43
corpus
linguistics.
The
logical
approach
to
corpus
linguistics
is
logic
in
action.
The
hope
is
that
a
concern
for
first
principles
of
corpus
linguistics
will
help
to
clarify
the
role
of
the
system
and
the
role
of
the
researcher
in
the
scientific
process,
and
thus
give
us,
the
system
designers,
better
ideas
of
what
kind
of
tools
to
implement,
and
how
to
let
the
user
interact
with
them.
Also,
the
hope
is
that
a
system
of
notions
from
the
philosophy
of
science
will
serve
as
a
good
conceptual
model
for
the
users,
and
give
them
the
ideas
and
the
perspective
that
will
support
a
correct
and
fruitful
use
of
the
system.
CHAPTER
3
The
Logic
of
TagLog
3.1
Introduction
In
this
chapter,
a
notion
of
formal
theory
is
developed,
that
to
some
extent
will
allow
us
to
capture
the
informal
view
of
a
theory
explained
in
Chapter
2.
This
notion
of
formal
theory
is
rather
standard
see
Partee,
ter
Meulen,
Wall,
1990
,
in
that
it
consists
of
three
parts:
A
formal
language
A
finite
set
of
axioms
or
axiom
schemata
1
A
finite
set
of
rules
of
inference
From
the
axioms,
a
potentially
infinite
number
of
additional
sentences
the
theorems
are
derivable
by
repeated
application
of
the
rules
of
inference.
Again,
this
is
a
syntactic
notion
of
a
theory,
and
the
inference
rules
are
possible
to
apply
in
purely
mechanical
way,
without
knowing
anything
1.
By
axioms
I
refer
to
non-logical
axioms,
rather
than
logical
axioms.
Introduction
45
about
the
meaning
of
the
formulas.
Obviously,
this
is
exactly
what
makes
them
useful
as
a
basis
for
computation.
The
formal
system
is
of
course
motivated
by
our
desire
to
reflect
corresponding
relations
of
logical
entailment
with
respect
to
statements.
As
will
be
seen,
this
is
where
the
notions
of
intended
interpretation,
soundness,
and
completeness
comes
in.
Language
design
is
a
big
thing
in
computer
science.
Almost
every
large
area
of
application
demands
its
own
language:
programming
languages,
knowledge
representation
formalisms,
database
languages,
grammar
formalisms,
and
so
on.
In
this
chapter,
I
present
my
current
choice
of
formalism
for
description
and
analysis
of
text,
and
I
discuss
some
of
the
design
decisions
that
I
have
made
in
order
to
come
up
with
a
language
to
facilitate
the
talking
about
text
with
computers.
I
hesitate
to
use
the
word
design
though,
because
our
language
will
only
be
a
rather
moderate
extension
of
Horn
clause
logic,
similar
to
Prolog,
but
with
a
different
set
of
built-in
predicates,
and
in
other
ways
tailored
to
its
task.
The
TagLog
formalism
is
a
declarative
language.
One
of
the
great
advantages
of
using
a
declarative
formalism
is
that
expressions
are
possible
to
understand
without
knowledge
of
how
they
will
be
put
to
use.
The
meaning
of
expressions
is
a
matter
of
the
relation
between
the
symbols
of
the
formalism
and
elements
of
the
world,
plus
the
way
the
symbols
are
combined,
and
nothing
else.
In
this
chapter,
but
even
more
so
in
the
next
chapter,
theories
about
text
will
be
presented,
expressed
as
sets
of
axioms
interpreted
by
the
user.
Not
until
later
chapters,
however,
will
the
use
of
these
theories
by
the
TagLog
analysis
tools
be
discussed.
The
third
component
of
a
formal
theory
the
rules
of
inference
will
also
be
given
proper
attention.
In
particular,
reasoning
in
TagLog
by
means
of
the
resolution
principle
is
explained.
First,
however,
before
moving
on
to
an
ample
presentation
of
the
language
of
TagLog,
some
important
requirements
for
such
languages
will
be
discussed.
Towards
the
very
end
of
the
present
chapter,
I
perform
a
brief
evaluation
with
respect
to
these
criteria.
46
The
Logic
of
TagLog
3.2
Requirements
for
the
TagLog
Language
Let
s
list
some
of
the
requirements
for
the
TagLog
language:
1.
It
should
have
the
necessary
expressive
power,
i.e.
it
should
allow
us
to
express
the
statements
that
are
needed
to
form
theories
about
texts,
and
it
should
support
the
functionality
needed
for
analysis
of
the
texts
by
means
of
the
theories.
2.
It
should
have
an
explicit
declarative
semantics,
i.e.
it
should
be
understandable
without
reference
to
the
procedures
that
use
it.
3.
It
should
have
an
explicit
theory
of
inference,
with
known
properties
with
respect
to
soundness
and
completeness
note
that
I
m
not
saying
that
it
must
be
sound
and
complete
.
4.
It
should
be
computationally
tractable.
In
fact,
with
an
eye
to
the
vast
amount
of
input
data
common
in
the
intended
application,
it
must
allow
very
efficient
implementation.
5.
It
should
be
easy
to
read,
easy
to
write,
and
easy
to
learn.
In
my
view,
the
requirements
1
4
are
extremely
important.
However,
I
consider
5
to
be
slightly
less
important,
for
reasons
to
be
given
later
on.
There
is
a
well-known
trade-off
between
the
expressive
power
of
a
language
and
its
computational
tractability.
The
more
expressive
power
a
formalism
has,
the
less
efficiently
we
can
reason
with
it.
To
put
it
drastically,
languages
will
either
be
limited
in
what
knowledge
they
can
represent
or
unlimited
in
the
reasoning
effort
they
might
require
Levesque
Brachman,
1985,
p.
43
.
To
simplify
things,
we
shall
take
as
our
point
of
departure
a
language
that
is
known
to
strike
a
very
good
compromise
between
expressiveness
and
efficiency,
namely
Horn
clause
logic.
There
is
a
simple
and
efficient
inference
rule
for
Horn
clause
logic
the
resolution
principle
that
is
both
sound
and
complete.2
Moreover,
Horn
2.
Prolog
s
proof
procedure
is
incomplete,
however,
since
it
uses
a
depth
first
strategy
for
applying
the
resolution
principle,
that
will
lead
to
non-termination
in
certain
cases.
Also,
due
to
lack
of
occurs
check,
most
implementations
of
Prolog
are
not
sound
either
see
e.g
Hogger,
1990
.
The
Language
of
TagLog
47
clause
logic
is
quite
powerful
and,
perhaps
surprisingly,
most
of
what
we
want
to
be
able
to
express
turns
out
to
be
expressible
in
Horn
clause
logic.
But
it
has
its
limitations:
there
is
no
classical
negation
available,
for
example,
and
as
a
consequence
there
are
certain
statements
that
we
simply
cannot
express.
Now,
for
some
of
the
ideas
that
we
will
want
to
develop
in
the
TagLog
framework,
it
turns
out
that
we
will
need
to
be
able
to
reason
about
negative
information.
In
Chapter
10,
we
will
therefore
extend
the
Horn
clause
logic,
very
carefully,
so
that
no
efficiency
is
lost
in
the
process,
by
adding
a
limited
form
of
classical
negation
and
various
meta-logical
operators.
These
extensions
will
bring
us
more
expressive
power,
but
one
disadvantage,
which
may
not
matter,
is
that
completeness
is
lost,
so
that
a
great
many
conclusions
that
actually
follow
logically
from
a
theory
expressed
in
the
language,
cannot
be
drawn.
What
about
the
requirements
in
5?
Perhaps
these
are
the
requirements
that
seem
to
have
stood
out
as
most
important
for
traditional
approaches
to
tagging
formalisms.
But
if
they
are
given
prominence
over
the
others,
we
are
bound
to
end
up
with
something
simple,
and
hence,
with
something
not
very
expressive.
The
requirements
in
5
should
really
be
formulated
as
follows
instead:
the
language
should
be
as
easy
to
read,
write,
and
learn,
as
possible,
given
the
expressive
power
required.
3.3
The
Language
of
TagLog
Most
of
the
syntax
of
TagLog
has
been
taken
over
from
the
programming
language
Prolog
Clocksin
Mellish,
1981;
Sterling
Shapiro,
1986
.
Prolog
is
a
well-established
programming
language
which
was
invented
in
the
early
seventies
and
for
which
an
ISO
standard
has
now
been
created
ISO,
1995
.
3.3.1
TagLog
native
syntax
Since
the
reader
of
this
thesis
is
likely
to
be
a
linguist,
I
prefer
to
give
the
syntax
for
the
TagLog
language
in
a
simple
context-free
grammar
notation,
rather
than
in
a
style
that
perhaps
mainly
computer
scientists
are
acquainted
with.
The
only
meta-symbol
used
here
is
.
48
The
Logic
of
TagLog
15
taglog
theory
clause
taglog
theory
clause
taglog
theory
clause
predication
:-
predications
.
clause
predication
.
predication
predicate
name
predication
predicate
name
term
sequence
predications
predication
predications
predication
,
predications
predications
predication
;
predications
term
sequence
term
term
sequence
term
,
term
sequence
term
constant
term
number
term
segment
expression
term
set
expression
term
list
term
string
term
variable
name
term
functor
name
term
sequence
set
expression
matrix
predications
matrix
variable
name
matrix
variable
name
,
matrix
query
!-
predications
.
We
now
go
on
to
exemplify
these
constructions,
and
in
passing
introduce
some
more
terminology:
16
17
18
lexcat
3-4,adj
.
function
S,statement
:-
mood
S,declarative
.
S
syncat
S,n
The
expression
in
16
is
a
clause
consisting
of
a
single
predication,
which
in
turn
consists
of
a
predicate
name,
an
opening
parenthesis,
a
segment-expression,
a
comma,
a
constant,
a
closing
parenthesis,
and
a
full
stop,
occurring
in
that
order.
More
specifically,
it
is
a
unit
clause,
or
as
it
is
often
called,
a
fact.
Note
that
the
term
fact
denoted
something
rather
different
in
Chapter
2:
something
in
the
world
rather
than
a
syntactic
construction.
Here,
I
will
allow
myself
to
use
the
term
in
both
senses,
and
rely
on
the
context
for
deciding
between
them.
The
expression
in
17
is
also
a
clause.
The
:-
symbol
is
an
implication
arrow,
pointing
backwards
,
so
that
the
antecedent
is
on
the
right,
and
the
consequent
is
on
the
left.
Variable
names
start
with
an
upper-case
letter,
The
Language
of
TagLog
49
and
variables
are
implicitly
universally
quantified.
Thus,
the
statement
in
17
would
be
written
as
19
in
standard
predicate
logic
notation.
19
s
mood
s,declarative
-
function
s,statement
In
Prolog
terminology,
a
clause
like
the
one
in
17
is
often
called
a
rule.
A
user
with
an
instrumentalist
inclination,
who
sees
the
statements
of
science
as
rules
of
derivation,
may
be
happy
with
this
term.
Other
users,
however,
would
want
to
reserve
the
term
rule
for
grammar
rules
and
condition-action
rules
see
Section
3.3.3
,
and
instead
speak
of
constructions
like
the
one
in
17
as
universally
quantified
implications
,
or
nonunit
clauses
.
The
expression
in
18
is
a
set-expression.
This
is
one
of
the
few
constructions
in
TagLog
that
is
not
part
of
Prolog,
at
least
not
on
the
surface.
3.3.2
The
grammar
notation
In
TagLog,
a
special
purpose
grammar
notation
can
be
used
to
express
grammar
rules.
The
syntax
of
this
notation
is
given
in
20
.
20
taglog
theory
grammar
rule
taglog
theory
grammar
rule
taglog
theory
grammar
rule
head
--
body
.
head
category
body
categories
category
functor
name
category
functor
name
term
sequence
categories
category
categories
category
,
categories
categories
category
;
categories
Here
is
an
example
of
a
grammar
rule
written
in
this
notation,
consisting
of
one
atomic
category
in
the
head,
and
two
categories
in
the
body,
that
both
contain
a
variable
Num:
21
s
--
np
Num
,
vp
Num
.
It
is
important
to
realize
that
grammar
rules
may
just
as
well
be
written
according
to
native
TagLog
syntax.
However,
the
expression
in
21
is
simpler
and
more
familiar
to
a
linguist
than
the
corresponding
expression
in
TagLog
native
syntax
would
be.
The
use
of
grammar
rules
is
exemplified
in
Chapter
4,
and
more
details
on
the
translation
of
grammar
rules
into
native
syntax
are
given
in
Chapter
5.
50
The
Logic
of
TagLog
3.3.3
Condition-action
rules
TagLog
has
at
its
disposal
also
a
type
of
rule
that
I
have
chosen
to
call
condition-action
rule.
The
syntax
for
condition-action
rules
is
as
follows:
22
Condition1,...,
Conditionn
Action1,...,
Actionm
where
Conditioni
and
Actionj
are
TagLog
predications
and
TagLog
actions
such
as
adding
or
deleting
clauses
from
a
theory
,
respectively.
The
meaning
of
such
a
rule
is
that
the
actions
Action1,...,Actionm
are
executed
once
for
each
way
in
which
the
conditions
Condition1,...,
Conditionn
can
be
proved.
An
interpreter
for
condition-action
rules
can
be
implemented
as
in
23
.
23
ca
run
:
Condition
Action
,
call
Condition
,
once
Action
,
fail
true
;
.
Here
s
an
example:
24
bstring
S,
s
add
lexcat
S,v
.
This
rule
will
add
a
statement
lexcat
S,v
,
with
S
instantiated
to
a
segment
expression,
for
each
S
that
instantiates
a
basic
string
that
ends
in
s
.
Condition-action
rules
do
not
express
statements.
They
do
not
have
a
truth
value,
and
they
are
not
logical.
They
do
not
form
theories.
They
are
simply
tools,
powerful
and
handy.
3.4
What
is
a
TagLog
Theory
About?
Section
3.3
provides
a
few
examples
of
what
might
be
considered
axioms:
16
and
17
,
as
well
as
21
.
But
what
do
they
mean;
what
are
they
about?
First,
it
is
very
important
to
note
that
the
aboutness
of
the
sentences
of
a
theory
is,
to
a
great
extent,
a
question
of
intended
interpretation.
It
is
the
user
who
gives
content
to
TagLog
theories.
The
intended
interpretation
of
a
formal
language
specifies
two
things:
What
is
a
TagLog
Theory
About?
51
the
domain,
and
the
meaning
of
the
symbols
of
the
language
in
the
domain
In
our
case,
the
relevant
domains
are
simply
the
constituents
of
the
facts
of
the
linguistic
and
extra-linguistic
reality,
that
were
discussed
in
Chapter
2:
segments
and
other
kinds
of
substance
,
their
properties,
relations
between
segments
and
between
segments
and
other
things,
and
so
on.
The
meaning
is
the
associative
links,
for
most
parts
established
by
the
user,
between
the
constants,
function
symbols,
and
predicate
symbols
of
the
TagLog
language,
and
objects
in
the
domain.
3.4.1
The
interpretation
of
terms
Among
the
terms
in
the
TagLog
language,
we
find
constants,
variables,
and
complex
terms.
Constants
In
general,
individual
constants
denote
simple
property
values,
or
individuals
in
the
text
or
in
the
extra-linguistic
reality.
Most
of
these
links
are
established
by
the
user.
For
example,
the
user
may
decide
to
let
the
constant
np
denote
the
syntactical
category
noun
phrase,
and
he
may
decide
to
let
the
constant
x378
denote
the
university
library
in
G
teborg.
A
point
expression
is
a
special
kind
of
individual
constant
that
denotes
a
rather
special
kind
of
individual:
a
point
in
the
current
text.
The
link
between
the
point
expression
and
the
point
in
the
text
is
established
automatically
by
the
system,
and
hence,
is
not
a
matter
of
the
user
s
association.
As
will
be
seen,
point
expressions
play
an
important
role
in
segment
expressions.
Complex
terms
By
means
of
the
application
of
functor
names
to
term
sequences,
complex
terms,
nested
to
an
arbitrary
depth,
can
be
built.
Typically,
complex
terms
in
the
language
denote
complex
properties,
such
as
grammatical
categories.
For
example,
a
term
like
v
fin
3,sg
,
np,pp
might
be
used
to
denote
a
syntactical
category
finite
verb,
third
person
singular,
subcategorizing
for
a
noun
phrase
and
a
prepositional
phrase
.
52
The
Logic
of
TagLog
A
segment
expression
is
a
special
kind
of
complex
term
consisting
of
functor
-
infixed
between
two
point
expressions
by
means
of
which
each
segment
in
a
text
is
given
a
unique
name
.
For
example,
the
segment
between
the
third
and
the
sixth
point
in
a
text
is
given
the
name
3-6.
We
say
that
3-6
is
a
segment
expression
that
denotes
the
segment
in
question.
3.4.2
The
interpretation
of
formulas
Atomic
sentences
By
applying
predicate
symbols
to
sequences
of
terms,
atomic
formulas
are
composed
in
such
a
way
that
when
stated
they
describe
states
of
affairs.
Such
a
formula
is
true
if
and
only
if
the
relation
denoted
by
the
predicate
symbol
holds
between
the
entities
denoted
by
the
terms
in
the
sequence.
More
formally:
25
P
t1,...,tn
is
true
iff
the
interpretation
of
P
is
an
n-ary
relation
that
holds
between
the
interpretations
of
t1,...,tn
As
a
special
but
very
common
case,
let
us
look
at
tags.
In
order
to
classify
segments
in
the
text,
we
use
atomic
sentences
of
the
form
P
S,V
where
P
is
referring
to
a
property,
where
S
is
a
segment
expression
referring
to
a
segment
in
the
current
text,
and
where
V
is
a
term
referring
to
a
value
of
the
property
denoted
by
P.
We
say
that
P
S,V
is
true
iff
the
segment
has
that
value
for
that
property.
For
example,
syncat
234-236,np
is
true
iff
the
segment
234-236
in
the
current
text
is
in
fact
a
noun
phrase.
Complex
sentences
So
far
we
have
considered
only
the
simplest
sort
of
sentence
atomic
formulas.
A
conventional
truth
conditional
semantics
for
complex
Horn
clause
logic
sentences
in
the
TagLog
language
might
be
given
as
follows.
For
variable-free
or
ground
sentences:3
26
A1
,
A2
is
true
iff
A1
is
true
and
A2
is
true
A1
;
A2
is
true
iff
A1
is
true
or
A2
is
true
A1
:-
A2
is
false
iff
A1
is
false
and
A2
is
true
If
a
clause
contains
variables,
they
are
universally
quantified,
and
thus:
3.
The
term
ground
is
logic
programming
jargon
for
variable-free
.
I
will
use
both
terms
in
this
thesis.
The
TagLog
Built-In
Predicates
53
27
A
is
true
iff
each
of
its
variable-free
instances
is
true
If
a
query
contains
variables,
they
are
existentially
quantified,
and
thus:
28
A
is
true
iff
it
has
some
variable-free
instance
which
is
true
Together,
the
rules
in
25
,
26
,
27
and
28
enable
us
to
determine
the
truth-value
of
any
sentence
under
a
given
interpretation.
3.4.3
The
interpretation
of
theories
Theories
consist
of
sentences.
A
theory
is
true
iff
each
of
its
sentences
is
true.
To
summarize
and
exemplify,
let
us
use
the
TagLog
language
to
express
the
statements
about
the
segment
selected
in
Figure
1
on
page
32
i.e.
the
instance
of
a
complex
.
This
segment
can
be
described
by
means
of
the
clauses
in
29
.
29
syncat
22-23,det
.
syncat
23-24,n
.
syncat
22-24,np
.
The
statements
expressed
by
the
sentences
in
29
are
true,
under
the
obvious
intended
interpretation,
since
again
the
segment
in
question
is
a
noun
phrase,
and
since
the
first
and
second
part
of
this
segment
is
a
determiner
and
a
noun,
respectively.
On
the
other
hand,
the
statement
in
30
is
false,
since
again
the
segment
referred
to
is
not
a
verb.
30
syncat
23-24,v
.
To
say
that
every
segment
consisting
of
a
determiner
followed
by
a
noun
is
a
noun
phrase,
the
sentence
in
31
is
sufficient.
31
syncat
P0-P2,np
:-
syncat
P0-P1,det
,
syncat
P1-P2,n
.
3.5
The
TagLog
Built-In
Predicates
This
section
describes
some
of
the
built-in
predicates
available
in
TagLog.
These
predicates
are
provided
in
advance
by
the
system
and
cannot
be
redefined
by
the
user.
Some
of
the
built-in
predicates
reflect
the
basic
ontology
assumed
in
TagLog
see
Chapter
2
,
in
the
sense
that
they
define
basic
relations
over
54
The
Logic
of
TagLog
segments,
strings,
and
other
entities.
The
relations
can
be
referred
to
in
sentences
formulating
definitions
or
descriptive
generalizations,
and
as
will
be
shown,
they
can
be
referred
to
in
set-expressions
specifying
segments
to
search
for
or
count,
and
in
set-expressions
specifying
concordances
and
tables.
The
following
categories
of
such
built-in
predicates
are
available:
1.
Segment
predicates
2.
String
predicates
3.
Tree
predicates
4.
Meta-level
predicates
5.
Arithmetic
6.
Schema
predicates
Other
TagLog
built-ins
define
the
functionality
of
the
system,
in
terms
of
what
analyses
can
be
performed,
and
how
their
results
can
be
presented:
7.
Presentation
procedures
8.
Corpus
text
and
corpus
theory
management
procedures
3.5.1
Segment
predicates
The
notion
of
segment
is
extremely
important
in
TagLog,
and
the
TagLog
system
comes
with
a
family
of
built-in
predicates
to
deal
directly
with
segments.
Here,
only
an
overview
of
the
most
important
segment
predicates
is
given,
but
a
complete
and
systematic
listing
of
all
the
available
predicates
can
be
found
in
Appendix
A.
First,
there
are
predicates
for
checking
if
something
is
a
segment
in
the
current
text,
and
there
are
predicates
for
determining
the
length
of
a
segment.
As
explained
in
Chapter
2,
a
segment
is
a
piece
of
text
substance
stretching
between
two
points
in
the
text.
Consider
the
utterance
in
32
,
where
the
relevant
points
are
indicated
by
subscripts.
32
1
bla
2
blabla
3
bla
4
blablabla
5
The
TagLog
Built-In
Predicates
55
Here,
1-2
is
a
segment,
2-3
is
another
segment.
As
a
matter
of
fact,
there
are
10
different
segments
in
32
.4
The
predicate
segment
1
is
true
of
each
of
them.5
The
length
of
a
segment
is
given
by
the
number
of
points
between
the
points
that
delimit
the
segment,
plus
one.
Thus,
the
length
of
the
segment
between
two
adjacent
points
is
1.
The
length
of
the
segment
2-5
in
32
is
3.
The
TagLog
built-in
predicate
segment
length
2
holds
between
a
segment
and
its
length.
A
basic
segment
is
a
segment
of
length
1.
The
predicate
bsegment
1
is
true
of
each
of
the
basic
segments
in
the
current
text.
Figure
2
gives
an
overview
of
some
of
the
possible
relations
that
can
hold
between
two
segments.
S1
is
a
prefix
of
S2
S1
precedes
S2
S1
is
a
suffix
of
S2
S1
is
adjacent
to
S2
S1
is
a
subsegment
of
S2
S1
and
S2
overlap
FIGURE
2.
Some
possible
relations
between
two
segments
4.
There
are
n
-
k
1
subsegments
of
length
k
in
a
segment
of
length
n.
The
total
number
of
subsegments
is
therefore
given
by:
n
k
1
k
1
n
k
1
k
n
1
-2
n
n
n2
n
------------------2
5.
I
do
not
assume
that
there
are
discontinuous
segments.
I
suspect
that
what
most
people
are
tempted
to
refer
to
by
that
name,
is
something
that
I
would
regard
as
two
or
more
segments,
rather
than
one
segment
distributed
over
different
locations.
56
The
Logic
of
TagLog
There
are
predicates
to
relate
a
segment
to
its
parts:
prefixes,
suffixes
or
subsegments;
and
there
are
predicates
that
relate
a
segment
to
surrounding
and
overlapping
segments.
The
predicate
segment
prefix
2
holds
between
a
segment
and
its
prefix
segments,
whereas
segment
suffix
2
holds
between
a
segment
and
its
suffix
segments.
The
predicate
subsegment
2
holds
between
a
segment
and
its
subsegments.
In
general,
these
predicates
can
be
used
called
with
all
sorts
of
instantiation
patterns,
i.e.
they
can
be
used
to
check
if
two
segments
stand
in
a
particular
relation,
or
they
can
be
used
to
find
a
segment
that
stands
in
a
particular
relation
to
a
given
segment.
They
can
even
be
used
to
enumerate
all
the
pairs
or
triples
of
segments
that
stand
in
a
particular
relation.
3.5.2
String
predicates
As
we
saw
in
Section
2.2.2,
segments
and
strings
must
not
be
confused.
Strings
are
very
important
objects
in
TagLog,
almost
as
important
as
segments.
Consequently,
there
are
built-in
predicates
for
reasoning
about
strings.
In
certain
respects
they
are
isomorphic
to
the
segment
predicates,
although
great
differences
exist
as
well.
Thus,
TagLog
provides
the
user
with
predicates
for
reasoning
about
the
relation
between
segments
and
strings.
There
is
a
predicate
for
computing
the
length
of
a
string,
and
there
are
predicates
for
reasoning
about
the
relations
between
a
string
and
its
prefixes,
suffixes
and
substrings.
The
atomic
formula
string
Segment,String
is
true
iff
Segment
is
a
segment
in
the
current
text
and
String
is
the
string
of
which
Segment
is
an
instance;
bstring
Segment,String
is
true
iff
Segment
is
a
basic
segment
in
the
current
text
and
String
is
the
string
of
which
Segment
is
an
instance.
The
predicate
string
length
String,Length
is
true
iff
Length
is
the
length,
i.e.
the
number
of
characters,
of
String.
The
atomic
formula
string
prefix
String1,String2
is
true
iff
String2
is
a
prefix
of
String1.
A
similar
predicate
for
reasoning
about
the
suffix
relation
is
available
as
well.
Finally,
substring
String1,String2
is
true
iff
String2
is
a
substring
of
String1.
Reasoning
in
TagLog
57
3.6
Reasoning
in
TagLog
Logical
reasoning
is
the
basis
for
prediction
and
explanation
in
TagLog,
and
an
integral
part
of
the
generalized
procedures
for
searching,
counting,
concordancing,
etc.,
that
are
supported
by
TagLog.
3.6.1
Reasoning
Reasoning
theorem
proving,
inferencing
is
the
business
of
trying
to
determine
whether
a
formula
A
is
a
theorem
in
the
current
theory
or
not,
i.e.
whether
A
holds
or
not.
Suppose,
for
example,
that
we
want
to
prove
X
human
X
in
a
theory
consisting
of
the
following
clauses
for
clarity,
I
have
used
explicit
quantifiers
here
:
33
biped
socrates
featherless
socrates
X
human
X
:-
biped
X
,
featherless
X
No
matter
what
sound
and
complete
proof
method
for
Horn
clause
logic
we
use,
we
will
get
the
same
result.
For
example,
we
could
prove
X
human
X
by
performing
a
bottom-up
natural
deduction
proof
which
is
called
natural
since
it
is
supposed
to
be
the
way
humans
perform
reasoning
.6
From
X
human
X
:-
biped
X
,
featherless
X
,
infer
human
socrates
:-
biped
socrates
,
featherless
socrates
Universal
Instantiation
.
From
biped
socrates
and
featherless
socrates
,
infer
biped
socrates
,
featherless
socrates
Conjunction
Introduction
.
Then,
by
Modus
Ponens,
infer
human
socrates
from
biped
socrates
,
featherless
socrates
and
human
socrates
:-
biped
socrates
,
featherless
socrates
.
Then
finally
infer
X
human
X
from
human
socrates
Existential
Generalization
.
The
TagLog
theorem
prover
does
not
use
natural
deduction
to
derive
theorems,
and
it
does
not
reason
from
the
bottom
and
up.
Nevertheless,
the
proofs
generated
by
the
TagLog
theorem
prover
could
be
thought
about
in
this
way.
6.
See
e.g.
Allwood
et
al.,
1977
for
a
short
introduction
to
reasoning
by
natural
deduction.
58
The
Logic
of
TagLog
Alternatively,
we
could
think
about
the
initial
A
as
a
query
is
there
an
X
such
that
X
is
human?
.
In
an
attempt
to
answer
the
main
query,
the
prover
uses
the
clauses
from
the
current
theory
in
order
to
break
the
query
down
into
simpler
queries
and
to
answer
the
simplest
ones.
Is
there
an
X
such
that
X
is
human?
Is
there
an
X
such
that
X
has
two
legs
and
X
has
no
feathers?
Is
there
an
X
such
that
X
has
two
legs?
Yes,
Socrates!
Is
Socrates
featherless?
Yes!
So
there
is
an
X
such
that
X
is
human,
namely
Socrates!
In
fact,
this
is
more
in
accordance
with
how
proofs
are
actually
handled
by
the
system,
top-down
and
all.
So
how
is
it
actually
done?
The
resolution
principle
is
somewhat
more
complicated
than
many
other
inference
rules,
since
it
is
machine
oriented,
rather
than
human
oriented
as
Robinson,
the
inventor
of
resolution,
puts
it
in
his
1965
paper.
Even
though
a
user
of
the
TagLog
system
does
not
have
to
know
the
intricacies
of
resolution-based
proofs,
a
brief
explanation
is
nevertheless
included
here
for
completeness.
3.6.2
The
resolution
principle
In
this
section,
a
rather
standard
introduction
to
the
resolution
principle
will
be
given,
akin
to
and
inspired
by
the
one
given
by
Hogger,
1990
.
Before
presenting
the
resolution
principle
as
such,
I
will
introduce
the
notions
of
substitution
and
most
general
unifier.
A
substitution
is
a
finite
set
of
pairs
of
the
form
U
t
where
U
is
a
variable
and
t
is
a
term.
Thus
any
non-empty
substitution
takes
the
form
U1
t1,...,Um
tm
.
The
application
of
to
any
formula
A
is
the
act
of
simultaneously
replacing,
for
all
i
1,..,m
,
every
free
occurrence
of
Ui
in
A
by
ti.
The
result,
denoted
by
A,
is
said
to
be
a
substitution
instance
of
A.
A
unifier
of
two
atomic
formulas
A1
and
A2
is
a
substitution
such
that
A1
A2;
is
the
most
general
unifier
mgu
of
A1
and
A2
iff
for
every
other
unifier
of
A1
and
A2,
A1
is
a
substitution
instance
of
A1.
For
example,
the
substitution
X
socrates
is
the
mgu
for
the
formulas
human
X
and
human
socrates
.
A
proof
by
resolution
is
a
proof
by
contradiction:
we
want
to
prove
A,
so
we
assume
the
negation
of
A,
and
if
a
contradiction
can
be
inferred,
A
is
in
fact
proved.
For
Horn
clause
logic,
and
assuming
a
formula
A1,...,Am
Reasoning
in
TagLog
59
possibly
containing
some
existentially
quantified
variables
to
be
proven,
the
resolution
principle
can
be
given
as
follows:
:-
A1,...,Ak,...,Am
B
:-
C1,...,Cn
:-
A1,...,Ak-1,C1,...,Cn,Ak
1,...,Am
mgu
Ak,B
The
formula
to
the
left
above
the
line
is
the
negated
form
of
A1,...,Am
expressed
as
a
conditional
with
an
empty
consequent.
It
is
called
the
input
clause.
To
perform
a
resolution
proof
we
apply
the
resolution
rule
repeatedly
to
the
input
clause
and
a
clause
from
the
current
theory,
the
consequent
of
which
unifies
with
a
conjunct
in
the
input
clause,
resulting
in
a
certain
mgu.
The
result
is
a
new
input
clause
where
the
application
of
the
mgu
may
have
replaced
some
of
the
variables
by
other
terms.
This
is
repeated
until
either
the
empty
clause
i.e.
:-
with
nothing
in
the
antecedent
and
nothing
in
the
consequent,
meaning
contradiction
is
derived,
and
in
which
case
the
proof
is
completed,
or
it
is
clear
that
no
such
derivation
is
possible,
and
thus
the
proof
has
failed.
For
example,
suppose
we
want
to
prove
X
human
X
,
i.e.
that
there
exists
an
X
such
that
X
is
human.
To
perform
a
proof
by
contradiction
we
first
negate
this
formula
and
get
X
human
X
.
This
is
equivalent
to
X
human
X
,
which
in
turn
can
be
rewritten
as
a
conditional
with
an
empty
consequent
X
:-
human
X
.
Finally,
hiding
the
quantifier
we
arrive
at
:-
human
X
,
which
is
what
the
resolution
principle
wants.
To
derive
our
theorem
from
the
theory,
we
start
from
this
clause
and
we
need
to
apply
the
resolution
principle
three
times
before
arriving
at
a
contradiction.
The
proof
can
be
displayed
as
in
Figure
3.
:-
human
X
human
Y
:-
featherless
Y
,
biped
Y
featherless
socrates
biped
socrates
:-
X
Y
Y
socrates
:-
featherless
Y
,
biped
Y
:-
biped
socrates
FIGURE
3.
SLD
resolution
proof
tree
The
above
principle
is
in
fact
a
particular
refinement
of
the
resolution
principle,
called
SLD
resolution.
It
is
demonstrably
sound
and
complete,
and
it
can
be
implemented
very
efficiently.
60
The
Logic
of
TagLog
It
is
characteristic
of
SLD
resolution
proofs
that
they
are
constructive.
The
procedure
returns
the
bindings
of
the
variables
the
substitutions
and
hence
not
only
do
we
learn
that
there
exists
a
human,
we
also
get
to
know
an
example.
As
we
will
see,
this
property
is
quite
important
for
the
particular
use
of
logic
that
we
will
make
in
this
thesis.
Horn
clause
logic
has
proved
to
be
a
stable
basis
for
computing
with
logic.
A
lot
of
research
has
addressed
the
problem
of
extending
to
full
first
order
logic,
while
still
keeping
the
good
computational
properties,
but
with
limited
success.
SLD
resolution
in
Horn
clause
logic
is
probably
still
the
most
efficient
way
of
computing
in
logic.
3.6.3
Proof
trees
No
matter
how
we
think
of
these
proofs,
it
is
convenient
to
have
a
way
of
displaying
a
proof
that
makes
it
easy
to
follow.
Proof
trees
are
often
displayed
as
in
Figure
4,
with
the
conclusion
at
the
top,
and
the
formulas
on
which
they
depend
thereunder.
This
captures
most
of
what
we
will
need
to
know
about
a
particular
proof.
human
socrates
featherless
socrates
biped
socrates
FIGURE
4.
Proof
tree
3.7
Summary
and
Conclusion
In
this
chapter,
a
notion
of
formal
theory
was
developed,
that
will
to
some
extent
allow
us
to
capture
the
informal
view
of
theories
explained
in
Chapter
2.
Some
important
requirements
for
a
language
for
description
and
analysis
of
text
were
proposed.
Syntax
and
semantics
for
the
formalism
of
choice
Horn
clause
logic
was
given,
and
the
only
rule
of
derivation
that
is
really
needed
the
resolution
principle
was
explained.
Many
advantages
come
with
the
use
of
logic
as
underlying
knowledge
representation
formalism:
a
great
deal
of
expressive
power,
existing
Summary
and
Conclusion
61
results
concerning
soundness
and
completeness,
formal
semantics,
and
explicit
theories
of
inference.
As
will
be
seen,
the
availability
of
logical
variables
and
universal
quantifiers
guarantee
a
level
of
expressiveness
sufficient
for
a
natural
formulation
of
systems
of
definitions
and
descriptive
generalizations,
and
the
availability
of
logical
connectives
and
higher-order
predicates
involving
sets
and
their
properties,
allows
us
to
formulate
and
deduce
answers
to
fairly
complex
queries.
A
formalism
like
this,
with
an
explicit
declarative
semantics,
will
allow
us
to
enforce
the
view
of
corpus
theories
as
declarative
bodies
of
knowledge,
detached
from
the
tools
that
use
them.
Everything
is
explicit,
nothing
is
hidden.
In
the
course
of
analysis,
the
user
only
has
to
state
what
information
he
wants
to
get
as
the
result
of
a
computation,
not
how
he
thinks
the
system
should
compute
it.
The
user
does
not
have
to
be
a
programmer.
If
we
pack
the
criteria
concerning
readability,
writeability,
and
learnability
together,
and
label
them
ease
of
use
,
I
think
we
would
soon
find
that
ease
of
use
in
this
sense
is,
to
a
considerable
degree,
a
question
of
previous
acquaintance.
Today
a
substantial
number
of
people
linguists
included
are
familiar
with
the
Prolog
programming
language,
and
this,
I
think,
is
a
major
argument
for
the
choice
of
a
Prolog-like
formalism.
Yet,
TagLog
is
not
Prolog.
TagLog
is
not
a
programming
language.
The
notorious
cut
in
Prolog,
for
example,
is
not
part
of
the
TagLog
language.
Hence,
TagLog
is
easier
to
learn
than
Prolog.
But
with
no
background
in
logic
or
Prolog
programming,
TagLog
fluency
does
not
come
cheap.
Part
of
my
task
here
is
to
convince
people
without
background
knowledge
in
any
relevant
area
that
it
is
well
worth
the
effort.
Despite
these
advantages
it
may
not
be
entirely
clear
that
Horn
clause
logic
is
a
formalism
useful
for
our
purpose.
Corpus
research
often
involves
searching
and
counting,
but
can
we
search
and
count
with
logic?
Can
we
build
concordances?
Can
we
implement
good
methods
for
automatic
tagging?
Yes,
from
a
logical
point
of
view,
as
we
will
see
in
Chapter
7,
searching
is
not
different
from
other
kinds
of
deduction.
Furthermore,
as
we
shall
see
as
well,
the
expressive
power
of
the
TagLog
query
formalism
is
certainly
enough
to
allow
natural
and
declarative
specifications
of
concordances
see
Chapter
7
,
tables
of
collocations
see
Chapter
9
,
etc.
to
be
written.
In
fact,
the
major
part
of
the
rest
of
this
thesis
can
be
read
as
an
argument
for
this.
CHAPTER
4
TagLog
Theories
Theories
are
nets
cast
to
catch
what
we
call
the
world
:
to
rationalize,
to
explain,
and
to
master
it.
We
endeavour
to
make
the
mesh
ever
finer
and
finer.
Karl
Popper
4.1
Introduction
In
this
chapter,
we
explore
the
expressive
power
of
the
TagLog
formalism,
and
use
this
power
to
develop
various
theories
about
different
aspects
of
texts.
Two
kinds
of
theories
will
be
considered:
data
descriptions
and
universal
theories.
The
actual
theories
as
such
are
not
important.
What
is
important
is
to
gain
an
understanding
of
the
numerous
possibilities
inherent
in
the
logical
approach
to
corpus
linguistics,
an
understanding
of
how
lexical,
syntactic,
semantic,
and
pragmatic
aspects
of
corpus
text
can
be
captured
by
means
of
TagLog
universal
theories
and
descriptions.
It
is
impossible
for
us
to
explore
all
the
possibilities
here,
however.
When
the
expressiveness
of
TagLog
meets
the
creativeness
of
corpus
linguists,
anything
can
happen.
Text
Descriptions
63
By
placing
the
present
chapter
before
the
chapters
describing
various
procedures
and
tools
producing
and
making
use
of
descriptions
and
theories,
I
am
trying
to
make
the
following
point.
TagLog
theories
and
descriptions
can
be
understood
without
reference
to
how
they
came
about,
or
how
they
are
to
be
used.
All
one
needs
to
know
is
what
the
formulas
mean,
what
they
claim
about
their
subject
matter.
This,
again,
is
the
heart
of
declarativity.
4.2
Text
Descriptions
Again,
consider
the
following
text:
34
John
loves
Mary
The
particular
segment
2-3
i.e.
the
occurrence
of
loves
can
be
described
as
a
verb,
as
follows:1
35
syncat
2-3,v
.
Note
the
use
of
names
of
points
to
explicitly
refer
to
points
in
the
text,
and
thereby
to
refer
to
segments
delimited
by
these
points.
This
then,
should
be
compared
with
the
practice
of
inserting
labels
at
or
in
relation
to
these
points,
that
was
described
in
Chapter
1.
One
might
think
that
the
notion
of
tagging,
in
the
sense
of
labelling
,
is
rather
strange,
if
taken
literally.
One
would
certainly
never
dream
of
approaching
the
study
of
the
non-linguistic
physical
world
by
attaching
labels
to
physical
objects.
In
physics,
and
in
linguistics,
it
seems
to
be
much
more
natural
to
start
by
describing
the
object
under
study,
not
just
marking
parts
of
it
with
labels.
From
this
point
of
view,
the
TagLog
approach
is
a
radical
alternative
to
the
traditional
approach,
and
it
is
also
more
in
accordance
with
scientific
practice
in
other
fields.
Another
possible
way
of
looking
at
the
Tagging
in
Logic
approach,
and
a
way
to
make
more
sense
of
the
Tagging
by
Labelling
approach,
is
to
see
the
former
as
a
logical
reconstruction
of
the
latter.
The
traditional
corpus
linguists
have
always
treated
the
labels
as
a
way
to
express
statements
1.
To
say
of
the
string
loves
that
it
is
a
verb
is
different,
and
could
be
expressed
as
follows:
syncat
loves
,v
.
For
more
on
this,
see
Section
4.7.
64
TagLog
Theories
about
the
text;
they
just
haven
t
been
very
explicit
about
it,
one
might
argue.
This
is
how
I
choose
to
regard
the
Tagging
by
Labelling
approach
in
this
thesis.
Just
as
traditional
mark-up
descriptions
do,
TagLog
descriptions
group
entities
into
classes,
the
motivation
being
that
the
entities
share
common
characteristics.
For
example,
the
segments
denoted
by
1-2
and
3-4
are
grouped
into
a
class
denoted
by
pn
,
on
the
basis
of
the
fact
that
both
are
names.
Segments
may
also
be
classified
according
to
which
string
types
they
belong
to:
36
bstring
2-3,
loves
.
Note
that
it
is
clear
that
it
is
not
strings
that
are
classified;
rather
segments
are
classified
as
instances
of
certain
strings,
just
as
they
are
classified
as
instances
of
certain
word
classes,
such
as
noun,
verb,
etc.
It
would,
of
course,
be
possible
to
speak
of
string
tokens
instead
of
segments
,
but
that
would
obscure
the
fact
that
not
only
are
segments
instances
of
strings,
but
also
of
various
other
classes.
Although
it
might
be
easier
to
see
that
a
segment
is
an
instance
of
saw
than
to
see
that
it
is
an
instance
of
say
a
noun,
in
principle
these
are
descriptions
on
the
same
level.
So
why
speak
of
them
as
string
tokens
rather
than
as
noun
tokens
?
Referring
to
them
as
segments
avoids
the
problem.
A
classification
is
perhaps
the
simplest,
most
primitive
form
of
theory,
but
also
the
most
basic.
No
theoretical
work
can
be
carried
on
without
classification.
In
my
view,
not
even
the
most
radically
quantitative
empiricist
approaches
such
as
clustering
approaches
to
word
classification
see
e.g.
Brown
et
al.,
1990
can
do
without
classes
and
classifications.
These
approaches
deal
with
string
tokens
segments,
that
is
,
and
it
is
characteristic
of
such
tokens
that
the
string
class
they
belong
to
can
be
read
off
the
tokens
themselves
it
is
written
on
their
back,
so
to
speak.
So
the
systems
of
classes
that
are
generated
in
these
approaches
do
not
emerge
out
of
raw
text
soup;
they
too
are
built
on
the
basis
of
other
classes.
A
set
of
sentences
such
as
these,
classifying
segments
of
a
text,
forms
a
description
of
the
text,
a
kind
of
theory
of
the
text.
Text
Descriptions
65
bstring
1-2,
John
.
bstring
2-3,
loves
.
bstring
3-4,
Mary
.
syncat
1-2,
pn
.
syncat
2-3,
v
.
syncat
3-4,
pn
.
syncat
1-4,
s
.
FIGURE
5.
A
text
and
its
description
I
have
already
made
some
comparisons
with
the
mark-up
descriptions
discussed
in
Chapter
1.
Let
me
continue
to
point
out
some
similarities
and
some
differences.
The
point
expressions
making
up
a
segment
expression
do
not
have
to
name
adjacent
points,
and
a
name
of
a
point
may
be
used
more
than
once,
in
different
sentences.
Thus,
TagLog
logic
allows
descriptions
of
segments
longer
than
a
word
e.g.
the
sentence
syncat
1-4,s
,
it
allows
simultaneous
descriptions
at
several
levels
e.g.
syncat,
function,
string
.
It
allows
also
descriptions
of
relations
between
segments,
using
such
terms
as
adjacent,
embedded
and
overlapping.
Together,
the
sentences
in
37
do
actually
claim,
not
only
that
1-2
is
a
proper
noun
and
that
2-3
is
verb,
but
also,
since
the
second
point
of
the
first
segment
is
identical
to
the
first
point
of
the
second
segment,
that
they
are
adjacent
segments.
37
syncat
1-2,pn
.
syncat
2-3,v
.
We
know
that
we
have
succeeded
in
describing
them
as
adjacent,
since,
with
respect
to
the
sentences
in
37
,
the
following
theorem
can
be
proved,
with
S1
and
S2
bound
to
1-2
and
2-3,
respectively:
38
!-
syncat
S1,pn
,
syncat
S2,v
,
adjacent
S1,S2
.
In
TagLog,
there
is
no
doubt
as
to
what
we
mean
by
assigning
two
tags
to
one
segment.
It
is
absolutely
clear
that
we
must
not
think
about
that
as
forming
a
disjunction.
The
sentences
in
39
claim
that
the
segment
2-3
is
a
noun
and
a
verb.
66
TagLog
Theories
39
syncat
2-3,n
.
syncat
2-3,v
.
If
we
want
to
describe
that
segment
as
a
noun
or
a
verb,
then
we
cannot,
since
this
is
something
that
the
language
does
not
allow.2
We
will
strongly
resist
any
temptation
to
hack
disjunction
e.g.
by
keeping
in
mind
that
under
certain
circumstances
something
like
39
really
means
noun
or
verb
.
I
certainly
have
nothing
against
the
use
of
disjunction,
but
to
use
it,
we
will
have
to
change
the
language
first.
Without
disjunction,
what
we
can
try
to
show
is
that
there
is
really
no
need
for
it,
and
that
phenomena
such
as
uncertainty
and
ambiguity
can
somehow
be
dealt
with
anyway.
All
in
all,
it
seems
that
we
have
at
least
the
kind
of
expressive
power
that
we
expect
from
a
mark-up
language,
except
that
it
is
much
clearer
that
this
is
a
language,
and
that
it
is
a
language
with
a
well-defined
meaning
and
an
explicit
inference
method.
So
far,
we
have
only
used
this
language
to
describe
simple
one-sentence
texts.
In
this
thesis,
I
often
use
examples
that
are
deliberately
oversimplified,
so
that
major
ideas
can
be
presented
in
a
simple
way.
However,
just
to
show
that
we
are
not
limited
to
laboratory
sentences
,
made-up
texts,
and
toy
theories
,
let
me
give
a
preview
of
the
TagLog
system
in
action
with
one
of
the
texts
from
the
Brown
corpus
Francis
Kucera,
1982
loaded
in
one
window,
and
descriptions
of
this
text
lined
up
in
the
windows
beside
it.
2.
The
formal
specification
of
the
TagLog
formalism
on
page
48
may
have
given
the
impression
that
disjunction
is
allowed.
Indeed,
disjunction
is
allowed,
but
only
in
a
query
or
in
the
antecedent
of
an
implication.
Thus,
we
cannot
say
about
a
segment
that
it
is
a
noun
or
a
verb,
but
we
can
say
that
if
it
is
a
noun
or
a
verb,
then
it
is
something
else.
Text
Descriptions
67
FIGURE
6.
Text
and
theory
in
the
TagLog
system
As
can
be
seen
in
Figure
6,
TagLog
labels
may
be
general
terms,
and
thus
they
may
be
structured.
To
take
a
very
simple
example,
to
mark
the
proper
nouns
as
singular,
we
introduce
a
number
feature,
and
a
value
sg
for
singular
.
40
syncat
1-2,pn
sg
.
Moreover,
by
means
of
general
terms
it
is
possible
to
represent
syntactic
analysis
trees:
41
syncat
1-4,s
syncat
1-2,np
syncat
1-2,pn
true,
syncat
2-4,vp
syncat
2-3,v
true,
syncat
3-4,np
syncat
3-4,pn
true
In
Chapter
10,
I
will
introduce
a
couple
of
extensions
to
the
expressive
means
of
the
basic
TagLog
formalism.
Disjunction
will
not
be
among
them,
but
a
limited
form
of
classical
negation
will.
Thus
the
extended
TagLog
formalism
will
allow
us
to
express
negative
statements
about
seg-
68
TagLog
Theories
ments.
We
will
then
be
able
to
say,
for
example,
about
a
segment,
that
it
is
not
an
adverb.
42
syncat
1-2,adv
.
To
explicitly
state
that
a
segment
is
not
an
adverb
is
different
from
just
leaving
that
information
out.
Thus,
a
clear
distinction
between
negative
and
unknown
facts
is
drawn.
More
importantly,
the
possibility
of
contradiction,
i.e.
the
simultaneous
derivation
of
a
statement
and
its
negation,
is
recognized.
Another
extension
that
will
be
introduced
is
a
certain
capability
for
meta-description
and
reasoning,
and
a
check
for
consistency
will
be
introduced
as
part
of
this.
The
formulas
of
TagLog
seem
hard
to
read
and
write
in
comparison
with
mark-up
languages.
But
the
mistake
is
to
think
that
we
need
to
make
a
compromise
between
expressive
power
on
the
one
hand
and
readability
and
writeability
on
the
other.
A
set
of
TagLog
clauses
may
be
unsatisfactory
from
a
presentational
point
of
view
and
terrible
from
an
entering
point
of
view,
but
it
is
easy
to
see
that
there
are
ways
to
map
the
information
contained
in
a
set
of
clauses
to
an
adequate
presentation,
and
that
there
are
other
ways
to
enter
clauses
than
to
write
them
down,
character
by
character,
in
a
text
editor.
Later
chapters
will
describe,
in
more
detail,
the
TagLog
implementation
of
some
of
these
ideas.
4.3
Situation
Descriptions
It
is
illustrative
to
compare
descriptions
of
texts
in
the
sense
of
Section
4.2
with
descriptions
of
other
things,
other
parts
of
the
world.
Also,
it
can
be
regarded
as
a
useful
preparatory
exercise,
because
in
Section
4.8,
I
will
present
a
framework
in
which
a
description
of
a
text
and
descriptions
of
the
situation
s
that
the
text
is
relating
to
are
integrated.
I
will
call
such
a
theory,
describing
a
situation,
a
situation
theory.
When
the
theory
consists
of
singular
statements
only,
it
will
be
referred
to
as
a
situation
description.
Taxonomies
69
As
a
very
simple
and
concrete
example,
consider
how
the
clauses
on
the
left
in
Figure
7
describe
the
particular
situation
depicted
on
the
right.
cat
x1
.
on
e1,x1,x2
.
mat
x2
.
FIGURE
7.
A
situation
theory
and
a
situation
There
are
two
objects
,
a
cat
and
a
mat,
and
there
is
an
on-eventuality
or
state
,
or
situation
that
they
take
part
in.
It
is
not
hard
to
see
the
similarities
between
a
theory
about
a
text
and
a
theory
about
a
situation
in
the
world.
There
are
names
identifiers
referring
to
objects
segments,
there
are
singular
statements
describing
states
of
affairs,
and
so
on.
4.4
Taxonomies
Up
to
this
point,
we
have
been
concerned
only
with
sets
of
singular
statements,
i.e.
with
descriptions.
In
the
following,
we
will
concentrate
on
universal
theories,
i.e.
on
sets
of
universal
statements.
One
form
of
classification
that
we
often
meet
with
in
the
traditional
literature
of
grammar
is
the
one
between
open
and
closed
word
classes,
i.e
between
word
classes
that
can
be
added
to,
and
word
classes
which
never
or
very
seldom
get
any
new
members.
In
TagLog,
classification
of
a
segment
as
belonging
to
an
open
word
class
can
be
done
as
follows:
43
syncat
2-3,open
.
It
is
common
practice
to
arrange
classes
in
a
class
hierarchy,
or
a
taxonomy.
A
taxonomy
can
be
displayed
graphically,
as
a
tree
or,
more
generally,
a
graph
.
A
very
simple
taxonomy
of
word
classes
can
be
displayed
as
in
Figure
8.
70
TagLog
Theories
word
class
open
closed
n
v
a
adv
pn
prep
conj
aux
det
pron
FIGURE
8.
A
taxonomy
of
word
classes
It
is
well-known
that
a
link
in
such
a
graph
can
be
rendered
in
logical
terms
as
a
universally
quantified
implication.
In
Horn
clause
logic,
and
in
case
of
the
link
between
v
and
open
above,
it
may
look
as
follows:
44
syncat
S,open
:-
syncat
S,v
.
Now,
it
suffices
to
describe
a
segment
as
in
35
;
the
sentence
in
43
will
follow
by
deduction.
The
whole
taxonomy
of
word
classes
in
Figure
8
may
be
modelled
as
follows:3
45
syncat
S,open
:syncat
S,n
;
syncat
S,v
;
syncat
S,a
;
syncat
S,adv
;
syncat
S,pn
.
syncat
S,closed
:syncat
S,prep
;
syncat
S,conj
;
syncat
S,aux
;
syncat
S,det
;
syncat
S,pron
.
46
Note
that
the
distinction
between
closed
and
open
word
classes
is
only
one
possible
distinction
out
of
many
related
ones.4
Indeed,
the
user
of
the
TagLog
system
is
free
to
encode
in
a
TagLog
theory
any
distinction
and
any
hierarchy
of
classes
that
he
or
she
thinks
would
be
interesting
and
or
3.
The
reader
is
reminded
that
the
character
;
is
the
TagLog
disjunction
symbol,
so
that
the
first
of
these
rules
should
be
read:
for
every
segment
S
holds
that
if
S
is
an
instance
of
a
noun
or
verb
or
adjective
or
adverb
or
name,
then
S
is
an
instance
of
an
open
word
class
.
A
related
distinction
is
the
one
between
content
words
and
function
words,
or
between
categorematic
and
syncategorematic
words.
4.
Definitions
71
useful
to
have
as
working
parts
of
the
developing
theories
of
text.
No
such
distinctions
or
taxonomies
are
hard-wired
into
the
system.
4.5
Definitions
As
has
already
been
mentioned,
universal
statements
can
be
used
to
capture
conceptual
relationships
in
a
domain.
Since
there
are
important
conceptual
relations
holding
between
some
of
the
built-in
predicates
of
TagLog,
in
the
sense
that
some
of
them
can
be
defined
in
terms
of
others,
we
use
them
to
illustrate
this
use
of
universal
statements.
Also,
these
are
clear
cases
of
conceptual
relations.
Note
that
we
do
not
need
to
actually
submit
these
definitions
to
the
system,
since
they
are
already
part
of
what
the
system
knows.
In
fact,
the
system
would
not
even
allow
us
to,
since
they
are
built-in
predicates
and
therefore
cannot
be
redefined
by
the
user.
One
of
the
most
important
relations
holding
or
not
holding
between
three
segments
is
the
concatenation
relation.
The
TagLog
predicate
segment
concat
3
can
be
defined
simply
as:5
47
segment
concat
P0-Pi,Pi-Pn,P0-Pn
.
The
prefix
relation
and
the
suffix
relation
can
be
defined
in
terms
of
segment
concat
3:
48
49
segment
prefix
S3,S1
:-
segment
concat
S1,S2,S3
.
segment
suffix
S3,S2
:-
segment
concat
S1,S2,S3
.
A
subsegment
of
a
segment
can
be
defined
as
a
prefix
of
a
suffix
of
that
segment
or,
alternatively,
as
a
suffix
of
a
prefix
of
that
segment
:
50
subsegment
S1,S2
:segment
suffix
S1,S3
,
segment
prefix
S3,S2
.
5.
To
allow
different
instantiation
patterns,
the
actual
system
definition
of
segment
concat
3
is
actually
slightly
more
complicated.
72
TagLog
Theories
4.6
Grammars
One
particularly
important
kind
of
universal
theory
in
TagLog
is
a
grammar,
although
I
hasten
to
add
that
I
do
not
consider
the
distinction
between
grammars
and
non-grammar
theories
to
be
of
any
fundamental
importance.
As
a
matter
of
fact,
one
point
that
I
would
like
to
make
here
and
that
point
will
be
returned
to
in
the
chapters
to
come
is
that
theories
of
texts
of
which
a
grammar
is
an
example
and
theories
of
situations
conceptual
models,
world
models
etc.
are
very
similar.
Moreover,
we
can
do
very
similar
things
in
them:
description,
prediction,
explanation,
hypothesis
testing,
and
so
on.
4.6.1
Logic
grammars
TagLog
grammars
are
just
one
particular
form
of
TagLog
theories.
Grammar
rules
are
not
really
rules,
but
rather
universal
statements
expressed
in
logic.
The
term
logic
grammar
has
come
to
refer
to
the
use
of
logic
to
formalize
grammar
rules.
Pereira
and
Shieber
1987,
p.
3
explain
the
basic
idea
as
follows:
Phrase
structure
rules
have
a
very
simple
expression
in
firstorder
logic:
u,v,w
NP
u
VP
v
conc
u,v,w
S
w
where
NP
represents
the
class
of
noun
phrases,
VP
the
class
of
verb
phrases,
S
the
class
of
sentences,
and
conc
holds
of
any
string
u,
v
and
w
such
that
w
is
u
followed
by
v,
that
is,
the
concatenation
of
u
and
v.
This
expression
in
first-order
logic
thus
states
that
any
noun
phrase
u
and
any
verb
phrase
v
can
be
concatenated
to
form
a
declarative
sentence
w
uv.
In
TagLog,
then,
a
rule
like
S
--
NP
VP
can
be
given
the
formulation
51
syncat
S,s
:-
syncat
S1,np
,
syncat
S2,vp
,
segment
concat
S1,S2,S
.
which
is
equivalent
to
and
can
be
partially
evaluated
to
the
formula
52
syncat
P0-Pn,s
:-
syncat
P0-Pi,np
,
syncat
Pi-Pn,vp
.
Such
a
rule
translates
into
English
as
every
instance
of
the
syntactical
category
noun
phrase
followed
by
an
instance
of
the
category
verb
phrase
is
an
instance
of
a
sentence
.
The
reading
a
sentence
can
consist
of
a
noun
phrase
followed
by
a
verb
phrase
is
implied.
Grammars
73
As
mentioned
in
Chapter
3,
a
special
purpose
grammar
formalism
can
be
used
instead
of
native
TagLog
syntax
to
express
grammar
rules.
It
is
vital
to
realize
that
the
grammar
formalism
is
only
a
notational
variant
of
ordinary
TagLog.
The
mapping
is
simple:
the
universal
statement
in
52
can
be
written
as
in
53
.
53
s
--
np,
vp.
4.6.2
A
grammar
as
an
axiomatic
system
The
term
formal
grammar
usually
refers
to
the
uses
of
formal
techniques
of
logic
and
mathematics
in
the
analysis
of
language.
One
particularly
ambitious
version
of
formal
grammar
is
an
axiomatic
system
in
the
sense
of
Chapter
3.
An
axiomatic
grammar,
then,
consists
of
three
parts:
a
language,
a
finite
set
of
axioms
or
axiom
schemata
,
and
a
finite
set
of
inference
rules.
One
way
to
see
a
grammar
as
an
axiomatic
system
is
to
regard
grammar
rule
statements,
along
with
singular
statements
about
particular
segments
of
text,
as
axioms
expressed
in
the
language
of
Horn
clause
logic,
and
to
repeatedly
use
just
one
rule
of
inference
the
resolution
principle
to
derive
other
claims
about
the
belonging
of
particular
segments
of
text
to
particular
grammatical
categories,
in
the
form
of
theorems.
4.6.3
What
is
a
TagLog
grammar
about?
We
have
come
to
regard
a
grammar
rule
as
a
kind
of
statement,
and
a
grammar
as
a
theory
consisting
of
a
set
of
such
statements.
But
if
a
grammar
rule
is
a
statement,
what
is
it
a
statement
about?
Again,
to
a
great
extent,
this
is
a
matter
of
intended
interpretation.
That
is,
it
is
up
to
you
and
me
to
decide
what
grammar
statements
are
about.
Many
grammarians,
following
Chomsky,
have
chosen
to
see
grammars
as
descriptions
or
models
of
grammatical
competence,
or
as
descriptions
of
abstract,
decontextualized
entities
called
sentences.
Logic
grammar
proponents
have
been
no
exception
in
this
respect,
but
we
do
not
have
to
buy
that
part
of
the
story.
In
this
thesis
I
will
try
to
defend
the
view
of
grammars
as
universal
theories
of
texts;
that
is,
as
statements
about
segments
or
about
facts
involving
segments
and
grammatical
categories
rather
than
about
strings,
about
utterances
rather
than
about
sentences.
74
TagLog
Theories
I
believe
there
are
several
important
methodological
advantages
to
be
gained
by
regarding
logic
grammars
as
empirical
statements
about
the
facts
of
linguistic
reality.
I
also
believe
that
certain
difficulties
will
arise
due
to
this
view.
The
most
important
advantage
arising
as
a
consequence
of
this
view
is
that
a
theory
of
grammar
becomes
an
ordinary
kind
of
empirical
theory,
no
different
from
a
theory
of
other
linguistic
or
non-linguistic
phenomena.
First
of
all,
that
means
grammar
rules
and
thus
grammars
can
be
treated
as
true
or
false.
Thus
the
grammarian
s
quest
for
correct
grammars
becomes
a
special
case
of
the
scientist
s
quest
for
true
theories
about
the
world.
Also,
it
means
that
what
can
be
done
in
theories
in
general
can
be
done
in
a
grammatical
theory.
Standard
methods
and
assumptions
as
we
know
them
from
other
sciences
apply.
In
particular,
notions
such
as
prediction
,
explanation
and
hypothesis
testing
become
applicable
and
relevant.
A
difficulty
that
might
arise,
however,
is
that
as
a
consequence
of
the
view
of
segments
as
the
subject
matter
of
corpus
linguistics,
context
cannot
easily
be
disregarded.
Segments
qua
segments
are
immersed
in
context,
and
that
s
an
essential
fact
about
segments,
and
an
inescapable
fact
about
linguistic
reality.
Thus,
grammars
developed
without
regard
for
context,
that
is,
most
traditional
grammars,
might
turn
out
to
be
false,
if
they
are
interpreted
as
statements
about
segments
of
text.
But
then
again,
an
advantage
with
grammars
regarded
as
complex
statements
is
that
such
problems
may
turn
out
to
be
solvable.
Conditions
can
be
logically
strengthened,
or
weakened,
and
thus,
the
truth
values
of
grammar
rules
can
be
changed.
As
we
shall
see,
it
is
even
the
case
that
conditions
referring
to
segments
and
linguistic
categories
conditions
on
linguistic
facts
can
be
interleaved
with
conditions
referring
to
entities
from
other
domains
conditions
on
extra-linguistic
fact.
4.6.4
Grammars
and
grammar
theories
One
traditional
distinction
worth
looking
at
is
the
distinction
between
descriptive
grammar
and
theoretical
grammar.
Whereas
a
descriptive
grammar
tries
to
account
for
the
facts
of
linguistic
usage
as
they
exist
in
particular
languages,
theoretical
grammar
goes
beyond
analysis
of
indi-
Grammars
75
vidual
languages
to
the
theoretical
foundations
of
such
language,
e.g.
to
explications
of
the
notions
of
category
and
rule,
the
connection
between
lexical
items
and
phrases,
the
relations
between
syntax
and
semantics,
etc.
In
my
view,
it
is
better
to
conceive
of
this
distinction
as
a
distinction
between
grammar
and
grammar
theory.
Of
course,
a
grammar
is
also
a
theory,
which
makes
grammar
theories
a
kind
of
meta-theories
theories
about
among
other
things
theories.
TagLog
is
geared
towards
descriptive
practice.
Foundational
claims
made
by
grammar
theory
are
assumed
but
not
explicitly
asserted
in
particular
TagLog
grammars,
and
cannot
be
tested
in
the
usual
TagLog
way.
A
false
instance
of
a
grammar
rule
does
not
necessarily
prompt
us
to
change
our
view
of
the
structure
of
a
particular
category,
for
example.
Also,
needless
to
say,
TagLog
as
such
embodies
certain
claims
of
that
sort,
and
thus
the
extent
to
which
TagLog
can
function
as
a
testbed
for
other
such
claims
is
clearly
limited.
However,
one
nice
thing
about
TagLog,
and
something
that
contrasts
it
with
most
other
tools,
is
that
many
of
the
structures
and
relations
assumed
by
current
schools
of
theoretical
linguistics
can
be
represented.
In
this
way,
TagLog
can
be
seen
as
an
attempt
to
strengthen
the
communication
between
the
theoretical
grammarians
and
descriptive
grammarians.
In
Section
4.6.5
4.6.7,
I
will
give
a
brief
overview
of
some
of
the
kinds
of
grammars
that
can
be
written
in
the
TagLog
framework.
4.6.5
Phrase
structure
grammar
Basic
in
some
respects
to
at
least
generative
grammar
is
the
pure
context
free
phrase
structure
grammars:
being
fully
recursive,
they
are
perhaps
the
simplest
devices
that
are
still
general
enough
to
be
interesting
and
relevant
from
a
grammarian
s
point
of
view.
In
54
,
a
simple
example
is
given.
54
syncat
P0-Pn,s
:-
syncat
P0-Pi,np
,
syncat
Pi-Pn,vp
.
syncat
P0-P1,np
:-
syncat
P0-P1,pn
.
syncat
P0-Pn,vp
:-
syncat
P0-Pi,v
,
syncat
Pi-Pn,np
.
In
the
grammar
rule
notation
provided
in
TagLog,
54
can
be
expressed
as
55
:
55
s
--
np,
vp.
np
--
pn.
vp
--
v,
np.
76
TagLog
Theories
For
example,
given
the
text
in
56
,
56
John
loves
Mary
and
given
that
1-2,
2-3,
and
3-4
denote
the
first,
second
and
third
word
token,
respectively,
these
universal
statements,
together
with
the
statements
57
syncat
1-2,pn
.
syncat
2-3,v
.
syncat
3-4,pn
.
form
a
small
but
still
useful
theory
about
syntax
in
which,
for
example
58
is
a
theorem.
58
syncat
1-4,s
4.6.6
Categorial
grammar
An
interesting
alternative
to
phrase
structure
grammar
is
categorial
grammar
Lambek,
1958;
Woods,
1993
.
In
contrast
to
phrase
structure
grammars,
categorial
grammars
are
radically
lexicalistic:
nearly
all
grammatical
information
is
encoded
in
lexical
categories,
and
only
a
very
small
number
of
rules
or
cancellation
schemata
are
needed.
The
two
categorial
grammar
rules
in
59
,
and
the
utterance
in
56
tagged
with
lexical
categories
as
in
60
,
allow
the
derivation
in
61
.
59
60
X
--
X
Y,
Y.
X
--
Y,
X
Y.
syncat
1-2,np
.
syncat
2-3,
s
np
np
.
syncat
3-4,np
.
!-
syncat
1-4,Sign
.
Sign
s
61
4.6.7
Unification-based
grammar
A
trend
arose
a
decade
ago
in
theoretical
and
computational
linguistics
towards
unification-based
grammar
formalisms
FUG,
GPSG,
LFG,
DCG,
UCG,
PATR,
HPSG
and
is
still
with
us
today.6
The
TagLog
sys6.
Two
good
introductions
to
unification-based
grammars
are
Kay,
1992
and
Shieber,
1986
.
Grammars
77
tem
contains
a
unification-based
grammar
system,
similar
in
most
respects
to
DCG.
Although
some
of
the
other
formalisms
are
based
on
feature-structures
rather
than
terms,
the
first-order
terms
of
Prolog
and
TagLog
provide
expressive
power
equivalent
to
feature
structures,
so
as
to
ensure
that
most
theoretical
grammar
models
can
be
implemented
or
at
least
partially
emulated
in
DCG,
and
therefore
in
TagLog.
In
this
section
we
look
at
some
examples
of
how
different
solutions
to
typical
problems
of
grammar
can
be
implemented
in
TagLog.
First,
consider
the
grammar
in
62
where
the
use
of
variables
and
unification
guarantees
number
agreement
between
the
subject
and
the
verb.
A
description
of
the
utterance
in
56
as
in
63
makes
derivations
like
the
one
in
64
possible.
62
s
--
np
Num
,
vp
Num
.
np
Num
--
pn
Num
.
vp
Num
--
v
Num
,
np
.
syncat
1-2,pn
sing
.
syncat
2-3,v
sing
.
syncat
3-4,pn
sing
.
!-
syncat
1-4,Syn
.
Syn
s
63
64
As
a
second
example,
the
grammar
in
65
implements
a
simple
scheme
for
verb
subcategorization
only
non-subject
complements
are
subcategorized
for
.
Given
56
tagged
as
in
66
,
the
derivation
in
67
can
be
performed.
65
s
--
np,
vp.
np
--
pn.
vp
--
v
SubCat
,
vcomp
SubCat
.
vcomp
--
.
vcomp
X
Xs
--
X,
vcomp
Xs
.
66
syncat
1-2,pn
.
syncat
2-3,v
np
.
syncat
3-4,pn
.
!-
syncat
1-4,Syn
.
Syn
s
67
In
our
third
example,
we
use
a
gap
threading
technique
to
handle
longdistance
dependencies,
for
instance,
as
it
is
introduced
by
the
utterance
of
a
WH-question
in
68
.
68
Who
does
John
love?
78
TagLog
Theories
69
s
GapIn,GapOut
--
wh,
s
np
GapIn
,GapOut
.
s
GapIn,GapOut
--
aux,
np
GapIn,Gap
,
vp
Gap,GapOut
.
np
np
Gap
,Gap
--
.
np
Gap,Gap
--
pn.
vp
GapIn,GapOut
--
v,
np
GapIn,GapOut
.
syncat
1-2,wh
.
syncat
2-3,aux
.
syncat
3-4,pn
.
syncat
4-5,v
.
!-
syncat
1-5,C
.
C
s
X,X
.
70
71
Next,
the
grammar
in
72
implements
a
simple
unification-based
compositional
semantics.
The
colon
is
used
to
separate
the
syntax
from
the
semantics.
The
utterance
in
56
described
as
in
73
allows
the
derivation
in
74
.
72
s:Sem
--
np:X,
vp:X
Sem.
np:Sem
--
pn:Sem.
vp:Sem
--
v:X
Sem,
np:X.
syncat
1-2,pn:
j
.
syncat
2-3,v:
Y
X
loves
X,Y
.
syncat
3-4,pn:
m
!-
syncat
1-4,SynSem
.
SynSem
s:
loves
j,m
73
74
Finally,
75
gives
the
forward
and
backward
application
rules
in
a
simple
unification-based
categorial
grammar
with
compositional
semantics.
Here
also,
the
colon
is
used
to
separate
the
syntax
from
the
semantics.
The
description
of
56
in
76
allows
the
derivation
in
77
.
75
76
C:S
--
C
C1:S1
:S,
C1:S1.
C:S
--
C1:S1,
C
C1:S1
:S.
syncat
1-2,np:j
.
syncat
2-3,
s
np:X
np:Y
:loves
X,Y
.
syncat
3-4,np:m
.
!-
syncat
1-4,Sign
.
Sign
s:loves
j,m
77
We
see
that
the
TagLog
grammar
formalism
is
quite
expressive.
More
suggestions
on
how
to
solve
particular
problems
in
the
area
of
syntax
and
in
the
syntax-semantics
interface
,
as
well
as
information
on
how
to
straightforwardly
combine
the
techniques
for
handling
agreement,
subcategorization,
semantics
and
long-distance
dependencies
in
one
single
Lexica
79
grammar,
can
be
readily
found
in
the
literature
on
unification-based
grammar
see
for
example
Shieber,
1986;
Pollard
Sag,
1995
.
4.7
Lexica
Another
common
form
of
body
of
linguistic
knowledge
is
the
lexicon.
In
standard
phrase
structure
grammar,
for
example,
a
lexicon
might
be
written
as
follows:
78
John:
pn
loves:
v
Mary:
pn
On
the
face
of
it,
this
looks
very
similar
to
a
segment
of
text
that
has
been
tagged
with
lexical
categories.
But
the
semantic
difference
is
great,
given
the
assumptions
made
so
far.
As
we
saw
in
Section
4.2,
a
tagged
segment
of
text
is
logically
reconstructed
as
follows.
79
syncat
1-2,pn
.
syncat
2-3,v
.
syncat
3-4,pn
.
That
is,
79
says
of
particular
segments
referred
to
by
the
segment
expressions
that
they
are
proper
nouns,
verbs,
etc.
That
is,
it
is
important
to
realize
that
by
the
introduction
of
the
sentence
syncat
2-3,v
in
the
description
we
have
said
something
about
that
particular
instance
of
the
word
loves
.
We
have
said
nothing
at
all
about
the
string
loves
as
such
except,
implicitly,
that
at
least
one
of
its
instances
is
a
verb
.
A
lexicon,
on
the
other
hand,
makes
much
more
general
claims.
For
example,
a
lexical
entry
may
say
of
a
word
that
all
of
its
instances
are
of
a
certain
category.
Thus,
a
logical
reconstruction
of
a
lexicon,
if
it
is
going
to
be
able
to
account
for
the
relation
to
tagged
corpora,
must
involve
universal
statements.
A
lexicon
is
really
a
universal
theory
of
certain
aspects
of
text,
with
sentences
quantifying
over
word
occurrences.
Indeed,
in
the
logic
grammar
tradition,
lexical
entries
are
usually
encoded
as
universally
quantified
conditionals,
roughly
as
in
80
.
80
syncat
S,pn
:-
bstring
S,
John
.
syncat
S,v
:-
bstring
S,
loves
.
syncat
S,pn
:-
bstring
S,
Mary
.
80
TagLog
Theories
Lexical
lookup
is
performed
by
deduction.
Given
an
encoding
of
a
string
as
follows:
81
bstring
2-3,
loves
.
the
following
sentence
is
derived
as
a
theorem,
and
thus,
the
lexical
category
is
tied
to
the
particular
segment
in
question.
82
syncat
2-3,v
.
It
is
tempting,
and
very
much
in
accordance
with
current
terminological
practice,
to
say
about
a
word,
all
the
instances
of
which
are
of
a
certain
category,
that
the
word
in
itself
is
of
that
category.
Thus,
in
the
case
of
loves
,
if
it
is
true
that
all
of
its
instances
are
verbs
and
for
the
sake
of
the
example,
let
us
assume
that
it
is
,
we
say
that
loves
is
a
verb
and
that
John
and
Mary
are
proper
nouns
:
83
lexicon
John
,pn
.
lexicon
loves
,v
.
lexicon
Mary
,pn
.
Then
again,
we
must
not
let
this
kind
of
representation
obscure
the
fact
that
a
lexicon
is
a
universal
theory;
and
it
is
indeed
the
fact
that
for
a
lexicon
of
this
kind
to
have
any
bearing
on
segments
of
text,
and
to
capture
the
relation
between
81
and
82
,
we
need
an
axiom
along
the
lines
of
84
,
that
is,
a
universally
quantified
sentence.
84
syncat
S,C
:-
bstring
S,W
,
lexicon
W,C
.
So
we
see
that
with
the
ambition
of
attempting
to
account
for
the
relation
between
part-of-speech
tagged
text
and
a
lexicon,
the
logical
structure
of
a
lexicon
is
slightly
more
complicated
than
the
list
of
pairs
of
words
and
lexical
categories
might
suggest.
As
soon
as
we
realize
that
the
logical
structure
of
the
lexicon
really
is
universal,
we
might
begin
to
see
some
further
possibilities
for
experimentation:
we
might
want
to
introduce
other
kinds
of
conditions,
we
might
want
to
reverse
the
directions
of
the
implication
arrows,
we
might
want
to
play
around
with
negative
information,
with
default
versions
of
the
logic,
and
so
on.
For
example,
instead
of
formulating
conditions
on
the
belonging
of
segments
to
a
certain
lexical
category
in
terms
of
the
string
classes
to
which
the
segments
belong,
we
might
want
to
introduce
lemmas
into
our
ontol-
Syntax,
Semantics,
and
the
Modelling
of
Context
81
ogy,7
and
condition
the
belonging
of
segments
to
a
certain
lexical
category
on
lemmas
rather
than
on
strings.
85
bstring
S,
saw
:-
lemma
S,see1
,
tense
S,past
.
syncat
S,v
:-
lemma
S,see1
.
This
gives
us
a
kind
of
lexicon
which
may
be
used
to
deduce
string
and
syncat
given
knowledge
of
lemma,
but
also,
as
we
will
see
in
Chapter
13,
may
be
used
to
abductively
infer
abduce
the
lemma
of
a
certain
segment,
given
information
on
string
and
or
syncat.
Or
by
introducing
meta-logical
conditions
on
the
consistency
of
the
current
theory,
and
by
defining
negative
facts,
a
lexicon
sensitive
to
the
local
co-text
can
be
implemented.
86
syncat
S,C
:-
bstring
S,W
,
lexicon
W,C
,
consistent
syncat
S,C
.
syncat
P1-P2,
v
:-
syncat
P0-P1,det
.
This
could
perhaps
be
conceived
of
as
a
move
towards
a
kind
of
lexicon
the
purpose
of
which
is
not
only
to
passively
serve
the
needs
of
the
grammar,
a
smart
lexicon,
that
could
reason
about
context.
And
or,
it
could
be
seen
as
a
move
towards
a
word
grammar
,
making
no
reference
to
any
segment
longer
than
a
word.
Both
of
these
approaches
could
conceivably
be
extended
to
include
the
kind
of
objects
that
typically
appear
within
a
lexicon,
such
as
senses,
definitions,
antonyms,
and
so
on.
Regardless
of
which
approach
we
eventually
choose
to
follow,
there
is
nothing
very
special
about
lexica;
they
too
are
just
theories.
4.8
Syntax,
Semantics,
and
the
Modelling
of
Context
Semantics
studies
the
relation
between
language
and
the
world,
between
linguistic
reality
and
extra-linguistic
reality.
For
a
grammar
to
handle
semantics,
it
must
be
able
to
relate
a
text
to
its
semantic
content,
in
virtue
of
the
meanings
of
the
expressions
used
and
other
facts
of
the
text.
7.
Or
perhaps
lexemes,
or
something
else
along
that
line.
82
TagLog
Theories
4.8.1
A
situation
theory
as
a
content
context
model
At
least
for
a
narrative
text,
it
makes
sense
to
think
of
its
content,
and
its
situational
context,
in
terms
of
situations
described
and
presupposed.
Thus
for
a
narrative
text,
a
suitable
situation
theory,
in
the
sense
explained
in
Section
4.3,
may
serve
as
a
model
of
the
content
and
context
for
the
text.
As
a
very
simple
and
concrete
example,
consider
how
the
situation
theory
from
Section
4.3
models
the
content
of
an
utterance
of
The
cat
is
on
the
mat
.
The
cat
is
on
the
mat.
cat
x1
.
mat
x2
.
on
e1,x1,x2
.
FIGURE
9.
A
text,
a
situation
theory,
and
a
situation
As
can
be
seen
in
the
diagram,
the
situation
theory
can
be
characterized
as
a
translation
of
the
natural
language
text,
expressed
by
a
logical
language.8
4.8.2
Integrated
grammar
A
perhaps
very
desirable
form
of
grammar
would
generalize
over
both
form
and
content
and
context,
and
would
thus
integrate
syntax,
semantics,
and
pragmatics.
What
would
such
a
grammar
look
like?
Hobbs
et
al.
1993
points
to
a
way
of
performing
a
thorough
integration
of
syntax,
semantics,
and
pragmatics,
by
combining
the
idea
of
interpre8.
Note
that
I
make
no
distinction
between
content
and
situational
context
that
is,
I
do
not
put
them
in
different
boxes
.
This
is
natural,
since
we
are
not
concerned
here
with
the
dynamics
of
text
comprehension,
and
since
the
content
of
a
particular
utterance
will
typically
serve
as
the
context
for
the
utterances
that
follow.
Hans
Kamp
refers
to
this
as
the
principle
of
the
unity
of
content
and
context
Kamp,
1985
.
Syntax,
Semantics,
and
the
Modelling
of
Context
83
tation
as
abduction
with
the
idea
of
parsing
as
deduction
Kowalski,
1979;
Pereira
Warren,
1980
.9
The
axioms
of
a
logic
grammar
are
augmented
with
predications
referring
to
content
and
context
in
appropriate
places.
In
this
section,
we
will
concentrate
on
the
declarative
aspect
of
the
integrated
approach.
We
will
return
to
the
use
of
abduction,
and
to
interpretation
as
abduction,
in
Chapter
13.
Consider
the
following
grammar:
87
s:E
--
np:X,
vp:X
E
A,
provable
A
.
np:X
--
det,
n:X
A,
provable
A
.
vp:A
--
cop,
pp:A.
pp:A
--
p:X
A,
np:X.
Again
the
colon
is
used
to
separate
the
syntax
from
the
semantics.
The
meta-predicate
provable
1
is
true
if
whatever
sentence
A
gets
bound
to
is
provable
in
the
current
situation
theory.
Let
us
translate
some
of
the
rules
of
this
grammar
into
logic
and
see
what
they
tell
us.
The
statement
in
88
says
that
if
we
have
a
noun
phrase
from
P0
to
Pi
referring
to
X
and
a
verb
phrase
from
Pi
to
Pn
denoting
X
E
A,
if
there
is
an
eventuality
E
such
that
A
is
true
of
E
and
X,
then
there
is
an
interpretable
utterance
from
P0
to
Pn
describing
eventuality
E.
88
syncat
P0-Pn,s:E
:syncat
P0-Pi,np:X
,
syncat
Pi-Pn,vp:X
E
A
,
provable
A
.
The
statement
in
89
says
that
if
P0-P1
is
an
instance
of
a
determiner,10
and
P1-P2
is
a
noun
denoting
X
A,
and
A
is
true
of
X,
then
the
segment
P0-P2
is
an
instance
of
a
noun
phrase
referring
to
X.
89
syncat
P0-P2,np:X
:syncat
P0-P1,det
,
syncat
P1-P2,n:X
A
,
provable
A
.
9.
In
particular,
see
section
6
in
Hobbs
et
al.
In
fact,
they
attribute
this
idea
to
Stuart
Shieber.
fact
that
the
determiner
does
not
get
any
semantics
here
is
of
course
a
gross
oversimplification.
Decades
of
research
in
formal
semantics
has
shown
that
determiners
have
a
lot
of
very
important
semantics
relating
to
quantification.
10.
The
84
TagLog
Theories
Thus,
these
rules
categorize
segments
of
text
on
the
basis
of
how
their
constituent
subsegments
are
categorized,
how
the
semantics
of
the
categories
combine,
and
how
the
result
of
evaluating
the
result
of
semantic
processing
in
the
content
context
model
comes
out.
In
Figure
10,
we
have
put
the
pieces
together:
a
grammar,
a
text,
and
a
situation
theory.
s:E
--
np:X,
vp:E
X
A,
provable
A
.
np:X
--
det,
n:X
A,
provable
A
.
vp:A
--
is
,
pp:A.
pp:A
--
p:X
A,
np:X.
The
cat
is
on
the
mat.
cat
x1
.
mat
x2
.
on
e1,x1,x2
.
FIGURE
10.
Integrated
grammar
We
will
now
be
able
to
perform
the
proof
in
90
.
90
!-
prove
syncat
1-7,C
.
C
s:e1
Thus,
we
are
able
to
deduce
the
fact
that
the
text
segment
1-7
is
a
sentence
denoting
an
eventuality
e1,
of
which
we
know
that
it
is
an
on-eventuality
in
which
a
cat
and
a
mat
participate.
We
might
explain
the
conclusion
as
follows.
The
segment
corresponding
to
The
cat
is
on
the
mat
is
a
sentence
denoting
eventuality
e1,
since
the
segment
corresponding
to
The
cat
is
noun
phrase
denoting
x1
and
the
segment
corresponding
to
is
on
the
mat
is
a
verb
phrase
denoting
X
E
on
E,X,x2
,
and
on
e1,x1,x2
is
true.
The
instance
of
The
cat
is
a
noun
phrase
denoting
Syntax,
Semantics,
and
the
Modelling
of
Context
85
x1,
since
The
is
a
determiner,
and
cat
is
a
noun
denoting
X
cat
X
,
and
cat
x1
is
true.
And
so
on...
A
few
remarks
are
in
order
here.
First,
the
reader
should
be
warned
against
seeing
this
in
terms
of
interpretation
processes.
All
we
are
doing
here
is
trying
to
describe,
on
a
general
level,
a
relation
between
a
text
and
its
content
and
context,
when
all
these
factors
are
known
in
advance.
The
problem
of
determining
the
content,
when
the
content
is
not
known,
is
a
different
problem,
to
which
we
will
turn
in
due
time.
Still,
the
above
theory
seems
to
capture
the
following
facts
about
the
linguistic
reality:
The
correct
grammatical
classification
s
in
the
above
example
of
a
text
segment
may
depend
on
the
content
and
context
of
that
segment.
The
content
of
a
text
segment
e1
and
the
associated
information
in
the
above
example
may
depend
on
its
context.
This
context,
in
turn,
may
include
the
content
corresponding
to
surrounding
in
particular
preceding
segments.
So
far,
we
have
really
only
discussed
sentences,
not
texts.
A
discourse
grammar
is
in
many
respects
like
a
sentence
grammar,
but
it
crosses
the
sentence
boundaries,
and
connects
sentences
into
discourses.
Once
again,
let
me
borrow
a
small
example
from
Hobbs
et
al.:
91
discourse:E
--
s:E.
discourse:E
--
discourse:E1,
discourse:E2,
coherence
rel
E1,E2,E
.
The
rules
in
91
claim
that
a
sentence
with
the
interpretation
E
is
a
discourse
with
the
same
interpretation,
and
that
two
adjacent
discourses
form
a
larger
discourse
if
they
stand
in
some
kind
of
coherence
relation
such
as
explanation,
cause,
motive,
etc.
.
4.8.3
Now,
what
s
the
point
of
all
this?
There
are
at
least
three
reasons
why
one
would
like
to
use
integrated
grammar
in
the
manner
sketched
here.
First,
if
theories
about
text
are
extended
to
content
and
context,
then
the
scope
of
the
tools
that
employ
these
theories
are
extended
to
content
and
context
as
well.
As
will
be
seen,
TagLog
tools
can
indeed
be
used
to
investigate
the
relation
between
form,
content,
and
context,
of
text.
86
TagLog
Theories
Secondly,
it
may
well
possibly
be
that
including
descriptions
of
content
and
context
in
our
theories
is
the
only
way
or
at
least
the
most
correct
way
in
which
the
properties
of
text
segments
can
be
successfully
predicted,
and
the
facts
of
linguistic
reality
correctly
explained.
Some
observations
that
we
make
in
Chapter
14
tend
to
indicate
that
this
is
indeed
the
case.
Thirdly,
on
the
side
of
usefulness,
determining
the
content
of
text
is
also
the
key
to
the
success
of
many
potential
natural
language
processing
applications.
The
purpose
of
a
message
understanding
system,
for
example,
is
to
map
text
into
some
representation
of
its
content.
So
much
for
increase
of
coverage,
truthlikeness,
and
usefulness.
Are
there
any
reasons
why
we
should
not
use
integrated
grammar?
Yes,
for
the
average
corpus
linguist,
there
may
be
many
such
reasons.
Constructing
an
integrated
grammar
for
even
a
very
small
text
is
a
very
difficult
and
time
consuming
work.
Integrated
grammars
are
something
to
strive
for,
but
in
practice,
for
most
tasks,
we
will
probably
have
to
be
content
with
less.
A
simple
syntax
grammar
a
couple
of
context
free
rules,
say
can
also
count
as
a
corpus
grammar,
although
one
must
not
forget
that
it
describes
only
a
certain
aspect
of
linguistic
reality
a
text
cut
off
from
its
context
and
only
certain
properties
of
this
text
that
relate
to
its
form
and
to
the
ordering
of
its
elements.
In
other
words,
it
tries
to
describe
text-in-context,
without
really
relating
to
this
very
context.
As
we
will
see
in
Chapter
14,
this
might
lead
to
problems.
4.9
Dialogue
Grammars
Grammarians
working
with
sentences
are
certainly
not
the
only
linguists
that
have
been
attracted
by
context
free
grammars,
rewriting
systems
and
the
like.
Some
researchers
have
claimed
that
the
structure
of
dialogue
too
can
be
fruitfully
described
with
the
help
of
such
devices.
These
grammars
are
usually
called
dialogue
grammars.
Some
of
the
attempts
to
construct
such
grammars
are
based
upon
the
concept
of
adjacency
pair.
This
concept
is
one
of
the
corner
stones
of
the
conversation
analysts,
the
school
of
sociology
that
for
the
last
couple
of
decades
has
investigated
recurring
patterns
in
natural
conversation,
on
the
Dialogue
Grammars
87
basis
of
recordings
and
transcriptions
of
such
conversations
Levinson,
1983
.
Adjacency
pairs
are
sometimes
claimed
to
be
the
fundamental
units
of
conversational
organization
cf.
Coulthard,
1977,
p.
70
.
A
typical
adjacency
pair
consists
of
a
greeting
followed
by
another
greeting,
92
A:
Hi
there!
B:
Hello
a
question
followed
by
an
answer,
93
A:
What
s
your
name?
B:
My
name
is
Hillevi
or
an
offer
followed
by
an
acceptance,
a
summons
followed
by
a
response,
etc.
Using
the
TagLog
grammar
formalism,
such
interactions
can
be
described
as
follows,
with
just
a
few
simple
rules:
94
adj
pair
--
greeting,
greeting.
adj
pair
--
question,
answer.
However,
as
noted
by
for
example
Levinson
1983,
p.
304
,
the
requirement
of
strict
adjacency
of
the
pair
is
too
strong,
and
insertion
sequences
often
occur
where,
for
instance,
one
question-answer
pair
is
nested
within
another,
or
where
the
level
of
nesting
can
be
even
deeper,
as
in
the
following
example:
95
A:
Can
I
speak
to
your
daddy?
B:
Who
s
asking?
A:
Is
he
at
home?
B:
Yes
A:
Is
he
working?
B:
Yes,
as
usual
A:
This
is
his
boss
B:
Okay,
hold
on
a
second
For
capturing
these
kinds
of
dialogue
structure,
we
will
adapt
a
dialogue
grammar
from
Gilbert,
Wooffitt,
Fraser,
1990
:
96
adj
pair
--
greeting,
greeting.
adj
pair
--
question,
answer.
adj
pair
--
summons,
response.
adj
pair
--
offer,
acceptance.
88
TagLog
Theories
adj
pair
--
question,
insertions,
answer.
adj
pair
--
offer,
insertions,
acceptance.
insertions
--
insertion,
insertions.
insertions
--
insertion.
insertion
--
question,
insertions,
answer.
insertion
--
question,
answer.
As
will
be
demonstrated,
such
a
system
of
rules
makes
it
possible
to
search
for
instances
of
adjacency
pairs,
to
count
them,
to
make
concordances
of
them,
to
explain
and
to
predict.
4.10
Summary
and
Conclusion
In
this
chapter,
the
expressive
power
of
the
language
defined
in
Chapter
3
was
explored.
Data
description,
the
TagLog
alternative
to
mark-up,
was
exemplified,
and
some
comparisons
with
mark-up
were
made.
Also,
a
description
of
a
particular
text
was
compared
with
a
description
of
a
particular
non-text
situation.
Combining
the
notions
of
logic
grammar
,
descriptive
grammar
,
and
corpus
data
,
we
arrived
at
the
notion
of
logic
grammar
for
describing
corpus
data
.
It
was
shown
that
many
different
approaches
to
grammar
theory
can
in
principle
be
supported
by
TagLog.
We
also
noted
that
the
TagLog
notion
of
grammar
is
indeed
very
broad.
In
the
broadest
possible
case,
a
grammar
is
a
theory,
not
only
of
text,
but
also
of
the
relation
between
text,
its
content,
and
its
context.
Furthermore,
an
approach
to
the
logic
of
the
lexicon
was
suggested,
in
which
the
relation
of
the
lexicon
to
descriptions
of
particular
texts
is
seen
as
just
as
important
as
the
relation
to
the
rest
of
the
grammar.
It
is
extremely
important
to
emphasize
that
regardless
of
whether
they
are
grammars,
lexica,
situation
theories,
or
text
descriptions,
these
are
all
theories.
This
lends
a
good
deal
of
uniformity
to
the
approach.
Finally,
it
is
important
to
emphasize
again
that
these
theories
are
nothing
but
examples
of
what
may
happen
when
the
expressiveness
of
the
TagLog
formalism
meets
the
creativeness
of
the
corpus
linguist.
CHAPTER
5
TagLog
System
Overview
5.1
Introduction
In
this
chapter,
the
design
of
the
TagLog
system
a
computer
program
implementing
the
logical
approach
to
corpus
linguistics
and
supporting
the
activity
of
corpus
linguistics
will
be
discussed.
The
TagLog
system
is
a
scientific
instrument,
and
by
definition,
science
is
about
exploration
and
creativity.
Thus,
TagLog
can
be
classified
as
what
Shneiderman
1992
calls
an
exploratory,
creative
system
.
This
should
perhaps
not
be
taken
to
imply
that
the
system
in
itself
is
creative,
or
that
it
is
able
to
explore
anything
by
itself,
but
rather
that
it
is
able
to
take
care
of
some
of
the
non-creative
tasks
of
science
storing,
counting,
searching
in
order
to
create
an
environment
to
support
the
user
s
own
acts
of
exploration
and
creativity.
This
chapter
also
presents
the
Macintosh
implementation
of
the
TagLog
system
used
in
this
thesis
to
illustrate
the
logical
approach
to
corpus
linguistics.
This
is
just
an
overview,
however,
and
more
detailed
descriptions
of
the
TagLog
analysis
tools
are
given
throughout
the
thesis,
in
connec-
90
TagLog
System
Overview
tion
with
the
sections
that
describe
the
theory
behind
the
tools,
and
that
give
examples
of
their
use.1
TagLog
has
been
designed,
in
the
sense
that
due
consideration
has
been
given
to
the
principles
and
rules
of
thumb
that
always
apply
to
the
design
of
software,
independently
of
its
domain
of
application
cf.
Shneiderman
1992
.
We
will
touch
upon
this
slightly.
In
this
chapter,
we
will
also
spend
a
few
pages
looking
away
from
the
particular
implementation
of
TagLog
that
we
use
to
exemplify
the
approach.
In
particular,
we
will
be
looking
at
questions
of
architecture
and
efficiency
at
a
more
general
level.
5.2
TagLog
for
Interactive
Theory
Revision
The
TagLog
system
is
designed
as
a
system
for
interactive
theory
revision.
I
will
explain
in
this
section
what
this
means,
and
I
will
contrast
this
design
with
a
more
traditional
design
of
corpus
tools.
In
particular,
I
will
contrast
it
with
what
Sinclair
1992
has
to
say
about
his
strategy
for
planning
and
designing
the
next
generation
of
analytical
software
cf.
Section
1.2.2
.
I
have
chosen
Sinclair
here,
because
his
views
are
so
different
from
mine.
5.2.1
TagLog
is
a
theory
revision
system
I
have
devised
TagLog
as
a
theory
revision
system.
It
is
nearly
always
the
case
that
the
researcher
approaches
a
corpus
with
some
kind
of
theory
in
his
luggage,
obtained
from
a
textbook,
from
interviews
with
an
expert,
from
his
own
intuitions,
etc.
It
is
usually
the
case
that
this
theory
does
not
quite
fit
the
data.
It
may
be
false,
it
may
lack
coverage,
it
may
be
too
general,
or
too
specific.
The
problem
then
is
to
revise
this
imperfect
theory
to
make
it
fit
the
facts
under
study.
This,
indeed,
is
a
theory
revision
problem,
and
therefore
I
have
devised
TagLog
as
a
theory
revision
system.
1.
The
current
implementation
of
the
system
is
not
complete
with
respect
to
the
specification
put
forward
in
this
thesis.
The
functionalities
involving
abduction,
situation
theories,
and
automatic
tagging
have
been
implemented,
but
have
not
yet
been
fully
integrated
with
the
rest
of
the
system.
Consequently,
some
figures
showing
the
system
in
action
have
been
fabricated.
TagLog
for
Interactive
Theory
Revision
91
5.2.2
TagLog
is
an
interactive
system
I
have
devised
TagLog
as
an
interactive
system.
Humans
and
computers
are
good
at
different
things.
Corpus
linguistics
involves
searching,
observation,
discovery,
counting,
understanding,
storing,
and
many
other
things.
All
this
interacts
in
complicated
ways,
that
simply
do
not
allow
implementations
as
programs
to
be
run
in
batches.
Therefore,
TagLog
does
not
try
to
replace
human
intelligence
or
expertise.
Rather,
it
applies
computer
intelligence
in
ways
that
seek
to
bind
the
user
and
the
system
closer
together.
In
the
TagLog
approach,
the
computer
focuses
on
those
tasks
at
which
it
excels.
Observation,
understanding,
and
creative
imagination
is
left
to
the
human
expert.
Contrast
this
with
Sinclair
s
view:
analysis
should
be
restricted
to
what
the
machine
can
do
without
human
checking,
or
intervention
.
It
is
understandable
that
Sinclair
takes
this
position:
he
is
interested
mainly
in
statistical
methods
for
corpus
work;
statistical
methods
usually
demand
very
large
corpora
in
Sinclair
s
words:
operations
should
be
designed
to
cope
with
unlimited
quantities
of
text
material
;
we
don
t
want
to
do
manual
work
on
very
large
corpora,
it
is
far
too
slow
speed
of
processing
should
take
precedence
over
ultimate
precision
of
analysis
.2
This
idea
of
corpus
work
as
a
one-shot
process
where
the
user
prepares
a
corpus
and
the
required
knowledge
and
then
runs
an
algorithm
on
the
data
e.g.
an
algorithm
for
part-of-speech
tagging
is
very
different
from
the
view
of
corpus
work
that
I
am
arguing
for
here.
In
my
view,
interaction
with
the
human
expert
is
necessary,
and
should
be
supported.
This
does
not
mean
that
no
support
is
given
for
automatic
processing.
On
the
contrary,
as
will
be
seen,
TagLog
implements
several
automatic
tagging
tools,
and
machine
learning
tools,
that
perform
parts
of
the
work
automatically.
However,
I
am
aiming
at
a
design
in
which
these
automatic
processes
always
inform
the
user
what
they
are
doing,
and
report
the
results
of
what
they
are
doing
as
soon
as
such
results
become
available.
2.
Normally,
software
unguided
by
a
human
expert
will
make
lots
of
errors.
Tagging
errors
and
parsing
errors
abound.
Sinclair
s
solution:
Software
will
be
designed
to
operate
at
more
than
one
level
of
discrimination,
so
as
to
bypass
doubtful
decisions.
I
am
advocating
a
bolder
approach
in
this
thesis.
Let
errors
arise.
Let
us
observe
them.
Let
us
take
care
of
them
by
changing
the
theories.
After
all,
this
is
what
makes
theories
change
into
better
theories.
92
TagLog
System
Overview
This
is
feedback.
And
the
user
may
always
interrupt,
break
in,
and
add
or
delete
a
couple
of
clauses,
to
which
the
system
should
always
be
prepared
to
accommodate.
This
is
incrementality.
But
if
something
goes
wrong,
the
system
should
always
be
prepared
to
back
up
and
undo.
This
is
reversibility.
Feedback,
incrementality,
and
reversibility
that
s
the
essence
of
what
it
means
to
be
an
interactive
system.
5.2.3
TagLog
does
this,
user
does
that
Given
that
we
do
divide
the
work
between
the
user
and
the
system,
how
do
we
divide
it?
What
can
a
system
do
all
by
itself,
what
can
it
support,
and
what
must
be
left
entirely
to
the
user?
In
the
following,
in
order
to
say
something
about
what
is
supported
by
TagLog,
and
what
is
not,
I
will
make
use
of
the
division
between
the
three
kinds
of
activities
from
Section
2.5.1,
and
the
analysis
of
the
dynamics
of
these
activities,
that
were
provided
in
Section
2.5.2.
There
I
distinguished
development
of
descriptions
of
particular
texts
from
development
of
descriptive
universal
theories
of
sets
of
texts,
and
from
development
of
theoretical
principles.
Furthermore
I
distinguished,
with
the
help
of
Popper,
a
theory
development
cycle
consisting
of
problem
detection,
proposal
of
tentative
solution,
and
error
elimination.
TagLog
supports
balanced
cooperative
development
of
descriptions
of
particular
texts
a
classification
of
parts
of
text
data
according
to
a
classification
schema.
Often,
it
is
up
to
the
user
to
do
the
observation
and
the
classification,
and
it
is
the
responsibility
of
the
system
to
present
the
user
with
the
text
in
a
way
that
supports
observation.
Also,
it
is
up
to
the
system
to
check
the
user-supplied
descriptions
for
consistency,
and
to
remember
them.
But
sometimes,
as
we
have
already
indicated,
TagLog
is
able
to
perform
automatic
classification.
This
does
not
mean
that
the
system
takes
over
.
The
user
may
very
well
break
in
and
add
a
clause
or
two
by
hand,
and
this
new
information
is
to
be
taken
into
consideration
by
the
system.
Note
that
classification
of
particular
segments
of
text
presupposes
the
existence
of
a
system
of
classes.
The
development
of
appropriate
systems
of
classes
and
taxonomies
is
indeed
a
difficult
and
important
theoretical
meta-level
task.
As
the
system
now
stands,
this
task
has
been
left
entirely
TagLog
for
Interactive
Theory
Revision
93
to
the
user.
In
a
future
implementation,
the
system
might
be
able
to
help
the
user
with
this
too.
What
about
the
development
of
descriptive
universal
theories?
In
support
of
the
development
of
quantitative
descriptions,
based
on
the
classifications,
TagLog
implements
counting,
and
tabling,
and
various
sorts
of
statistical
calculations.
But
above
all,
the
TagLog
system
is
designed
to
support
development
of
explicit
representations
of
qualitative,
descriptive
theories.
First
of
all,
with
respect
to
the
development
cycle
as
given
above,
there
are
tools
in
TagLog
that
may
assist
the
researcher
in
finding
facts
that
are
not
covered
by
a
certain
theory.
That
is,
tools
to
identify
some
of
the
problems
that
may
be
there.
In
general,
the
initial
conjecture
concerning
a
particular
linguistic
phenomenon
as
it
occurs
in
a
text
is
a
truly
creative
part
of
scientific
work,
even
when
it
comes
to
descriptive
theories.
Nevertheless,
this
aspect
of
scientific
work
can
be
supported,
in
at
least
an
indirect
way,
by
a
system
providing
means
of
information
exploring.
Efficient
information
exploration
has
two
important
requirements:
quick
ways
of
navigating
through
data,
and
powerful
means
of
visualizing
data
and
analyses.
TagLog
supports
both,
for
both
texts
and
theories.
The
TagLog
Search
Tool
and
the
Concordance
Tool
provide
means
of
exploration
of
the
texts
under
study.
These
tools
are
even
able
to
use
the
theory
under
development
to
support
navigation.
Presentation
visualization
of
search
results
is
done
in
various
ways.
To
support
exploration
of
theories
under
development,
there
are
browsers
that
implement
various
aids
for
navigation
in
theories,
and
various
modes
of
presentation
of
theories.
The
generation
and
presentation
of
explanations
are
supported
as
well.
Some
aspects
of
the
theory
testing
activity
have
been
taken
over
by
the
TagLog
system.
The
system
handles
the
search
for
confirming
or
falsifying
instances
of
universal
statements,
or
the
generation
of
predictions
given
such
statements,
followed
by
the
checking
of
predictions
against
data,
in
order
to
confirm
or
disconfirm
the
predictions.
TagLog
is
very
open
to
the
incorporation
of
tools
that
implement
methods
for
learning
certain
aspects
of
descriptive
theories
automatically.
I
will
present
some
such
methods
further
on
in
this
thesis.
94
TagLog
System
Overview
On
the
other
hand,
the
task
of
discovering
and
formulating
theoretical
principles
in
contrast
to
shallow
and
low-level
descriptive
theories
of
data
is
a
truly
creative
part
of
scientific
work.
It
is
extremely
unlikely
that
we
will
ever
be
able
to
provide
a
system
with
tools
capable
of
doing
much
useful
work
here.
In
this,
we
should
let
ourselves
be
guided
by
a
general
quest
for
truth
rather
than
by
tools.
But
one
thing
seems
clear:
to
discover,
one
must
explore.
Hence,
tools
for
exploration
will
be
useful
in
this
context
too.
The
division
of
labour
between
the
user
and
the
system
is
summed
up
in
Table
1.
User
Navigating
linguistic
reality
Observing
facts
Describing
facts
Summarizing
data
Questioning
statements
Testing
statements
System
Providing
the
means
of
navigating
linguistic
reality
Presenting
facts
Remembering
descriptions
Providing
the
means
of
summarizing
data
Explaining
statements
Detecting
inconsistencies
and
other
errors
TABLE
1.
Division
of
labour
between
user
and
system
5.3
Against
Tools!
Now
let
us
turn
to
a
discussion
of
tools.
To
begin
with,
let
us
think
of
different
tools
as
corresponding
to
different
tasks
within
the
process
of
developing
and
revising
a
description
of
a
text.
In
Chapter
1,
we
saw
Sinclair
1992
give
an
impressive-looking
list
of
tools
believed
by
him
to
be
needed
by
the
corpus
researcher:
a
parser,
a
boundary
marker,
a
tagger,
a
collocator,
a
lemmatizer,
a
compounder,
a
disambiguator,
a
lexical
parser,
a
phrase
finder,
an
exemplifier,
a
setter,
a
classifier,
a
typologizer,
etc.
Many
of
the
tools
in
this
collection
as
well
as
many
of
the
other
tools
listed
in
Chapter
1
have
the
general
architecture
depicted
in
Figure
11.
Against
Tools!
95
Annotated
text
Tool
Annotated
text
Knowledge
FIGURE
11.
From
text
to
text
with
traditional
corpus
tools
That
is,
tools
map
possibly
annotated
text
into
annotated
text.
In
order
to
perform
this
mapping,
the
tool
needs
to
be
fed
some
kind
of
knowledge
.
A
parser
needs
to
be
fed
a
grammar.
A
tagger
needs
lexica,
transition
probabilities,
rules,
or
constraints.3
As
the
picture
indicates,
no
clear
separation
between
the
texts
and
the
descriptions
of
the
texts
i.e.
the
annotations
is
made
in
traditional
systems.
Tagging
or
parsing,
or
lemmatizing
means
changing
a
text,
by
annotating
it.
In
TagLog,
by
contrast,
tagging,
syntactic
analysis,
etc.
means
changing
what
follows
from
the
theory
describing
the
text.
The
text
is
not
changed
at
all.
As
a
matter
of
fact,
the
text
is
not
even
editable.4
In
contrast
to
traditional
systems,
TagLog
has
very
few
tools,
but
can
still
realize
very
many
functionalities.
The
most
important
tool
is
a
theorem
prover.
This
is
a
very
general
tool.
Throughout
this
thesis,
we
will
see
this
theorem
prover
work
as
a
parser,
as
a
tagger,
as
a
search
engine,
as
a
concordancer,
a
lemmatizer,
etc.
Apart
from
this,
there
are
very
few
tools.
3.
Figure
11
is
slightly
misleading
in
suggesting
that
the
knowledge
used
by
this
kind
of
tool
is
clearly
separated
from
the
tool
itself.
Granted,
the
knowledge
is
usually
stored
separately,
but
an
understanding
of
what
the
knowledge
means
can
usually
only
be
gained
in
relation
to
how
it
effects
the
behaviour
of
this
particular
tool.
As
will
be
seen,
it
is
different
with
TagLog:
the
semantics
of
the
TagLog
formalism
is
independent
from
any
processing
considerations.
Hence
TagLog,
but
not
the
traditional
approach,
guarantees
adherence
to
the
principle
advocated
by
Leech
1991
in
the
following:
A
n
annotated
corpus
should
never
totally
replace
the
corpus
as
it
existed
prior
to
annotation.
The
original
raw
corpus
...
should
always
be
available,
so
that
those
who
find
the
annotations
useless
or
worse
can
recover
the
text
in
its
virgin
purity.
4.
96
TagLog
System
Overview
TagLog
does,
however,
have
tools
in
a
different
sense,
on
a
different
level,
as
well.
TagLog
tools
are
more
on
the
level
of
update
tools,
browsers,
query
interfaces,
and
presentation
tools.
In
fact,
most
TagLog
tools
are
not
absolutely
essential;
they
are
there
for
the
sake
of
user
friendliness
only.
In
principle,
we
could
call
the
theorem
prover
directly,
from
a
command
line
interface,
in
order
to
parse,
search,
tag,
or
whatever.
In
the
TagLog
approach
we
prefer
to
think
in
terms
of
theories
rather
than
in
terms
of
tools.
TagLog
has
no
separate
parser,
no
phrase
finder,
no
collocator,
no
tagger,
no
lemmatizer.
Instead,
different
functionalities
are
realized
by
means
of
the
same
general
tool
a
theorem
prover
either
by
using
different
sub-theories
e.g.
a
grammar
for
parsing;
a
lexicon
for
part-of-speech
tagging
,
or
by
using
the
same
body
of
knowledge
in
different
directions
,
with
different
variables
uninstantiated
e.g.
a
grammar
can
be
used
for
searching
for
grammatical
constructions,
as
well
as
for
parsing
.
Presentation
tool
Update
tool
Theorem
prover
Theory
Text
FIGURE
12.
TagLog:
tool,
theory,
text
So
instead
of
invoking
a
parser,
we
add
grammar
rules
to
the
current
theory.
Then
sentences,
describing
the
syntactic
properties
of
text
segments,
will
follow
as
theorems
in
the
theory.
This
is
a
logical
relation,
so
it
happens
instantly,
as
it
were.
The
theorem
prover
is
there
so
that
we,
when
the
need
arises,
can
generate
explicit
representations
of
the
theorems.
Against
Tools!
97
5.3.1
Cooperation
between
tools
In
this
section,
we
focus
on
the
distribution
of
work
between
several
tools.
Can
one
tool
use
the
result
of
another
tool?
Can
several
tools
use
the
same
knowledge?
Can
one
tool
call
another
tool?
Sinclair:
I
am
advocating
a
modularised
kit
of
software
tools.
Each
will
specify
input
requirements
and
a
variety
of
output
conventions.
Complex
routines
can
be
built
up
for
particular
applications
by
organizing
several
tools
together
.
This
idea
of
piping
between
tools,
illustrated
in
Figure
13,
seems
to
be
the
standard
way
of
implementing
cooperation
between
corpus
tools.
Text
Tool
1
Ann.txt
Tool
2
Ann.txt
Tool
3
FIGURE
13.
Piping
between
tools
In
contrast,
TagLog
is
a
system
that
integrates
several
tools
in
the
sense
of
functionalities
,
using
a
uniform
representation.
Presentation
tool
Theorem
prover
Theory
1
Theory
2
Text
FIGURE
14.
Tools
making
use
of
uniform
representation
Thus,
the
need
for
interfaces
is
greatly
reduced.
Composing
functionalities
is
a
matter
of
logically
interfacing
different
sub-theories
.
Whenever
one
theory
e.g.
a
grammar
takes
as
primitive
what
another
theory
e.g.
a
lexicon
generates,
communication
between
the
theories
is
there.
Or
composing
functionalities
is
a
matter
of
composing
expressions
in
the
query
98
TagLog
System
Overview
language.
In
either
case,
this
is
as
it
should
be!
The
evolving
theory
is
placed
in
focus,
and
we,
as
designers
of
corpus
tools,
are
relieved
of
the
burden
of
designing
input
requirements
and
a
variety
of
output
conventions
of
different
tools.
Composing
functionalities
in
TagLog
is
sometimes
not
a
logical
matter
at
all,
in
case
it
is
a
matter
of
initial
update
to
a
theory,
or
final
output
to
a
presentation
tool.
Consider
a
situation
in
which
we
want
to
generate
a
concordance
of
say
all
the
nouns
in
a
particular
corpus
text.
In
TagLog
we
would
just
load
a
theory
that
purports
to
make
correct
predictions
about
the
lexical
categories
of
segments.
We
would
then
open
the
Concordance
Tool
in
order
to
monitor
the
theorems
of
a
certain
form,
in
the
form
of
a
concordance.
Now,
suppose
a
certain
tagging
error
is
detected
during
inspection
of
the
concordance,
an
error
that
we
would
prefer
to
correct
on
the
spot.
In
TagLog
we
just
select
the
offending
row
in
the
concordance.
The
corresponding
segment
in
the
text
is
then
automatically
selected,
and
the
Statement
Browser,
a
tool
to
be
introduced
in
more
detail
in
Chapter
8,
shows
the
formulas
describing
it.
These
formulas
may
then
be
selected
and
subsequently
deleted
or
modified.
The
Concordance
Tool
will
be
updated
accordingly.
The
behaviour
of
the
TagLog
system
is
consistent
with
a
certain
view
of
searching
and
concordancing
as
tools
for
navigation
in
a
corpus.
An
important
aspect
of
a
descriptive
theory
is
that
it
works
like
a
map.
It
greatly
extends
our
means
of
navigating
in
the
corpus.
By
so
navigating,
by
observing
and
exploring,
we
may
be
able
to
correct
the
errors,
and
extend
the
coverage,
of
the
theory
map
even
further.
We
will
then
be
able
to
explore
further,
and
so
on.
In
exactly
this
manner,
TagLog
allows
extensions
and
corrections
of
the
map
incrementally,
as
the
user
moves
along,
and
they
take
effect
immediately.
In
a
traditional
setting,
we
would
probably
have
to
feed
the
text
into
a
part-of-speech
tagger,
and
then
pipe
the
resulting
annotated
text
to
a
concordance
tool.
Macintosh
TagLog
User
Interface
99
Text
Tagger
Ann.txt
Conc.
tool
FIGURE
15.
From
text
to
annotated
text
to
concordance
Such
a
set-up
could
hardly
support
the
kind
of
connectivity,
incrementality,
and
interactiveness
that
I
am
arguing
for
here.
5.4
Macintosh
TagLog
User
Interface
The
kernel
TagLog
user
interface
consists
of
a
set
of
windows,
a
menu
bar
with
menus,
and
a
set
of
dialog
boxes.
These
are
of
course
highly
dependent
on
the
particular
TagLog
approach
to
description
and
analysis,
but,
except
for
two
of
the
menus,
they
are
meant
to
be
independent
of
the
user
s
choice
of
descriptive
levels,
his
way
of
organizing
theories,
his
choice
of
coding
categories,
and
so
on.
5.4.1
Menus
The
Macintosh
implementation
of
the
TagLog
system
provides
a
standard
menu
bar
and
familiar
pull-down
menus.
Apart
from
the
usual
File,
Edit,
and
Window
menus,
there
is
a
Browser
menu,
a
Tool
menu,
a
Tag
menu,
and
a
Value
menu.
FIGURE
16.
The
TagLog
system
s
menu
bar
and
some
of
its
menus
100
TagLog
System
Overview
The
browsers
provide
ways
to
administer
texts,
theories,
tags
and
tag
values,
and
ways
to
access
the
tools
in
an
object
oriented
way.
The
Tool
menu
contains
tools
for
searching,
counting,
concordance
building,
tabling,
explanation,
hypothesis
testing,
and
consistency
checking.
The
Tag
menu
contains
tags
and
the
Value
menu
contains
values
for
these
tags.
5.4.2
Windows
The
TagLog
system
consists
of
the
following
kind
of
windows:
the
Text
window,
the
Theory
windows,
and
the
Dialog
window.
All
windows
can
be
moved,
resized,
zoomed,
scrolled,
and
closed,
in
the
usual
Macintosh
way.
5.4.2.1
The
Text
window
The
Text
window
contains
the
current
text,
partially
described
by
the
current
theory.
This
text
is
not
editable,
but
portions
of
it
can
be
selected,
in
the
usual
Macintosh
manner.
By
selecting
a
segment
in
the
text,
selecting
a
tag
in
the
Tag
menu,
and
selecting
a
value
from
the
Value
menu,
a
statement
is
introduced
into
the
current
theory,
describing
this
segment.
Or,
by
selecting
a
segment
of
the
text
and
entering
a
query
of
the
right
form
in
an
appropriate
window,
the
properties
of
the
selected
segment
may
be
queried.
Also,
the
TagLog
system
is
able
to
select
a
segment
of
the
text
programmatically,
and
thus
point
to
the
segment,
in
response
to
a
request
posed
by
the
user,
for
example.
FIGURE
17.
The
TagLog
Text
window
Macintosh
TagLog
User
Interface
101
5.4.2.2
Theory
windows
The
idea
is
to
have
one
collection
of
statements
ordinary
TagLog
statement
or
grammar
rules
in
each
window,
and
to
give
the
window
a
symbolic
name,
under
which
the
content
of
it
can
be
referred
to.
These
collections
are
modified
manually,
by
editing
them.
The
content
of
a
Theory
window
may
be
saved
to
a
file.
FIGURE
18.
A
TagLog
Theory
window
A
particular
kind
of
Theory
window
is
the
Grammar
window.
This
is
where
grammars,
expressed
in
the
grammar
notation,
are
written.
An
example
is
given
in
Figure
19.
FIGURE
19.
A
Grammar
theory
window
The
name
of
a
grammar
window
plays
a
special
role,
in
that
rules
of
the
form
given
in
97
are
translated
by
the
system
into
statements
of
the
form
in
98
.
97
98
C
--
C1,...,Cn.
name
P0-Pn,C
:-
name
P0-P1,C1
,...,
name
Pn-1-Pn,Cn
.
The
grammar
in
Figure
19,
for
example,
is
translated
into
the
clauses
in
99
.
102
TagLog
System
Overview
99
syncat
P0-P2,s
:-
syncat
P0-P1,np
,
syncat
P1-P2,vp
.
syncat
P0-P1,np
:-
syncat
P0-P1,pn
.
syncat
P0-P2,vp
:-
syncat
P0-P1,v
,
syncat
P1-P2,np
.
5.4.2.3
Situation
Theory
windows
Typically,
a
Situation
Theory
window
contains
singular
statements
that
model
the
situation
s
to
which
a
text,
or
a
collection
of
texts,
relates.
These
too,
can
only
be
modified
by
hand.
FIGURE
20.
A
TagLog
Situation
Theory
window
The
TagLog
environment
supports
the
integration
of
syntax,
semantics
and
pragmatics
in
the
sense
given
in
Section
4.8.
Statements
describing
the
content
and
context
are
introduced
in
the
Situation
Theory
window
during
the
modelling
process.
The
symbols
that
are
parts
of
expressions
written
in
this
window
can
be
highlighted
the
usual
way,
for
example
in
order
to
point
out
a
relation
between
a
segment
of
text
and
some
entity
in
the
content
context
model.
As
will
be
shown,
many
TagLog
tools
can
be
used
to
analyse
the
relation,
in
both
directions,
between
a
text
and
a
content
context.
5.4.2.4
The
Dialog
window
The
TagLog
Dialog
window
is
a
so
called
command
line
interface
to
TagLog.
By
calling
the
built-in
predicates
from
this
interface,
all
the
functionality
of
TagLog
can
be
accessed;
nothing
is
buried
behind
the
buttons
and
menus
in
the
user
interface.
On
the
other
hand,
this
command
line
interface
is
not
particularly
user
friendly
.
The
only
thing
that
meets
the
eye
of
the
user
is
a
prompter
that
invites
him
to
formulate
his
request.
To
distinguish
the
TagLog
prompter
from
the
ordinary
Prolog
prompter
?-
,
the
TagLog
prompter
consists
of
an
exclamation
mark,
followed
by
a
dash
!-
.
Typically,
the
user
writes
his
request
after
the
prompter
and
presses
the
return
key,
and
it
is
normally
the
case
that
when
the
system
has
computed
The
TagLog
Schema
103
an
answer
to
the
query,
it
presents
the
result,
and
waits
for
input
from
the
user.
Referring
back
to
Figure
17,
the
loading
of
the
window,
and
the
highlighted
area
in
it,
is
an
effect
of
making
the
two
commands
in
Figure
21.
FIGURE
21.
The
TagLog
Dialog
window
To
save
space,
many
of
the
examples
in
the
present
thesis
will
be
formulated
in
this
simple
environment.
This
does
not
mean
that
they
cannot
be
formulated
and
executed
in
any
other
way;
on
the
contrary,
the
TagLog
tools
most
often
provide
a
much
more
accessible
interface.
5.5
The
TagLog
Schema
The
role
of
the
schema
is
to
unify
and
coordinate
the
elements
of
a
TagLog
corpus
project
.
Creating
a
schema
is
usually
the
first
step
in
the
process
of
developing
a
corpus
theory.
The
user
specifies
the
texts
that
are
to
be
included
and
how
they
are
to
be
tokenized,
i.e.
divided
up
into
basic
segments.
The
user
also
specifies
what
coding
categories
will
be
available,
what
universal
theories
will
be
drawn
upon,
etc.
However,
the
schema
will
usually
be
further
refined
in
the
course
of
theory
development.
A
schema,
like
many
other
things
in
TagLog,
is
a
kind
of
theory.5
One
group
of
schema
predicates
is
used
for
telling
the
system
about
the
texts
that
are
available
in
the
corpus
defined
by
a
particular
schema,
and
which
5.
Formally
speaking,
the
schema
is
a
meta-theory
with
respect
to
other
TagLog
theories.
As
a
meta-theory,
however,
it
is
very
limited.
It
determines
the
behaviour
of
certain
aspects
of
the
system,
and
it
influences
the
object
theories,
but
no
proper
meta-level
reasoning
is
performed
in
it.
104
TagLog
System
Overview
theories
are
associated
with
them.
Each
text
is
associated
with
an
abstract
identifier
ID
.
A
table
maps
IDs
to
file
names.
One
ID
may
correspond
to
many
different
files,
one
for
storing
the
text
itself,
others
for
storing
descriptions
of
the
texts,
as
well
as
various
indices
that
map
between
the
text
and
the
descriptions
e.g
that
map
file
positions
onto
point
expressions
.
Universal
theories
too,
are
associated
with
identifiers.
Another
group
of
schema
predicates
is
used
to
inform
the
system
about
the
current
tag
set,
by
declaring,
for
some
of
the
predicates
of
the
theories,
the
possible
values
of
their
arguments.
For
example,
if
the
clause
tag
syncat
and
the
clauses
tag
value
syncat,n
and
tag
value
syncat,v
are
added
to
the
current
schema,
then
the
menu
command
syncat
is
automatically
added
to
the
Tag
menu,
and
the
menu
commands
n
and
v
are
added
to
the
Value
menu.
An
overview
of
the
TagLog
schema
predicates
is
given
in
Appendix
A.
5.6
The
Choice
of
Interaction
Styles
An
important
step
in
the
process
of
designing
an
interactive
system
is
the
choice
of
interaction
style.
In
his
1992
textbook,
Shneiderman
distinguishes
the
following
interaction
styles:
menu
selection
form
fill-in
command
language
natural
language
direct
manipulation
I
will
argue
that
the
TagLog
user
interface
is
best
characterized
as
a
hybrid
system,
where
three
different
interaction
styles
direct
manipulation,
menu
selection
and
command
language
are
carefully
blended.
In
what
sense
is
TagLog
a
direct
manipulation
system?
If
it
is
true
that
presenting
a
visual
representation
of
task
objects
and
actions
is
the
heart
of
the
direct
manipulation
approach
to
design
Shneiderman,
ibid.
p.
65
then
surely
TagLog
is
a
direct
manipulation
system.
Texts,
theories,
statements,
properties,
etc.
abstract
task
objects
are
represented
visually
in
The
Choice
of
Interaction
Styles
105
the
system.
The
pointing
to
these
objects
by
means
of
the
mouse
is
a
kind
of
direct
manipulation
action.
5.6.1
The
pragmatics
of
human-computer
interaction
Since
I
realize
that
a
blend
of
different
interaction
styles
may
seem
to
run
the
risk
of
being
confusing,
I
will
spend
some
effort
on
the
defence
of
this
choice.
It
is
my
contention
that
the
branch
of
linguistics
called
pragmatics
is
able
to
throw
light
on
this
kind
of
mixed
system.
In
fact,
it
is
my
conviction
that
notions
from
pragmatics
such
as
communicative
act,
turn
taking,
feedback,
deixis,
presupposition,
cooperativeness,
and
relevance
ought
to
play
a
much
more
important
role
in
theories
of
computer
system
usability
than
they
currently
do.
For
example,
speech
act
theory
can
be
used
to
argue
that
the
term
command
language
is
misleading,
when
it
is
meant
to
denote
all
uses
of
artificial
as
opposed
to
natural
languages
in
user
interfaces.
TagLog,
for
instance,
uses
a
formal
language
which
we
do
not
want
to
describe
merely
as
a
command
language,
but
also
as
a
declarative
description
language
and
a
query
language
a
language
for
making
statements,
for
giving
commands,
and
for
asking
questions.
In
speech-act
theory,
statements,
questions
and
commands
are
regarded
as
different
kinds
of
communicative
acts.
A
natural
language
example
of
the
differences
would
be
the
following:
100
The
door
is
closed.
Is
the
door
closed?
Close
the
door!
Only
an
utterance
of
the
last
sentence
above
deserves
to
be
called
a
command.
I
think
that
this
distinction
is
a
very
important
one,
and
that
the
failure
to
make
it
is
a
sad
reflection
of
the
state
of
pragmatics
in
the
field
of
human-computer
interaction
research
today.
Another
notion
from
pragmatics
that
we
will
make
use
of
in
the
current
analysis
is
the
notion
of
deixis.
In
human
face-to-face
conversation
participants
often
specify
elements
of
their
common
visual
world
by
means
of
a
combination
of
deictic
expressions
e.g.
pronouns
such
as
this,
that
and
there
and
body
movements,
mainly
pointing
gestures.
Typical
natural
language
examples
would
be
the
utterance
of
Close
that
door!
while
106
TagLog
System
Overview
pointing
to
a
door,
or
the
utterance
of
This
is
a
noun.
,
or
What
kind
of
word
is
this?
,
while
pointing
to
a
word
in
a
text.
In
TagLog,
talking
about
text
typically
involves
deictic
reference
to
particular
segments
of
text,
by
selecting
with
the
mouse,
and
the
making
of
statements,
commands
or
queries,
by
means
of
choosing
from
menus
or
by
means
of
the
formal
language.
In
this
sense,
deixis
establishes
a
link
between
expressions
in
the
TagLog
formalism
and
the
graphical
representation
of
text.
5.6.2
Talking
with
TagLog
tools
about
texts
and
theories
In
an
interesting
paper,
Brennan
1990
advocates
a
pragmatics
approach
to
human-computer
interaction.
Her
view
is
that:
...thinking
of
user
interfaces
in
terms
of
real
conversation
leads
to
more
specific
predictions
about
how
people
will
behave
and
to
more
informative
explanations
of
why
interface
features
succeed
or
fail.
The
hope
is
that
employing
the
conversation
metaphor
will
lead
to
insights
in
the
kinds
of
architectures
and
features
that
are
necessary
to
support
cooperation
between
people
and
computers.
In
accordance
with
these
ideas,
we
will
regard
the
user
s
interaction
with
the
TagLog
system
as
a
kind
of
conversation,
a
conversation
about
texts
and
theories,
a
conversation
involving
both
the
use
of
a
declarative
formal
language,
direct
manipulation,
and
menu
selection.
For
example,
in
TagLog,
the
process
of
hand
coding,
i.e.
the
process
of
describing
a
segment
of
text
as
belonging
to
a
certain
category,
can
be
regarded
as
a
kind
of
dialogue,
comprising
two
turns:
the
user
selects
a
segment
in
the
text
direct
manipulation
,
chooses
a
tag
in
the
Tag
menu
menu
selection
,
and
chooses
an
appropriate
value
from
the
Value
menu
menu
selection
.
In
the
second
turn,
the
system
gives
feedback
typically
by
using
language
to
indicate
whether
it
accepts
the
description
or
not.
As
another
example,
a
query
from
the
user
to
the
system
typically
involves
the
user
writing
a
formal
declarative
specification
in
a
text
entry
field
formal
language
use
,
choosing
from
menus
to
set
additional
parameters
menu
selection
,
and
clicking
a
button
in
order
to
execute
the
query
menu
selection
.
The
system
response
is
sometimes
given
as
statements
in
the
formal
language,
and
sometimes
it
involves
the
setting
of
a
selection
of
a
segment
in
the
current
text
direct
manipulation
.
This
turn-
The
Choice
of
Interaction
Styles
107
taking
scheme
could
be
extended
with
turns
where
the
user
demands
an
explanation
of
a
statement
by
pointing
to
it
and
clicking
a
button
and
the
system
gives
such
explanation,
which
in
turn
would
imply
more
interaction
with
the
user
in
the
same
manner.
The
notion
of
conversation
may
seem
to
stand
in
some
kind
of
opposition
to
the
notion
of
direct
manipulation.
However,
as
studies
of
face-to-face
communication
clearly
show,
conversation
usually
involves
not
only
words,
but
also
pointing
gestures,
nods,
eye-to-eye
contact,
um
humns,
sign
language,
etc.
Moreover,
communication
most
often
takes
place
in
the
context
of
some
activity
involving
non-communicative
acts
as
well.
I
agree
with
Brennan
when
she
claims
that:
...the
dichotomy
between
direct
manipulation
and
conversation
is
a
false
one
..
First,
direct
manipulation
interfaces
succeed
because
they
share
important
features
with
real
conversations.
Second,
when
so-called
conversational
interfaces
fail,
it
is
because
they
lack
these
pragmatic
features
that
is,
words
alone
do
not
a
conversation
make.
Third,
real
conversations
actually
fulfil
many
of
the
criteria
for
direct
manipulation.
The
conversational
view
is
a
powerful
design
metaphor
for
a
corpus
theory
development
system.
By
looking
at
the
interaction
between
the
user
and
the
system
as
a
conversation,
we
are
free
to
apply
maxims
of
cooperative
communication
Grice,
1975;
Allwood,
1976
,
and
on
such
grounds
design
the
most
efficient
and
cooperative
dialogue
possible.
Besides,
it
is
reasonable
to
believe
that
a
potential
user
of
the
system
will
feel
quite
at
home
with
this
view,
especially
if
the
user
is
a
linguist.6
To
sum
up,
the
TagLog
working
environment
integrates
a
logical
approach
to
description
and
analysis
of
texts,
and
a
theory
revision
approach
to
scientific
method,
with
a
conversational
view
on
human-computer
interaction,
while
still
exploiting
high-resolution
graphics,
interface
objects
like
menus
and
windows,
and
a
mouse.
The
interaction
between
6.
To
talk
about
ourselves
as
having
conversations
with
our
tools
is
not
as
strange
as
it
may
sound.
In
particular,
this
view
does
not
imply
that
we
must
regard
the
tools
as
intelligent,
alive,
human,
or
conscious;
and
this
is
fortunate,
since
I
agree
with
Shneiderman
that
such
anthropomorphism
should
be
avoided
in
most
circumstances.
It
may
be
charming
for
a
while,
but
it
is
very
tiring
in
the
long
run.
I
am
confident
that
we
can
regard
TagLog
as
a
set
of
tools
with
which
the
user
interacts,
without
throwing
away
the
insight
that
interaction
with
computers
is
a
kind
of
communicative
behavior
a
kind
of
conversation.
108
TagLog
System
Overview
researcher
and
system
is
analysed
in
terms
of
statements,
questions
and
commands,
and
deixis
is
seen
as
an
important
link
of
integration
between
expressions
in
the
TagLog
formalism
and
the
graphical
representations
of
text,
and
between
menu
commands
and
such
representations.
The
principles
underlying
human
conversation
are
very
powerful,
and
I
strongly
believe
that
a
theory
which
is
able
to
explain
why
some
interfaces
work
while
others
do
not,
will
have
to
relate
to
these
principles
in
one
way
or
another.
5.7
Architecture
Logic
is
good
for
achieving
machine
independence,
implementation
independence,
even
proof
strategy
independence.
Indeed,
the
logical
approach
to
corpus
linguistics
does
not
have
to
involve
an
Apple
Macintosh;
it
does
not
have
to
involve
a
von
Neumann
computer;
it
does
not
have
to
involve
the
Prolog
programming
language;
it
does
not
have
to
involve
a
depthfirst
search
strategy,
nor
does
it
have
to
involve
the
resolution
inference
rule.
Now,
it
just
so
happens
to
involve
all
that
in
the
implementation
reported
on
here,
but
it
does
not
have
to.
5.7.1
The
current
system
TagLog
for
the
Macintosh
is
built
as
an
extension
of
a
Prolog
system.
Around
the
Prolog
kernel,
a
layer
of
TagLog
specific
procedures
are
built,
which
the
user
can
run
by
manipulating
the
menus
and
buttons
in
the
user
interface,
or
by
calling
them
from
the
command
line
interface.
User
Interface
TagLog
Engine
Prolog
System
Thus,
TagLog
is
implemented
on
top
of
rather
than
in
Prolog.
There
is
almost
never
any
level
of
interpretation
between
TagLog
and
Prolog.
Most
of
the
time,
TagLog
uses
the
raw
power
of
Prolog
to
perform
its
reasoning;
TagLog
theories
are
run
as
Prolog
programs.
Architecture
109
5.7.2
Alternative
backbone
systems
To
base
TagLog
on
Prolog
is
not
the
sole
option.
As
can
be
seen
in
Figure
22,
there
are
a
number
of
alternative
backbone
systems
around.
User
Interface
TagLog
Engine
Prolog
System
External
Database
User
Interface
TagLog
Engine
Deductive
DB
User
Interface
TagLog
Engine
Parallel
Prolog
User
Interface
TagLog
Engine
G
del
System
FIGURE
22.
Four
ways
of
building
a
TagLog
system
By
coupling
a
Prolog
system
to
an
external
database,
e.g.
a
relational
database,
it
is
possible
to
lessen
the
RAM
requirement
of
TagLog.
The
cost
in
terms
of
speed
will
be
great,
however,
and
besides,
RAM
is
cheap
these
days.
A
deductive
database
is
a
generalization
of
a
relational
database,
where
not
only
facts,
but
also
rules,
just
like
in
Prolog,
can
be
used.
In
fact,
the
coupling
of
Prolog
with
a
relational
database
described
above
can
be
regarded
as
a
poor
man
s
version
of
a
deductive
database.
Since
deductive
database
technology
aims
at
very
large
fact
bases
and
rule
bases,
they
seem
to
be
very
good
candidates
for
a
TagLog
backbone.7
However,
the
technology
has
not
really
matured
yet,
and
as
far
as
I
am
aware,
no
commercial
deductive
database
management
system
DDMS
exists.
The
prospects
for
fast
parallel
Prologs
are
very
good.
As
a
matter
of
fact,
at
the
time
of
writing
this,
Sicstus
Prolog
version
3.0
comes
with
library
procedures
for
running
Prolog
in
parallel
running
a
particular
example
program
with
5
instead
of
1
processor
is
reported
to
speed
up
the
execution
with
a
factor
of
3.8!
.
G
del
Hill
Lloyd,
1991
is
one
particular
example
of
a
logic
programming
language
that
addresses
some
of
the
shortcomings
of
Prolog,
in
particular
logical
shortcomings.
Mercury
Zomogyi,
Henderson
Conway,
7.
In
an
interesting
paper,
Dahl,
Huang,
Tarau,
1994
show
that
logic
grammars
may
very
well
be
implemented
within
a
deductive
database
framework,
and
that
some
problems
that
Prolog
has
with
logic
grammars,
in
particular
the
difficulty
to
handle
left-recursive
grammars,
disappear
within
this
framework.
110
TagLog
System
Overview
1994
is
another
logic
programming
language
efficient
and
purely
declarative
that
looks
promising.
5.7.3
Client-server
architecture
The
current
Macintosh
implementation
of
TagLog
is
a
rather
traditional
single-user
application.
A
client-server
architecture
is
an
interesting
alternative.
In
a
client-server
TagLog
system,
a
central
TagLog
server
would
handle
all
the
hard-core
reasoning
and
other
processing.
It
would
also
maintain
single
copies
of
texts
and
theories
and
make
them
available
to
all
users,
resulting
in
a
cut
down
on
data
redundancy
and
potential
inconsistency.
A
protocol
would
serve
as
the
sole
interconnection
between
the
server
and
the
clients.
Clients
User
Interface
User
Interface
User
Interface
TagLog
Engine
Server
FIGURE
23.
TagLog
as
a
client-server
system
When
a
client
tool
needs
information
from
a
theory,
it
would
form
a
request
for
it
and
send
that
request
to
the
server.
After
the
server
has
performed
the
desired
reasoning
or
processing,
it
would
send
the
resulting
information
back
over
the
net
to
the
client.
A
single-user
setup
is
merely
a
special
case
of
this
schema,
where
the
server
and
one
client
reside
on
the
same
machine.
Since
the
user
interface
code
always
the
most
difficult
code
to
port
is
restricted
to
the
clients,
a
client-server
architecture
also
has
the
potential
of
making
porting
between
different
platforms
easier.
Efficiency
111
5.8
Efficiency
It
follows
from
the
fact
that
TagLog
theories
are
run
as
Prolog
programs
that
the
performance
of
TagLog
is
dependent
on
the
performance
of
Prolog.
Prolog
has
a
reputation
of
being
slow
that
probably
dates
back
to
the
years
when
Prolog
was
interpreted.
But
many
modern
Prolog
systems
of
today
come
with
both
an
interpreter
and
a
compiler.
Some
systems
even
allow
for
compilation
to
machine
code.
Reasoning
in
a
compiled
theory
is
very
efficient.
Still,
as
we
shall
see,
in
particular
the
logical
approach
to
automatic
tagging,
and
lemmatization
as
abduction,
have
efficiency
problems
which
might
even
render
them
practically
useless
until
these
problems
are
solved.
Moreover,
certain
other
operations
in
TagLog
might
turn
out
to
take
a
very
long
time,
if
one
is
not
careful
when
formulating
queries
and
specifications.
Fortunately,
fairly
general
techniques
exist
for
optimization
of
Horn
clause
queries:
dynamic
reordering
of
goals
so
as
to
constrain
the
search
for
solutions
as
early
as
possible
see
e.g.
Rayner,
1993
,
memo
functions
to
save
the
results
of
sub-computations
to
be
used
later
in
computations
see
e.g.
Sterling
Shapiro,
1986
,
etc.
Furthermore,
optimization
techniques
that
use
knowledge
of
invariants
in
the
domain
of
text
in
order
to
rewrite
queries
into
forms
more
suitable
for
efficient
computations
would
probably
be
possible,
although
this
would
need
to
be
investigated
in
more
detail.
Finally,
certain
kinds
of
reasoning
in
TagLog
theories
are
probably
more
efficient
to
implement
as
text
crunching
procedures
instead.
This
does
not
mean
that
we
are
back
to
square
one:
we
have
realized
that
text
crunching
is
something
that
belongs
on
the
level
of
implementation,
not
on
the
conceptual
logical
level.
5.9
Summary
and
Conclusion
In
this
chapter,
the
design
of
the
TagLog
system
a
computer
program
implementing
the
logical
approach
to
corpus
linguistics
and
supporting
112
TagLog
System
Overview
the
activity
of
corpus
linguistics
was
discussed.
Since
the
development
of
corpus
theories
is
best
described
as
a
theory
revision
problem
the
corpus
is
usually
approached
with
a
theory
that
will
have
to
be
tailored
to
fit
that
particular
corpus
I
have
designed
the
system
as
a
theory
revision
system,
a
balanced
cooperative
system.
What
emerges
is
a
picture
suggesting
that
the
computer
supported
corpus
research
envisioned
in
the
present
work
is
quite
a
complex
activity
consisting
of
very
many,
in
essence,
highly
interrelated
subactivities:
the
user
observing
the
parts
of
a
text
presented
by
the
system;
the
user
asking
the
system
to
present
a
new
part
of
the
text
in
a
new
way
;
the
user
describing
the
segments
of
the
text
and
the
system
storing
the
descriptions;
the
user
browsing
a
theory
created
by
him
and
stored
by
the
system;
the
user
demanding
an
explanation
of
a
statement
and
the
system
giving
it
to
him;
the
system
testing
a
hypothesis
supplied
by
the
user;
the
system
generating
a
prediction,
and
the
user
checking
and
confirming
or
disconfirming
it
by
adding
to
the
theory
,
the
system
finding
a
contradiction
and
the
user
resolving
it
i.e.
by
removing
statements
from
the
theory
;
the
system
suggesting
different
and
competing
hypotheses
that
explain
a
fact
and
the
user
selecting
among
them;
etc.
I
have
argued
that
throughout
this
process
it
is
the
evolving
theory,
in
relation
to
the
texts
that
it
purports
to
describe,
that
ought
to
be
focused
on.
The
interaction
between
user
and
system
is
there
to
support
this.
The
goal
is
a
system
that
vanishes
as
users
become
completely
absorbed
in
thinking
about
theories
about
texts.
But
not
only
do
the
tools
of
the
system
interact
with
the
user.
Tools
interact
with
tools
as
well.
I
have
argued
that
piping
is
not
the
best
way
of
implementing
this.
Rather,
the
use
of
a
common
uniform
representation
the
explicit
representation
of
the
evolving
theory
is
crucial
here.
In
this
chapter,
a
particular
implementation
of
TagLog
for
Apple
Macintosh
computers
has
also
been
introduced.
This
system
will
be
used,
in
this
thesis,
to
provide
a
concrete
illustration
of
the
approach.
But
it
was
emphasized
that
this
implementation,
and
indeed
any
Prolog-based
implementation,
is
not
essential
to
the
approach;
alternatives,
such
as
deductive
databases
and
logic
programming
languages
other
than
Prolog,
were
mentioned
as
well.
CHAPTER
6
Description
6.1
Introduction
In
Chapter
2,
description
was
characterized
as
a
stage
of
scientific
linguistic
study
consisting
in
categorizing
data
obtained
by
observing
the
facts
of
linguistic
reality.
Moreover,
description
was
seen
as
a
preparatory
stage
of
transition
to
the
development
of
descriptive
universal
theories.
Description
and
explanation
are
rather
closely
connected:
without
a
description
of
facts
it
is
impossible
to
explain
them.
On
the
other
hand
and
this
will
be
one
of
the
themes
in
Chapter
13
description
without
explanation
is
not
enough
for
science.
In
Chapter
3,
I
presented
the
TagLog
formalism
as
a
language
in
which
we
are
able
to
express
theories
about
texts.
In
the
first
part
of
Chapter
4,
we
looked
at
particularly
simple
kinds
of
theories,
namely
descriptions.
In
this
chapter,
the
focus
will
be
on
the
process
of
description,
or
tagging,
as
it
is
sometimes
called
in
the
context
of
corpus
linguistics.
Where
traditional
approaches
suggest
the
use
of
a
word
processor
for
entering
tags,
or
a
program
that
is
able
to
tag
a
text
automatically,
the
TagLog
approach
takes
as
its
point
of
departure
a
rather
different
view
of
what
tagging
con-
114
Description
sists
in.
From
the
point
of
view
of
logic
and
knowledge
representation,
the
process
of
describing
a
text
must
involve
changing
what
follows
from
the
theory
purporting
to
describe
the
text,
i.e.
updating
this
theory.
I
use
the
slogan
Tagging
with
a
Theory
to
designate
this
approach.
Within
this
framework,
there
are
kinds
of
tagging
that
are
possible
to
perform
in
an
automatic
or
semi-automatic
fashion,
and
we
will
return
to
some
such
methods
in
Chapter
11.
But
we
will
begin
by
considering
tagging
as
a
manual
process.
I
call
this
hand
coding.
6.2
The
Importance
of
Hand
Coding
For
some
reason,
tagging
has
become
almost
synonymous
with
automatic
tagging
.
Moreover,
a
lot
of
work
has
been
done
on
automatic
tagging,
whereas
hand
coding
has
received
very
little
attention,
if
any
at
all.
The
present
thesis
takes
the
issue
of
hand
coding
rather
more
seriously.
In
my
view,
there
are
many
reasons
for
not
neglecting
this
subject:
Hand
coding
is
linked
to
observation,
which
is
an
extremely
important
notion
of
empirical
science.
Through
his
senses,
a
human
observer
can,
in
principle,
have
full
access
to
the
local
and
global
contexts
and
co-texts
of
segments
of
text.
Many
coding
tasks
cannot,
at
present,
be
performed
automatically,
and
must
therefore
be
done
by
hand.
A
description
often
needs
to
be
revised
in
the
light
of
the
findings
when
using
an
analysis
tool.
There
is
no
clear-cut
division
between
manual
and
automatic
tagging
and
there
are
many
opportunities
for
semi-automatic
tagging.
Automatic
taggers
often
need
manually
coded
corpora
in
order
to
learn
to
perform
their
tasks
automatically.
An
automatically
tagged
corpus
often
needs
to
be
corrected
manually.
The
Elements
of
Hand
Coding
115
6.3
The
Elements
of
Hand
Coding
Analytically,
one
can
distinguish
at
least
the
following
steps
in
the
complex
process
of
a
user
U
describing
a
fact
F
to
a
system
S,
by
stating
P
by
means
of
a
sentence
S:
1.
U
observes
F
2.
U
decides
to
describe
F
3.
U
forms
a
sentence
S
by
grouping
together
certain
syntactic
primitives
in
a
language
4.
By
asserting
S,
and
perhaps
also
by
pointing
to
a
segment
or
to
another
object,
U
states
P
5.
S
checks
that
P
is
consistent
with
what
S
already
knows
6.
S
stores
P
by
storing
S,
or
something
equivalent
to
S
In
the
following
sections,
under
the
headings
of
Observing,
Deciding,
Framing,
Stating,
Checking,
and
Storing,
each
of
these
steps
will
be
discussed.
6.3.1
Observing
Empirical
science
is
based
on
observation.
But
in
corpus
linguistics,
what
is
it
we
observe?
In
general,
and
this
is
how
I
like
to
see
it,
knowledge
about
a
text
is
obtained
through
observation
of
the
facts
that
make
up
the
text.
The
facts
that
form
the
content
and
the
context,
and
the
facts
that
relate
the
text
to
its
content
and
context,
can
sometimes
be
observed
as
well,
and
are
important.
For
the
purpose
of
the
present
discussion,
observation
is
to
be
regarded
as
perception
of
the
outside
world
for
the
purpose
of
providing
the
primary
data
for
scientific
research.
In
addition,
observation
may
be
classified
as
simple
or
complex,
direct
or
indirect,
aided
or
unaided,
etc.
In
many
sciences,
observation
implies
the
use
of
various
instruments
such
as
microscopes
and
telescopes
to
compensate
for
the
natural
limitations
of
human
sense
organs.
The
use
of
various
vehicles
such
as
space
craft
and
submarines
to
support
transportation
to
advantageous
points
of
observation
is
also
common.
Texts
are
made
to
be
read,
so
the
corpus
linguist
does
not
116
Description
need
sense-amplifying
instruments.
The
need
for
transportation
is
there,
however,
since
the
textual
reality
is
immense,
almost
endless,
so
that
the
lack
of
complete
overview
must
be
compensated
for
by
support
for
mobility.
We
will
return
to
this
point
in
Chapter
7.
Obviously,
reading
may
be
regarded
as
one
particular
form
of
observation
of
texts.
The
particular
form
of
knowledge
obtained
through
reading,
namely
knowledge
of
the
content
of
the
text,
is
what
we
mean
by
understanding.
Now,
the
purpose
of
corpus
linguistics
is
not
to
read
and
understand
the
content
of
texts,
but
to
describe,
analyse
and
explain
them
and
then
often
other
aspects
than
their
content
.
So,
in
this
respect,
scientific
observation
of
text
is
different
from
reading.
How
does
scientific
observation
work
in
corpus
linguistics?
Let
us
begin
with
some
simple
observations
no
pun
intended
:
A
linguist
seems,
at
least
as
long
as
we
are
not
too
philosophical
about
it,
to
be
able
to
directly
observe
at
least
some
of
the
facts
of
a
text,
even
in
cases
where
these
facts
appear
to
be
very
high-level
and
complex.
For
example,
think
of
someone
observing
the
fact
that
a
particular
segment
of
text
is
a
sentence,
or
that
it
functions
as
a
question,
or
that
it
relates
to
context
in
a
particular
way.
Usually,
there
is
no
trace
of
conscious
derivations
of
high-level,
complex
facts
from
more
low-level,
basic
ones.
In
fact,
the
linguist
may
not
even
be
able
to
observe
the
more
low-level
basic
facts!
Seemingly
without
effort,
and
usually
without
even
noticing
any
alter-
natives,
researchers
seem
to
be
able
to
disambiguate
between
different
senses
of
words,
different
interpretations
of
sentences,
different
possible
communicative
purposes
of
uttering
a
sentence,
and
so
on.
However,
to
its
full
extent,
the
above
holds
only
if
the
linguist
in
ques-
tion
is
properly
trained.
Moreover,
observation
is
not
infallible,
and
once
in
a
while,
even
a
trained
linguist
does
make
observational
mistakes,
seeing
something
which
is
not
there,
or
missing
the
obvious.
Clearly,
what
makes
all
this
possible
is
the
researcher
s
in
principle
more
or
less
unrestricted
access
to
the
contexts
of
text
segments
under
investigation,
together
with
his
theoretical
and
common
sense
knowledge,
tacit
and
explicit.
It
would
be
far
beyond
the
scope
of
this
thesis
to
attempt
to
unpack
all
this,
however,
and
we
will
content
ourselves
with
the
basic
insights
as
they
stand:
trained
researchers
are
able
to
observe
the
facts
of
The
Elements
of
Hand
Coding
117
linguistic
reality,
and
although
the
observations
are
usually
correct,
mistakes
are
certainly
possible.
Just
a
few
remarks
to
link
all
this
to
some
intensely
debated
questions
in
the
philosophy
of
science:
What
is
observable?
Popper
claims
that
observability
is
a
notion
that
can
only
become
sufficiently
precise
in
use,
in
the
context
of
a
particular
scientific
practice
cf.
Popper,
1980,
p.
103
.
The
assumption
that
I
make
in
the
present
connection
is
that
all
facts
of
texts
are
in
principle
observable.
I
simply
do
not
know
of
any
convincing
attempt
to
demarcate
between
what
is
observable
and
what
is
not,
that
would
deem
some
facts
of
text
observable,
but
exclude
others.
Now,
it
is
true
that
we
cannot
directly
observe
what
goes
on
in
the
minds
of
the
writer
or
the
reader
of
a
text,
so
either
we
could
decide
that
what
goes
on
in
people
s
minds
is
not
what
corpus
linguistics
is
about
anyway,
or
we
could
decide
that
it
is
appropriate
for
a
corpus
linguist
to
make
assumptions
about
rather
than
to
observe
what
goes
on
in
people
s
minds,
and
to
build
his
theories
on
these.
Besides,
we
may
not
be
able
to
determine,
just
by
looking
at
a
particular
segment
of
text,
the
nature
of
the
beliefs
and
attitudes
that
made
the
writer
produce
it,
and
we
may
not
be
able
to
determine
what
reading
this
segment
gave
rise
to
in
the
mind
of
a
particular
reader,
but
we
are
presumably
in
the
position
to
say
that
a
certain
phrase
in
a
text
presupposes
certain
beliefs
or
attitudes
on
the
parts
of
the
writer
and
the
reader,
and
to
say
that
a
particular
phrase
has
a
certain
content,
just
by
looking
at
that
segments
in
context.
But
then
we
are
talking
about
the
text,
not
about
the
minds
of
the
reader
and
the
writer.
Texts
are
public
objects,
no
less
public
than
chairs,
or
football
games,
or
microwave
ovens.
Researchers,
given
that
they
have
been
appropriately
trained,
are
able
to
observe
facts
relating
to
the
syntactic
properties
of
text
segments,
facts
relating
to
the
contents
of
texts,
and
other
facts
relevant
to
them.
Unless
their
theoretical
backgrounds
differ
considerably,
they
are
normally
willing
to
agree
on
what
they
see,
and
they
are
usually
convinced
that
if
they
go
away
and
then
come
back
again,
they
will
be
able
to
make
the
same
observations.
As
far
as
I
can
see,
this
is
what
it
takes
to
be
observable.
Is
observation
theory
neutral?
In
contrast
to
the
logical
positivists,
who
used
to
think
of
observation
as
being
unaffected
by
theoretical
perspec-
118
Description
tive
of
the
trained
observer,
most
philosophers
of
today
tend
to
think
of
observation
as
necessarily
theory
impregnated.
As
should
be
evident
from
the
above,
I
see
no
reason
to
think
otherwise.
However,
knowing
that
observation
is
theory
impregnated
certainly
does
not
make
observation
less
important
for
empirical
science;
it
is
still
the
only
tie
we
have
to
reality,
be
it
texts,
be
it
other
parts
of
the
world.1
We
cannot
resolve
these
questions
here,
nor
even
discuss
them
in
any
depth.
My
point
is
that
I
think
that
there
is
hope
in
corpus
linguistics
for
a
notion
of
observability
that
can
become
sufficiently
precise
in
use
.
In
conclusion,
it
seems
obvious
to
me
that
when
designing
a
system
for
description
and
analysis
of
texts,
the
remarkable
capability
of
a
trained
linguist
to
observe
what
is
really
there
in
a
text
should
not
be
allowed
to
go
to
waste.
This
capability
should
be
exploited
and
supported
to
the
greatest
possible
extent,
and
nothing
should
be
allowed
to
hamper
it.
6.3.2
Deciding
Needless
to
say,
not
every
fact
that
happens
to
be
observed
by
the
linguist
is
considered
worth
describing.
The
linguist
is
likely
to
decide
to
describe
only
facts
that
he
thinks
will
matter
to
his
future
theory.
6.3.3
Framing
After
the
observation
of
a
fact,
and
the
decision
to
describe
it,
comes
framing
of
the
description.
Description
of
a
fact
F
involves
the
act
of
composing
a
sentence
S
that
expresses
a
statement
that
describes
F.
The
symbols
of
S
refer
to
the
elements
of
F
by
virtue
of
the
intended
interpretation
of
the
description
language.
Segment-expressions,
predicate
symbols
and
terms
refer
to
segments,
properties
and
property
values,
respectively.
For
example,
given
the
observation
of
a
particular
segment
in
the
current
text
as
a
noun,
and
given
that
the
intended
interpretations
of
the
symbols
lexcat
and
n
are
lexical
category
and
noun,
respectively,
and
that
the
1.
Let
me
also
say
that
I
oppose
the
view,
which
has
been
held,
that
a
theory
about
texts
should
take
as
its
point
of
departure
its
foundation
the
printer
s
ink,
the
laserwriter
s
toner,
or
the
pixel
pattern
on
the
screen
of
a
computer.
This
may
be
an
appropriate
point
of
departure
for
a
theory
of
our
perception
of
the
outside
world,
but
it
would
hardly
serve
the
needs
of
corpus
linguistics.
The
Elements
of
Hand
Coding
119
intended
interpretation
of
the
term
3-4
is
that
it
denotes
that
particular
segment,
the
sentence
in
101
is
composed.
101
lexcat
3-4,n
6.3.4
Stating
In
this
thesis,
stating
something
means
adding
it
to
the
current
theory.
The
TagLog
built-in
predicate
add
1
will
add
a
unit
clause
expressing
a
singular
statement
given
in
the
argument
to
the
current
theory,
e.g,
the
call
in
102
,
will
add
the
clause
lexcat
3-4,n
to
the
current
theory.
102
!-
add
lexcat
3-4,n
.
Clearly,
since
the
names
of
the
segments
are
not
very
user
friendly
,
a
better
way
to
refer
to
them
is
often
the
kind
of
demonstrative
reference
that
the
use
of
the
built-in
predicate
selection
1
allows.
103
!-
selection
S
,
add
lexcat
S,n
.
The
execution
of
the
selection
1
procedure
binds
S
to
the
expression
denoting
the
current
segment,
i.e.
to
the
segment
highlighted
in
the
current
text,
and
the
execution
of
the
add
1
procedure
adds
the
resulting
statement
to
the
current
theory.
There
are
other
ways
to
assert
a
statement
than
to
write
a
sentence
down,
character
by
character,
in
a
text
editor,
or
in
a
command
line
interface,
and
then
issue
a
command.
In
TagLog,
the
most
important
alternative
is
to
compose
statements
by
pointing
to
segments,
and
selecting
from
menus.
The
segment
which
is
selected
highlighted
in
the
Text
window
is
called
the
current
segment.
In
order
to
tag
the
current
segment
with
a
value
for
a
tag
selected
from
the
Tag
menu,
a
value
is
chosen
from
the
Value
menu.
This
action
will
add
a
statement
of
the
form
P
S,V
to
the
current
theory,
where
P
is
the
tag,
S
the
current
segment
and
V
the
chosen
value.
120
Description
FIGURE
24.
Hand
coding
in
TagLog
Technically,
what
happens
is
that
the
following
TagLog
goal
is
called:
104
!-
selection
S
,
add
P
S,V
.
Again,
note
that
even
though
it
would
also
be
accurate
to
say
that
we
are
pointing
to
a
string,
we
are
not
saying
anything
about
that
string.
Only
if
selecting
from
the
Tag
menu
were
to
introduce
a
sentence
syncat
loves
,v
would
it
be
correct
to
say
that
we
were
saying
something
about
a
string.
Hand
coding
means
speaking
about
segments,
not
about
strings.
6.3.5
Checking
A
yes
-response
from
the
system,
following
directly
after
a
statement
from
the
user,
indicates
that
the
statement
was
accepted
and
will
be
stored
accordingly.
In
other
words,
it
works
like
a
kind
of
feedback.
105
!-
selection
S
,
add
lexcat
S,n
.
yes
There
is
at
least
one
occasion
when
an
attempt
to
update
the
current
theory
should
not
lead
to
an
actual
update,
namely
when
the
statement
gives
rise
to
logical
inconsistency.
Therefore,
add
1
implements
a
consistency
checking
step,
and
the
next
step
the
storing
step
is
taken
only
if
the
consistency
checking
step
succeeds.
If
it
fails,
the
current
implementation
will
notify
the
user
and
refuse
to
update
the
current
theory.
Coding
as
a
Dialog
121
6.3.6
Storing
The
storing
step
is
where
the
system
makes
itself
remember
the
sentence
composed
in
the
framing
step,
and
asserted
in
the
stating
step.
Given
a
Prolog
backbone
to
TagLog,
the
storing
step
is
implemented
by
the
Prolog
database
manipulation
predicates,
where
assert
1
is
the
most
important
one.
The
most
important
effect
of
asserting
a
clause
is
that
it
becomes
active
in
the
sense
that
it
becomes
visible
to
the
TagLog
prover,
and
thus
that
it
can
take
part
in
reasoning,
as
a
premise
in
a
deductive
step.
The
clause
is
stored
in
the
internal
memory
of
the
computer
at
least
this
is
true
for
the
current
implementation
,
and
is
indexed
for
efficient
access.
Efficient
access
to
knowledge
is
of
course
an
important
requirement
for
a
system
like
TagLog,
but
it
will
not
be
further
discussed
here.
6.4
Coding
as
a
Dialog
In
TagLog,
the
process
of
coding
can
be
regarded
as
a
kind
of
dialogue,
in
which
the
user
makes
statements
about
segments
by
highlighting
the
segments
and
selecting
from
menus.
The
role
of
the
system
in
this
dialog
is
to
accept
or,
in
certain
cases,
refuse
to
accept
the
statements,
and
to
remember
them,
e.g.
106
U:
This
is
a
noun
phrase
pointing
to
a
segment
S
:
Ok
Furthermore,
I
would
like
to
promote
the
view
of
description
and
analysis
as
two
aspects
of
the
same
ongoing
conversation
,
the
purpose
of
which
is
talking
about
text
.
The
following
dialog
illustrates
the
idea:
107
U:
Please
find
me
a
sentence
in
this
text
S
:
Ok,
here
s
one
pointing
to
a
segment
U:
WHY
is
this
a
sentence?
S
:
It
consists
of
a
noun
phrase
followed
by
a
verb
phrase,
etc.
U:
I
would
regard
this
as
an
instance
of
a
question
then
S
:
Ok
In
simple
terms,
this
dialog
can
be
seen
as
a
request
for
search,
followed
by
an
ostensive
demonstration
by
the
system
of
what
was
found.
Then
a
122
Description
request
for
explanation,
followed
by
an
explanation.
Finally,
a
description
followed
by
a
kind
of
feedback
indicating
acceptance
of
the
description.
There
are
at
least
two
good
reasons
for
integrating
the
functionalities
for
description
and
analysis
in
one
single
system,
and
to
build
support
for
cooperation
between
them,
or
at
least
for
quick
and
effortless
switching.
First,
a
description
often
needs
to
be
revised
in
the
light
of
the
findings
that
the
researcher
made
when
using
an
analysis
tool.
Secondly,
analysis
can
often
lend
a
substantial
amount
of
support
to
the
process
of
description.
6.5
Hand
Coding
in
the
TagLog
System
Hand
coding
is
a
time-consuming
and
error
prone
process.
It
therefore
lies
in
the
interest
of
the
researcher
to
make
coding
as
fast
and
accurate
as
possible.
The
TagLog
system
supports
various
methods
for
speeding
up
the
process
of
coding,
means
of
providing
feedback
in
order
to
ensure
accuracy,
and
simple
ways
to
correct
an
error,
should
it
arise
anyway.
6.5.1
Ways
to
speed
up
coding
An
important
way
to
speed
up
hand
coding
is
to
use
the
Search
Tool
or
the
Concordance
Tool
in
order
to
set
the
current
selection
more
or
less
automatically
to
segments
denoted
by
a
set-expression
given
by
the
user.
More
on
this
in
Chapter
7.
To
support
quick
and
easy
access
to
menus,
the
TagLog
system
features
so
called
tear-off
menus.
The
Value
menu
can
be
torn
off,
and
be
made
to
float
at
any
position
of
the
screen.
This
is
often
a
faster
way
of
coding
since
the
pointer
does
not
have
to
be
moved
all
the
way
up
to
the
menu
bar
all
the
time.
The
Repeat
command
of
the
Value
menu
repeats
the
assertion
of
the
most
recently
added
statement,
but
for
the
current
segment.
This
command
has
a
keyboard
equivalent
which
allows
the
user
to
select
segment
after
segment
and
just
press
these
keys
in
between,
in
order
to
assign
tags
to
these
segments.
This
is
a
way
to
speed
up
the
tagging
process
considerably
when
there
are
many
segments
that
are
to
be
assigned
the
same
tag
value.
Hand
Coding
in
the
TagLog
System
123
Although
not
implemented
yet
in
the
current
system,
it
is
easy
to
imagine
a
facility
for
assigning
tags
and
tag
values
to
keys
and
key
combinations
on
the
computer
s
keyboard.
For
some
tasks,
this
might
be
the
fastest
way
to
code
a
text.
6.5.2
Feedback
when
coding
The
Statement
Browser
is
able
to
display
the
statements
describing
the
segment
currently
selected
in
the
Text
window.
Updates
to
these
statements
will
always
be
dynamically
reflected
in
the
Statement
Browser.
See
Chapter
8
for
more
details
about
the
Statement
Browser.
Furthermore,
as
already
mentioned,
the
system
is
able
to
make
various
consistency
checks.
The
current
way
to
inform
the
user
of
an
inconsistency
is
to
display
an
error
message
in
the
usual
Macintosh
way.
6.5.3
Undoing
statements
When
coding,
mistakes
are
easily
made.
There
are
currently
two
ways
to
withdraw
a
statement,
in
order
to
repair
such
a
mistake:
1.
The
most
recently
asserted
statement
may
be
taken
back
by
means
of
the
Undo
command
found
under
the
Edit
menu.
The
actual
wording
of
the
Undo
menu
item
indicates
if
there
is
a
statement
that
can
be
taken
back.
2.
The
Statement
Browser
allows
you
to
delete
any
singular
statement
about
a
segment
from
the
current
theory.
Place
the
insertion
point
on
the
statement
you
want
to
delete
and
press
the
Backspace
key,
or
choose
Clear
from
the
Edit
menu.
6.5.4
Macros
Many
tag
values
and
this
is
especially
true
for
terms
representing
the
kind
of
categories
that
modern
grammar
theory
talks
about
can
be
rather
unwieldy,
and
they
can
be
very
many,
too
large
and
too
many
to
fit
in
a
menu.
The
macros
in
TagLog
address
both
of
these
problems.
The
TagLog
formalism
allows
the
user
to
define
macros
which
expand
into
property
values
at
tagging
time
.
For
example,
108
defines
a
macro
named
v4
and
109
declares
v4
to
be
a
possible
value
of
syncat.
108
109
v4
macro
v
finite,
np
nom
,np
obj1
,np
obj2
.
tag
value
syncat,
v4
.
124
Description
Then,
if
a
segment
say
5-6
is
selected
in
a
text,
and
if
v4
is
selected
from
the
Value
menu,
then
the
statement
syncat
5-6,v
finite,
np
nom
,
np
obj1
,np
obj2
is
added
to
the
current
theory.
The
result
of
the
expansion
of
a
macro
can
be
made
to
depend
on
what
state
the
system
is
in,
e.g.
on
the
current
selection.
For
example,
110
defines
a
macro
named
iv,
and
111
declares
iv
to
be
a
possible
value
of
syncat.
110
iv
macro
s
np:X
:P
:selection
S
,
string
S,W
,
string2atom
W,F
,
P
..
F,X
.
tag
value
syncat,
iv
.
111
Then,
if
a
segment
say
23-24
is
selected
in
a
text,
and
if
this
segment
is
an
instance
of
the
string
walks
,
and
if
iv
is
selected
from
the
Value
menu,
then
the
statement
syncat
23-24,s
np:X
:walks
X
is
added
to
the
current
theory.
The
TagLog
macro
facility
is
hardly
of
any
theoretical
interest,
and
it
does
not
increase
the
expressive
power
of
the
formalism.
Macros
are
just
one
of
those
things
that
makes
TagLog
practical,
in
particular
for
handling
the
large
and
complex
structures
dealt
with
in
unification-based
grammars.
6.6
Summary
and
Conclusion
I
have
argued
that
from
a
TagLog
point
of
view,
the
process
of
tagging
a
text
means
changing
what
follows
from
the
theory
purporting
to
describe
the
text,
i.e.
updating
this
theory.
This
can
be
done
by
hand,
and
in
some
cases
it
can
be
done
automatically.
This
chapter
dealt
with
hand
coding.
The
TagLog
system
was
shown
to
support
the
view
of
hand
coding
as
an
interactive
dialog
between
the
system
and
the
user,
where
the
user
makes
statements
about
segments
by
highlighting
them
and
selecting
from
a
menu.
Various
means
of
speeding
up
coding,
ways
to
monitor
the
tagging
process
in
order
to
provide
feedback,
and
the
use
of
macros,
were
explained.
CHAPTER
7
Exploring
Linguistic
Reality
7.1
Introduction
The
distinction
between
data
and
theory
assumed
in
this
thesis
is
reflected
in
a
division
of
labour
between
two
kinds
of
browsers
in
the
TagLog
system:
there
are
browsers
for
browsing
text,
and
there
are
browsers
for
browsing
theories.
In
this
chapter
we
will
be
looking
at
text
browsing
tools,
and
in
Chapter
8
we
return
to
tools
for
theory
browsing.
As
was
noted
in
Chapter
5,
there
are
two
important
aspects
to
the
linguist
s
exploration
of
texts:
navigation
and
observation.
A
system
has
to
support
both
of
these
aspects:
it
has
to
be
able
to
provide
quick
and
accurate
ways
of
transporting
the
linguist
to
where
he
wants
to
go
in
the
text,
and
it
has
to
be
able
to
give
him
a
nice
view
of
the
surroundings
when
he
gets
there.
In
this
chapter
we
look
at
searching
and
concordancing
from
this
perspective.
The
TagLog
Search
Tool
is
able
to
take
a
user-supplied
partial
description
of
a
segment,
find
an
actual
segment
in
the
current
text
that
matches
126
Exploring
Linguistic
Reality
the
description,
point
it
out
to
the
user,
find
the
next
segment
that
matches,
point
that
out,
and
so
on.
The
Concordance
Tool
accepts
a
set-expression
denoting
a
set
of
segments,
finds
each
of
these
segments
and
presents
the
corresponding
string
on
one
line
in
a
special
window,
surrounded
by
some
of
the
co-text
of
that
occurrence.
To
extend
our
geographical
analogy,
the
Concordance
Tool
sends
postcards
from
the
linguistic
reality!
Both
of
these
tools
are
able
to
use
a
theory
e.g.
a
grammar
in
order
to
extend
their
scope.
Thus,
they
provide
a
clear
example
of
a
theory
being
used
not
only
for
the
purpose
of
answering
queries,
but
also
for
navigation.
In
this
respect,
theories
are
the
maps
and
guidebooks
to
the
linguistic
reality.
7.2
Searching
For
the
purpose
of
the
present
discussion,
searching
is
an
attempt
to
find
segments
of
text
that
satisfy
a
certain
partial
description.
The
segments
being
searched
for
will
be
referred
to
as
the
target
segments.
In
the
Artificial
Intelligence
literature,
where
searching
is
usually
used
in
a
different
sense,
it
is
sometimes
claimed
that
deduction
is
nothing
but
searching
cf.
Nilsson,
1980
.
However,
as
will
be
shown,
one
might
just
as
well
think
of
it
the
other
way
around:
searching
as
deduction.
7.2.1
Searching
as
deduction
It
may
not
be
obvious
that
the
choice
of
logic
as
a
representation
formalism
will
allow
searching
in
the
required
sense.
But,
as
a
matter
of
fact,
from
a
logical
viewpoint,
and
given
an
appropriate
representational
strategy,
searching
is
not
different
from
other
kinds
of
deduction.
As
regards
the
representational
strategy,
the
most
important
requirement
is
that
the
segments
are
made
explicit,
i.e.
that
they
are
recognized
as
members
in
the
domain
in
the
sense
of
model
theory
,
so
that
they
can
be
quantified
over.
This
is
catered
for
in
TagLog:
at
least
one
argument
in
a
singular
statement
about
text
should
be
a
segment-expression,
denoting
a
segment
in
the
text.
Searching
127
As
a
first
attempt
at
searching
as
deduction,
suppose
we
tried
to
prove
an
existentially
quantified
sentence
formulated
in
ordinary
predicate
logic
notation,
as
follows:
112
s
syncat
s,conj
-
The
problem
here
is,
of
course,
that
we
normally
would
not
be
satisfied
by
just
getting
to
know
that
there
exists
a
segment
in
the
current
text
such
that
it
is
an
instance
of
a
conjunction;
we
would
also
want
to
learn
the
exact
identity
of
that
segment.
Thus,
we
need
to
be
able
to
perform
constructive
proofs.
Fortunately,
the
TagLog
prover
is
basically
a
constructive
theorem
prover,
that
will
almost
always
bind
variables
in
the
course
of
a
proof.
Thus,
since
TagLog
is
explicit
about
segments,
and
since
the
TagLog
proof
procedure
is
able
to
constructively
infer
the
existence
of
segments
satisfying
a
particular
description,
search
is
possible
in
TagLog.
For
example,
to
search
for
instances
of
the
syntactic
category
conj
unction,
all
we
have
to
say
is:
113
!-
provable
syncat
S,conj
.
Remember
that
the
explicit
existential
quantifier
in
the
predicate
logic
formula
is
implicit
in
the
corresponding
TagLog
query.
Note
that
a
certain
amount
of
under-specification
of
property
values
corresponding
to
properties
of
the
target
segments
is
possible
in
those
cases
where
terms
denoting
property
values
are
complex.
Thus,
if
we
are
interested
in
both
singular
and
plural
nouns,
we
write
our
query
as
in
114
.
114
!-
provable
syncat
S,n
.
Furthermore,
if
we
want
to
find
the
segments
which
are
instances
of
some
syntactic
category,
a
total
under-specification
is
possible:
115
!-
provable
syncat
S,W
.
A
complex
search
query
is
a
search
query
involving
several
atomic
formulas
linked
by
connectives
like
conjunction,
disjunction
or
negation.
Such
queries
will
also
often
be
complex
in
the
sense
that
they
will
involve
several
levels
of
analysis.
Consider
the
query
in
116
,
for
example:
116
!provable
utterance
S,customer
,
mood
S,interrogative
,
function
S,question
,
segment
prefix
S,S1
,
string
S1,
do
,
subsegment
S,S2
,
syncat
S2,conj
.
128
Exploring
Linguistic
Reality
What
we
are
looking
for
here
is
an
interrogative
question,
uttered
by
customer
,
which
begins
with
an
instance
of
do
and
which
contains
a
conjunction.
It
is
easy
to
imagine
more
complex
queries,
many
of
which
are
probably
impossible
to
formulate
with
traditional
corpus
search
tools.
7.2.2
Searching
for
syntax
In
Chapter
4,
an
overview
of
different
ways
of
writing
declarative
grammars
in
the
TagLog
environment
was
given.
As
is
often
claimed,
the
appeal
of
a
declarative
grammar
is
its
bi-directionality,
i.e.
its
usability
for
generation
as
well
as
parsing
cf.
Shieber,
1986
.
Parsing
has
in
fact
been
demonstrated
already.
The
derivations
in
64
,
67
,
71
,
74
,
and
77
all
demonstrate
parsing
in
the
sense
that
given
a
segment,
the
syntactical
or
in
some
cases
the
syntactico-semantic
category
of
which
it
is
an
instance
is
derived.
While
parsing
is
certainly
something
that
the
corpus
syntactician
wants
to
be
able
to
do,
this
is
not
so
with
generation.
The
corpus
syntactician
is
usually
interested
in
searching
for
certain
grammatical
constructions
in
text,
rather
than
generating
them
from
a
grammar
and
checking
them
against
his
intuitions
of
grammaticality.
Fortunately,
the
way
we
have
encoded
the
grammar
gives
us
a
way
to
search
for
segments.
If
we
formulate
a
query,
with
the
first
argument
uninstantiated,
and
the
second
argument
instantiated
to
np,
TagLog
will
find
the
position
of
the
first
instance
of
a
noun
phrase
for
us
and
instantiate
the
first
argument
to
it:
117
!-
provable
syncat
S,
np
.
S
1-2
Then
if
we
force
TagLog
to
backtrack,
it
will
find
the
next
instance,
and
subsequently
all
the
instances,
of
noun
phrases:
118
!-
provable
syncat
S,
np
.
S
1-2
;
S
3-4
It
is
easy
to
see
that
this
search
process
relates
to
generation
in
a
certain
way.
It
works
in
the
same
direction,
in
that
we
start
out
with
a
category
and
end
up
with
a
string.
The
difference
consists
in
the
extra
constraint
that
this
string
must
be
found
in
the
text.
Instead
of
asking,
for
a
certain
Searching
129
category
is
there
a
string
in
this
language
that
is
of
this
category?
we
ask
is
there
a
segment
in
this
text
that
is
of
this
category?
.
In
the
literature
e.g.
in
Voutilainen,
1993
,
one
sometimes
finds
references
to
a
kind
of
tool
that
goes
under
the
name
of
a
phrase
detector
or
phrase
finder
.
The
idea
is
that
a
phrase
detector
would
be
able
to
take
a
specification
of
a
kind
of
phrase
and
then
find
all
occurrences
of
it
in
a
text.
Clearly,
searching
as
deduction
might
be
seen
as
one
possible,
and
nicely
principled,
step
towards
an
implementation
of
a
phrase
recognizer.
In
Chapter
14,
however,
we
will
see
that
a
phrase
detector
would
have
to
be
able
to
use
more
than
just
the
traditional
kind
of
grammar
rules;
some
kind
of
constraints
on
the
context
is
necessary
as
well.
7.2.3
Pointing
to
what
is
found
To
make
searching
more
practical,
we
use
the
TagLog
built-in
predicate
select
1
to
select
i.e.
highlight
the
segment
found,
as
in
119
.
119
!-
provable
syncat
S,conj
,
select
S
.
Calling
this
goal
will
bind
S
to
a
segment
expression
and
the
subsequent
call
to
select
1
will
select
the
segment
denoted
by
this
expression.
In
case
the
argument
of
select
1
is
not
instantiated
to
a
segment
when
it
is
being
called,
but
to
another
term
a
constant,
say
,
a
free
text
search
in
the
Situation
Theory
window
is
performed
instead,
for
occurrences
of
that
term.
For
example,
suppose
the
Situation
Theory
window
contains
the
clause
in
120
,
modelling
the
existence
of
a
person
in
the
described
or
presupposed
situation.
120
person
x34
.
If
the
call
in
121
is
performed,
the
symbol
x34
in
the
Context
window
will
be
selected.
121
!-
provable
person
X
,
select
X
.
As
we
will
see
in
Section
7.4.3,
this
is
how
we
can
use
an
integrated
grammar
in
order
to
investigate
and
navigate
the
relation
between
the
segments
of
a
text,
and
its
content
and
context,
represented
as
a
situation
theory.
130
Exploring
Linguistic
Reality
7.2.4
Searching
as
a
dialog
Note
that
the
query
in
119
can
be
regarded
as
a
formalization
of
the
natural
language
query:
Where
is
the
first
instance
of
a
conjunction?
or
as
a
formalization
of
the
request:
Please
find,
and
point
out,
the
first
instance
of
a
conjunction!
.
The
system
replies
by
pointing
to
a
segment.
Thus,
looking
at
the
whole
thing
as
a
dialog,
we
have
something
along
the
lines
of
122
.
122
U:
Please
find,
and
point
out,
a
conjunction
in
this
text
S
:
Ok,
here
s
one
pointing
to
a
segment
As
argued
in
Chapter
3,
looking
at
particular
user
commands
and
system
responses
as
turns
in
an
ongoing
conversation
may
help
to
clarify
their
connection
to
other
user
commands
and
system
responses.
7.2.5
Searching
and
prediction
The
notion
of
prediction
relates
to
searching
in
the
sense
that
the
consequent
of
a
universally
quantified
conditional
can
be
said
to
predict
a
property
of
what
we
are
going
to
find
if
we
search
for
what
is
specified
by
the
antecedent.
As
an
example,
consider
the
statement
in
123
again:
123
syncat
P0-P2,np
:-
syncat
P0-P1,det
,
syncat
P1-P2,n
.
In
Chapter
4,
the
meaning
of
this
statement
was
said
to
be:
a
determiner
followed
by
a
noun
is
always
a
noun
phrase
.
What
the
statement
predicts
can
be
paraphrased
as
follows:
the
next
time
you
find
a
determiner
followed
by
a
noun,
the
two
together
will
form
a
noun
phrase.
In
Chapter
12,
we
will
see
how
the
TagLog
Hypothesis
Testing
Tool
allows
us
to
check
a
prediction
made
by
a
universal
statement,
by
exploiting
the
fact
that
the
consequent
of
such
a
statement
amounts
to
a
specification
of
a
segment
to
search
for,
select
and
thus
present
to
the
user.
7.2.6
Precision
and
recall
When
searching
for
segments
matching
a
particular
description,
by
means
of
a
grammar
for
example,
the
system
often
finds
too
much,
and
or
too
little.
That
is,
it
finds
segments
not
in
the
target
set
of
segments,
and
or
it
fails
to
find
segments
that
are
in
the
target
set.
These
problems
are
usually
The
TagLog
Search
Tool
131
referred
to
as
lack
of
precision,
and
lack
of
recall,
respectively
cf.
Salton,
1989
.
The
exact
definition
of
these
measures
is
not
crucial
in
this
thesis
since
no
actual
evaluation
of
searching
is
performed
,
but
we
can
think
of
precision
as
the
ratio
between
the
number
of
target
segments
that
were
found
and
the
total
number
of
segments
that
were
found,
and
by
recall
is
meant
the
ratio
between
the
number
of
target
segments
that
were
found
and
the
total
number
of
target
segments.
Thus,
a
precision
of
less
than
100
means
that
the
system
found
something
that
is
not
regarded
as
a
correct
result,
whereas
a
recall
of
less
than
100
means
that
the
system
missed
some
of
the
desired
segments.
We
will
return
to
these
notions
again
in
Chapter
14,
as
part
of
the
discussion
of
certain
errors
that
some
TagLog
tools
seem
to
make,
but
which
will
turn
out
not
to
be
errors
for
which
the
tools
can
be
blamed.
7.3
The
TagLog
Search
Tool
The
TagLog
Search
Tool
is
a
tool
which
combines
the
use
of
the
provable
1
and
select
1
predicates
as
presented
in
Section
7.2.3.
The
tool
makes
it
easier
to
enter
and
execute
a
search
specification,
and
provides
dynamic
links
to
both
the
Statement
Browser
and
the
TagLog
Explanation
Tool.
7.3.1
Search
Tool
layout
The
Search
Tool
dialog
box,
depicted
in
Figure
25,
consists
of
one
text
entry
field
and
four
buttons.
FIGURE
25.
The
TagLog
Search
Tool
Initially,
the
only
button
enabled
is:
132
Exploring
Linguistic
Reality
First:
Find
the
first
element
in
the
set
specified
by
the
set-expression
in
the
text
entry
field
and
select
it
in
the
Text
Window.
But
besides
finding
a
matching
segment,
clicking
this
button
enables
three
other
buttons:
Next:
Find
the
next
instance.
Subsequent
clicks
on
this
button
will
find
the
rest
of
the
specified
segments,
one
at
a
time.
Reset:
Return
to
top
level.
Explain:
Activate
the
link
to
the
TagLog
Explanation
Tool
see
Section
7.3.4
.
7.3.2
How
to
use
the
Search
Tool
To
search
for
a
particular
set
of
segments
in
the
current
text,
proceed
as
follows:
1.
Select
the
Search
command
from
the
tool
menu.
2.
Enter
the
specification
S
Description
in
the
Search
Tool
s
text
entry
field,
where
Description
describes
each
of
the
target
segment
S.
Section
7.3.3
defines
a
simplified
notation
that
may
be
used
instead
.
3.
Click
the
Find
button.
The
first
matching
segment
will
then
be
selected
and
highlighted
in
the
text.
In
order
to
see
the
next
matching
segment:
4.
Click
the
Next
button.
To
exit
the
search,
e.g.
in
order
to
enter
a
different
search
expression:
5.
Click
the
Reset
button.
7.3.3
A
simplified
notation
The
current
version
of
the
TagLog
system
implements
a
shorthand
notation
for
a
certain
class
of
target
set
of
segments.
For
example,
instead
of
the
set-expression
124
P0-P2
syncat
P0-P2,pn
;
bstring
P0-P1,
the
,
syncat
P2-P2,n
to
be
read
the
set
of
segments
that
are
either
proper
nouns
or
consists
of
the
followed
by
a
noun
,
we
may
write
125
syncat
pn
;
bstring
the
,
syncat
n
Exemplifying
Searching
133
Moreover,
when
values
are
unique
to
one
property,
i.e.
when
there
are
no
other
properties
for
which
these
values
are
possible,
then
we
may
write
the
specification
as
the
even
shorter:
126
pn
;
the
,
n
This
is
an
example
of
how
we
might
define
a
simple
query
notation
on
top
of
the
TagLog
formalism.
On
the
other
hand,
it
is
clear
that
there
are
set-expressions
for
which
this
particular
notation
cannot
provide
us
with
equivalent
shorthand
expressions.
7.3.4
Linking
to
other
tools
There
is
a
link
from
the
Search
Tool
to
the
Statement
Browser
such
that
if
the
Statement
Browser
is
open,
it
will
automatically
display
the
statements
describing
the
segment
selected
by
the
Search
Tool.
For
more
information
on
the
Statement
Browser,
see
Chapter
8.
Clicking
the
Explain
button
activates
the
link
to
the
TagLog
Explanation
Tool,
i.e.
if
not
already
open,
it
opens
this
tool,
and
it
produces
an
explanation,
in
the
mode
determined
by
the
settings
of
the
relevant
controls
in
the
Explanations
Tool
s
dialog
box,
of
why
the
system
considers
the
segment
selected
by
the
Search
Tool
to
be
an
instance
of
the
kind
of
segment
searched
for.
See
Chapter
13
for
a
more
detailed
description
of
the
Explanation
Tool.
7.4
Exemplifying
Searching
7.4.1
Searching
for
instances
of
a
basic
string
Suppose
that
we
want
to
search
for
an
instance
of
the
basic
string
the
in
the
current
text.
We
select
the
Search
command
from
the
Tool
menu,
enter
the
specification
S
bstring
S,
the
in
the
Search
Tool
s
text
entry
field,
and
click
the
Find
button.
The
first
instance
of
the
if
such
an
instance
exists
will
then
be
selected
and
highlighted
in
the
text.
In
order
to
see
the
next
instance,
we
click
the
Next
button,
until
we
have
seen
enough,
and
then
we
click
Reset.
134
Exploring
Linguistic
Reality
It
is
true
that
this
task
could
be
performed
by
any
word
processor
program,
probably
in
a
faster
and
simpler
way.
The
generality
and
flexibility
of
the
TagLog
approach
does
not
show
until
we
try
to
find
instances
of
properties
other
than
strings.
7.4.2
Searching
for
adjacency-pair
segments
In
order
to
find
instances
of
adjacency
pairs,
as
given
by
the
theory
in
Section
4.9,
we
only
have
to
change
the
specification,
the
other
steps
are
to
be
done
exactly
as
before.
So
the
search
specification
will
now
have
to
be
S
d
syn
S,
adj
pair
.
By
clicking
the
First
button,
the
result
we
get
display
as
in
Figure
26.
FIGURE
26.
Finding
an
adjacency
pair
7.4.3
Searching
for
errors
and
incompleteness
Suppose
we
have
defined
a
couple
of
clauses
like
the
following:
127
inconsistent
S
:-
syncat
S,C1
,
syncat
S,C2
,
C1
C2.
inconsistent
S
:-
bsegment
S
,
unprovable
syncat
S,
.
Exemplifying
Searching
135
Then
the
following
set-expression
would
allow
us
to
search
systematically
for
ambiguities
or
incompleteness.
128
S
inconsistent
S
The
descriptions
for
the
segments
that
are
found
are
displayed
in
the
Statement
Browser
if
it
is
open
where
we
may
try
to
correct
or
complete
them.
7.4.4
Tracing
the
Text-Content
Context
relation
Many
TagLog
tools
can
be
used
to
analyse
the
relation,
in
both
directions,
between
text
and
content
context.
I
will
exemplify
this
here
with
the
use
of
the
Search
Tool.
Suppose
we
have,
as
part
of
the
current
theory,
the
integrated
grammar
presented
in
Section
4.8.
Suppose
we
select
a
segment
in
the
Text
window,
and
that
we
enter
the
set-expression
in
129
in
the
Search
Tool
s
text
entry
field.
129
X
selection
S
,
syncat
S,
:X
This
expression
should
be
read
as
follows:
the
set
of
entities
e.g.
objects,
events,
states
X
such
that
X
is
the
interpretation
of
the
segment
S
selected
in
the
current
text.
With
a
click
on
the
Find
button,
the
Search
Tool
is
able
to
find
and
highlight,
in
the
Situation
Theory
window,
each
occurrence
of
a
symbol
denoting
the
entity
that
is
singled
out
as
the
interpretation
of
the
segment
selected
in
the
Text
window.
This
is
illustrated
in
Figure
27,
where
the
first
token
of
x18
has
been
selected.
Subsequent
clicks
on
the
Next
button
will
highlight
the
other
occurrences
of
x18.
FIGURE
27.
Tracing
the
text-content
context
relation
Suppose,
on
the
other
hand,
during
inspection
of
the
statements
in
the
Situation
Theory
window
we
observe
a
name
of
an
entity
x17,
say
,
for
which
we
would
like
to
know
which
segments
if
any
in
the
text
that
are
connected
to
it
by
a
relation
of
interpretation.
We
enter
the
set-expression
136
Exploring
Linguistic
Reality
in
130
in
the
appropriate
field
in
the
Search
Tool,
and
click
the
First
button.
130
S
syncat
S,
:x17
The
segment
corresponding
to
the
string
The
cat
will
become
highlighted.
Thus,
both
directions
are
accounted
for.
7.5
Text
in
Co-text
Concordancing
In
TagLog,
searching
is
the
basis
for
concordancing.
In
fact,
a
concordance
can
be
described
as
a
way
of
presenting
the
result
of
an
exhaustive
search.
7.5.1
The
logic
of
concordancing
A
concordance
is
declaratively
specified
by
a
set-expression
denoting
a
set
of
segments,
and
is
further
determined
by
the
setting
of
parameters
that
specify
sorting
order
and
where
to
print
the
concordance.
This
approach
to
concordancing
is
much
more
powerful
than
traditional
approaches:
set-expressions
may
involve
logical
connectives
like
conjunction,
disjunction
and
negation;
subformulas
in
the
set-expressions
may
refer
to
any
value
for
any
property
i.e.
not
only
text
strings,
but
also
lexical
categories,
syntactic
or
functional
categories,
etc.
;
the
statements
expressed
by
the
subformulas
do
not
have
to
be
explicitly
given
in
the
tagging
process,
but
may
be
defined
by
universal
statements
given
in
a
general
theory
e.g.
a
grammar
.
7.6
The
Concordance
Tool
The
TagLog
Concordance
Tool,
like
other
TagLog
system
tools,
supports
declarative
specifications,
is
very
general,
and
is
dynamically
linked
to
some
of
the
other
tools
and
presentation
areas
in
the
system.
A
declarative
specification
of
a
concordance
is
given
by
writing
a
setexpression
denoting
a
set
of
segments
in
the
Concordance
Tool
s
text
entry
field.
A
TagLog
concordance
can
be
sorted
in
alphabetical
order,
or
The
Concordance
Tool
137
in
order
of
appearance,
and
can
be
written
to
a
Live
window,
the
Dialogue
window,
to
the
Macintosh
clipboard,
or
to
a
file.
These
choices
are
made
by
setting
the
values
of
pop-up
menus.
7.6.1
Concordance
Tool
layout
Selecting
Concordance
from
the
Tool
menu
brings
up
the
dialog
box
in
Figure
28,
containing
one
text
entry
field,
two
pop-up
menus,
and
one
button.
FIGURE
28.
The
Concordance
Tool
dialog
box
The
set-expression
denoting
the
target
segments
is
entered
in
the
text
entry
field.
The
controls
are
as
follows:
Sorting
order:
This
pop-up
menu
has
two
possible
values:
Alphabetical:
The
concordance
is
sorted
in
alphabetical
order.
This
is
the
default
value.
Appearance:
The
concordance
is
sorted
in
the
order
that
the
segments
appear
in
the
text.
Output
to:
This
pop-up
menu
has
four
possible
values:
Live
window:
The
concordance
is
presented
in
the
Live
window.
This
is
the
default
value.
Dialogue:
The
concordance
is
printed
in
the
Dialogue
window.
Clipboard:
The
concordance
is
sent
to
the
clipboard,
from
which
it
can
be
pasted
into
other
applications,
such
as
a
word
processor.
File:
The
concordance
is
printed
to
a
file
specified
by
the
user.
Build:
This
button
will
start
building
the
concordance
specified
in
the
text
entry
field,
sorted
according
to
the
setting
of
the
Sorting
Order
menu,
138
Exploring
Linguistic
Reality
and
printed
to
the
target
specified
by
the
current
value
of
the
Output
To
menu.
7.6.2
How
to
use
the
Concordance
Tool
In
order
to
build
a
concordance
of
segments
in
the
current
text
that
satisfy
Description,
proceed
as
follows:
1.
Select
the
Concordance
command
from
the
Tool
menu.
2.
Enter
the
specification
S
Description
in
the
Concordance
Tool
s
text
entry
field.
The
simplified
notation
defined
in
Section
7.3.3
may
be
used
here
instead
.
3.
Make
sure
that
the
pop-up
menus
of
sorting
and
output
options
are
set
according
to
your
requirements.
4.
Click
the
Build
button.
7.6.3
Linking
with
other
tools
The
Concordance
window
is
dynamically
linked
to
the
Text
window
and
the
Statement
Browser.
By
clicking
a
particular
line
in
the
Concordance
window
the
user
can
locate
the
corresponding
segment
in
the
main
text,
thus
seeing
a
larger
co-text
than
is
possible
in
the
concordance,
which
is
limited
to
one
line
per
target
segment.
The
segment
selected
in
this
way
can
be
further
described
in
the
usual
way.
As
well
as
this,
the
statements
describing
the
segment
thus
selected
are
displayed
in
the
Statement
Browser.
7.7
Concordancing
Examples
7.7.1
A
concordance
of
word
form
tokens
In
order
to
build
a
concordance
of
the
word
the
with
respect
to
the
current
text,
and
present
it
in
the
Live
window,
we
select
the
Concordance
command
from
the
Tool
menu,
enter
the
specification
S
bstring
S,
the
in
the
text
entry
field,
and
click
the
Build
button.
The
result
of
these
steps
is
depicted
in
Figure
29.
Concordancing
Examples
139
FIGURE
29.
A
concordance
of
the
Again,
the
task
is
trivial
and
could
be
performed
by
any
concordance
program;
the
TagLog
approach
does
not
reveal
its
power
until
we
try
more
difficult
things.
7.7.2
Concordancing
based
on
non-string
properties
Given
a
hand
coded
or
automatically
obtained
description
of
the
segments
of
a
text,
a
concordance
can
be
specified
that
picks
out
some
of
the
segments.
Figure
30
exemplifies
this.
Note
the
use
of
the
logical
connective
disjunction
in
the
specification
as
well,
and
note
that
the
sorting
order
now
has
been
set
to
Alphabetical.
FIGURE
30.
A
concordance
of
nouns
140
Exploring
Linguistic
Reality
7.7.3
Using
a
theory
to
build
a
concordance
A
noun
phrase
grammar
can
be
used
to
build
a
concordance
of
noun
phrases.
We
select
the
Concordance
command
from
the
Tool
menu,
and
enter
the
specification
S
syncat
S,np
in
the
Concordance
Tool
s
text
entry
field.
We
set
the
menu
of
output
options
to
Live
Window,
and
the
Sorting
order
menu
to
Alphabetical.
Finally,
we
click
the
Build
button.
Note
that
this
was
done
in
exactly
the
same
way
as
with
strings
the
ordinary
type
of
concordance
and
lexical
categories,
except
that
the
specification
was
different.
In
this
way,
we
would
also
be
able
to
produce
a
concordance
of
adjacency
pairs,
as
defined
by
the
dialog
grammar
given
in
Section
4.9,
a
concordance
of
segments
functioning
as
questions,
as
defined
in
Section
10.5.1,
etc.
7.7.4
The
Concordance
Tool
Statement
Browser
link
As
we
learned
in
Section
7.3.4,
the
Concordance
Tool
and
the
Statement
Browser
can
be
dynamically
linked
via
the
Live
Window.
The
link
is
established
automatically
as
soon
as
the
Concordance
Tool
s
Live
window
and
the
Statement
Browser
are
open
simultaneously.
For
instance,
when
the
Statement
Browser
is
open
in
the
context
of
the
example
in
Section
7.7.2,
we
get
the
display
depicted
in
Figure
31.
FIGURE
31.
The
Concordance
Tool
Statement
Browser
link
Concordancing
for
Hand
Coding
141
Selection
of
a
row
in
the
Live
Window
concordance
by
clicking
on
it
with
the
mouse,
or
by
using
the
Up-
and
Down
arrow
keys
will
automatically
update
the
Statement
Browser
so
that
the
statements
describing
the
segment
corresponding
to
this
row
are
displayed.
7.7.5
Concordances
and
stop
lists
In
order
to
reduce
the
size
of
concordances
produced,
it
is
not
unusual
for
a
traditional
concordance
program
to
be
fed
a
so
called
stop
list,
i.e.
a
list
of
words
or
categories
of
words
which
the
program
is
instructed
not
to
include
in
the
concordance.
This
works,
of
course,
but
the
intended
effect
is
also
easy
to
obtain,
in
a
more
declarative
and
high-level
way,
by
using
negation
in
the
specification
of
the
concordance.
For
example,
the
specification
in
131
produces
a
concordance
of
all
the
basic
strings
in
the
current
text,
except
for
all
strings
classified
as
form
words.
131
S
bstring
S,W
,
not
form
word
W
More
correct
but
useful
only
if
the
text
has
been
tagged
with
syntactic
categories
the
specification
in
132
uses
the
taxonomy
given
in
Chapter
4
to
exclude
all
closed
class
words
from
a
concordance.
132
S
syncat
S,open
Thus,
we
see
that
at
least
some
of
the
extra
machinery
and
input
requirements
that
is
usually
built
into
a
concordance
program
is
not
needed
if
the
language
for
specifying
the
target
concordances
is
expressive
enough.
7.8
Concordancing
for
Hand
Coding
Sometimes
we
wish
to
describe
a
subset
of
some
easily
circumscribed
set
of
segments
in
a
particular
way:
instances
of
a
particular
string,
for
example,
or
instances
of
a
particular
syntactic
category.
It
may
then
be
more
expedient
to
do
this
coding
in
a
concordance
corresponding
to
this
set,
instead
of
directly
in
the
Text
window.
The
result,
in
terms
of
updates
to
theories,
is
exactly
the
same,
but
the
concordance
provides
a
very
good
overview
of
the
potential
targets
for
coding,
and
thus
increases
the
chance
of
assigning
the
codes
in
a
well-informed
and
consistent
way.
Moreover,
due
to
the
dynamic
link
between
the
Concordance
Tool
and
the
Statement
Browser,
the
latter
tool
can
be
used
to
monitor
the
coding
process.
142
Exploring
Linguistic
Reality
In
Figure
32,
the
user
has
entered
a
set-expression
in
the
Concordance
Tool
s
text
entry
field,
and
has
clicked
the
Build
button.
A
concordance
has
been
built,
and
the
user
has
selected
a
line
in
it,
resulting
in
the
selection
of
the
corresponding
segment
in
the
Text
window
and
the
display
of
the
statements
describing
this
segment
in
the
Statement
Browser.
Next,
the
user
is
about
to
select
a
tag
from
the
Tag
menu,
in
order
to
elaborate
the
description
of
the
current
segment,
perhaps
by
saying
something
about
its
function.
FIGURE
32.
Concordancing
for
hand
coding
7.9
Concordancing
for
Grammar
Development
The
set-expression
in
133
denotes
the
set
of
segments
in
the
current
text
that
are
instances
of
the
syntactical
category
np.
133
S
syncat
S,np
By
building
a
concordance
on
the
basis
of
this
specification,
the
user
of
the
TagLog
system
can
easily
observe
the
implications
of
the
grammar
rules
responsible
for
the
description
of
noun
phrases.
If
some
of
the
Concordancing
for
Grammar
Development
143
strings
in
the
concordance
are
observed
not
to
be
noun
phrases,
he
may
have
to
consider
changing
the
grammar.
But
the
user
may
also
observe
the
impact
of
a
grammar
rule
without
actually
adding
it
to
the
grammar
first.
The
specification
in
134
generates
a
concordance
listing
of
all
the
instances
of
a
preposition
followed
by
a
noun
phrase
in
the
current
text.
134
P0-Pn
syncat
P0-Pi,prep
,
syncat
Pi-Pn,np
So
here,
if
the
grammar
writer
is
not
happy
with
what
he
sees
in
the
corresponding
concordance,
he
may
decide
that
the
addition
of
the
rule
pp
--
prep,
np
to
the
grammar
is
not
such
a
great
idea
after
all.
Sometimes
we
want
to
be
able
to
distinguish
statements
that
have
been
added
explicitly,
on
the
basis
of
observation
during
a
manual
tagging
process,
from
statements
that
follow
deductively
from
a
theory
by
means
of
one
or
more
applications
of
rules.
Fortunately,
TagLog
contains
builtin
predicates:
observed
1,
unobserved
1,
predicted
1,
and
unpredicted
1,
that
will
let
us
do
exactly
that.
In
135
these
predicates
are
used
to
build
a
concordance
of
all
segments
that
are
described
manually
as
noun
phrases,
but
where
the
grammar
fails
to
recognize
that
they
are
noun
phrases.
135
S
observed
syncat
S,np
,
unpredicted
syncat
S,np
Another
group
of
TagLog
built-in
predicates
that
are
convenient
for
the
corpus
grammarian
to
know
about
is
the
tree
predicates.
Here
I
just
exemplify
the
use
of
the
predicates
parse
tree
2,
node
2
and
parent
3,
and
I
will
have
to
refer
the
reader
to
Appendix
A
for
a
closer
presentation
of
these
and
other
predicates
in
this
family.
Suppose
we
want
to
build
a
concordance
of
all
the
verb
phrases
that
contain
at
least
one
noun
phrase.
This
set
is
specified
as
in
136
.1
136
S
parse
tree
syncat
S,vp
,Tree
,
node
Tree,np
Example
137
finally,
specifies
the
set
of
segments
S
such
that
S
is
a
phrase
according
to
a
particular
grammar,
and
this
phrase
contains
a
verb
phrase
which
stands
in
the
parent
relation
to
a
noun
phrase.
137
S
parse
tree
syncat
S,
,Tree
,
parent
Tree,np,vp
1.
An
alternative
formulation,
avoiding
reference
to
a
tree:
S
syncat
S,vp
,
subsegment
S,S1
,
syncat
S1,np
144
Exploring
Linguistic
Reality
7.10
The
Treebank
Approach
Before
closing
this
chapter,
let
me
say
a
few
words
on
the
subject
of
treebanks.
A
treebank
is
a
corpus
that
has
been
annotated
for
skeletal
syntactic
structure.
Examples
of
treebanks
include
the
Penn
Treebank,
mentioned
in
Chapter
1.
A
treebank
can
be
implemented
in
TagLog
simply
by
tagging
all
segments
in
a
text
corresponding
to
sentences
with
trees,
i.e.
by
using
predications
of
the
form
tree
Tree
,
where
Tree
encodes
the
syntactic
structure
of
the
sentence.2
An
example
is
given
in
138
.3
138
tree
syncat
3-7,s
syncat
3-4,np
syncat
3-4,pn
true,
syncat
4-7,vp
syncat
4-5,v
true,
syncat
5-7,np
syncat
5-6,det
true,
syncat
6-7,n
true
.
To
find,
to
count,
or
to
make
a
concordance
of
say
all
the
noun
phrase
segments
in
such
a
treebank
we
may
specify
the
target
set
of
segments
as
follows:4
139
S
tree
Tree
,
node
Tree,syncat
S,np
That
is,
what
we
are
looking
for
here
is
the
set
of
segments
S
such
that
S
is
a
segment
corresponding
to
a
np
node
in
a
tree
in
the
treebank.
Here,
node
2
is
a
built-in
predicate
in
TagLog,
defined
as
follows:
140
node
Node
Subtree,Node
.
node
Node
Subtrees,Node1
:conj
member
Subtree,Subtrees
,
node
Subtree,Node1
.
One
advantage
with
the
treebank
approach
is
that
it
does
not
suffer
from
the
imprecision
problem
mention
in
Section
7.2.6;
provided
the
descrip2.
3.
4.
But
note
that
proof
trees
rather
than
parse
trees
are
used
here.
I
am
certainly
not
claiming
that
this
is
the
most
economical
way
to
encode
the
tree.
Thus,
this
simple
TagLog
query
could
be
compared
with
a
tool
for
pattern
matching
against
a
skeletally
parsed
corpus
such
as
the
Penn
treebank
tgrep.
Summary
and
Conclusion
145
tion
of
the
tree
structures
corresponding
to
segments
is
true,
then
what
we
find
will
always
be
what
we
were
looking
for.
Now
the
pre-processing
part
of
building
a
treebank
is
not
something
that
we
would
like
to
do
manually.
The
treebanks
referred
to
above
have
been
constructed
by
means
of
an
automatic
parser,
followed
by
a
phase
of
manual
disambiguation.
In
TagLog,
a
treebank
can
be
obtained
by
caching
the
results
of
proof
tree
2,
and
then
manually
removing
all
trees
except
the
correct
ones.
7.11
Summary
and
Conclusion
This
chapter
was
concerned
with
two
important
aspects
of
the
exploration
of
linguistic
reality,
namely
navigation
and
presentation.
It
was
also
shown
how
navigation
and
presentation
in
this
sense
is
linked
to
observation
and
description
in
the
sense
of
Chapter
6.
It
was
shown
that
the
TagLog
system,
through
an
implementation
of
the
notion
of
searching
as
deduction,
and
by
means
of
a
built-in
predicate
that
implements
a
kind
of
deixis,
is
able
to
find
segments
satisfying
partial
descriptions
provided
by
the
user
and
to
point
them
out
to
him.
The
search
functionality
of
TagLog
is
very
general
and
powerful.
In
principle
every
segment
that
can
be
described
by
means
of
a
TagLog
expression
can
also
be
searched
for.
Also,
it
should
have
become
clear
from
this
chapter
that
one
of
the
traditional
tools
of
the
corpus
linguist
the
concordancer
can
be
designed
on
principles
that
are
compatible
with
a
logical
approach
to
corpus
linguistics,
and
that
the
result
is
a
tool
more
powerful
than
traditional
concordance
programs.
In
the
case
of
hand
coding,
syntax
studies,
conceptual
modelling
work,
etc.
such
a
concordance
tool
is
indeed
a
useful
thing
to
have.
CHAPTER
8
Browsing
Theories
8.1
Introduction
As
was
explained
in
Chapter
2,
a
rather
sharp
division
between
theory
and
data
is
assumed
in
this
thesis.
With
respect
to
this
division,
there
are
basically
two
ways
to
set
the
focus
when
using
the
TagLog
system.
Either
and
this
was
the
theme
of
Chapter
7
we
start
with
a
possibly
partial
description
and
ask
for
a
matching
piece
of
linguistic
reality:
find
this
segment,
take
me
there!
.
Or
else
and
this
will
be
the
theme
of
this
chapter
our
point
of
departure
is
a
particular
segment
of
text:
what
does
the
current
theory
predict
about
this
segment?
.
These
questions
can
be
combined,
of
course:
given
this
partial
description,
for
each
of
the
matching
segments,
how
is
it
described
by
the
current
theory?
.
The
descriptions
referred
to
above
are
descriptions
of
local
properties
of
text
segments.
But
TagLog
also
deals
with
descriptions
of
global
properties,
i.e.
properties
of
a
whole
text,
or
a
whole
corpus.
Typically,
the
latter
descriptions
are
of
a
quantitative
nature.
The
topic
of
quantitative
descrip-
Retrieval
by
Deduction
147
tion
is
sufficiently
different,
however,
to
warrant
a
chapter
of
its
own.
Thus,
it
will
be
the
topic
of
Chapter
9.
8.2
Retrieval
by
Deduction
To
ask
the
TagLog
system
to
store
a
sentence
describing
a
particular
segment,
as
explained
in
Chapter
6,
makes
sense
only
if
the
system
is
able
to
retrieve
that
description
again
on
a
later
occasion,
when
the
user
asks
it
to
do
so.
To
retrieve
the
value
for
a
certain
attribute
for
a
particular
segment
in
a
traditional
tagging
system
assuming
a
tier-based
mark-up
language
,
we
need
only
to
look
below
the
segment
in
question,
on
the
tier
corresponding
to
this
attribute.
Thus,
for
example,
we
can
easily
determine
that
the
lexical
category
of
the
instance
of
the
string
Mary
in
Figure
33
is
pn,
by
just
looking
immediately
below
that
segment.
...
John
and
Mary
...
lexcat:
...
pn
conj
pn
...
FIGURE
33.
Traditional
retrieval
of
tags
In
the
logical
approach
to
corpus
linguistics,
not
only
searching,
but
also
retrieval
in
the
above
sense,
is
performed
by
deduction.
But
as
was
the
case
with
searching
finding
out
whether
a
certain
existentially
quantified
first
order
logic
formula,
such
as
the
formula
in
141
,
is
provable
or
not
is
not
enough:
141
x
syncat
2-3,x
-
Rather
than
a
yes
or
a
no,
we
need
the
values
of
the
variables;
thus,
we
need
to
use
a
constructive
prover
here
too.
In
142
the
prover
from
Chapter
3
is
used
in
order
to
prove
an
existentially
quantified
sentence.
It
succeeds,
and
thus
binds
the
variable
bound
by
the
quantifier
to
the
value
of
the
property
syncat
for
the
segment
2-3.
142
!-
provable
syncat
2-3,X
.
X
conj
148
Browsing
Theories
So
we
see
that
two
seemingly
very
different
operations
searching
for
segments
satisfying
particular
descriptions,
and
retrieval
of
the
descriptions
of
particular
segments
are
supported
by
the
same
procedures,
using
the
same
declarative
theory.
To
make
retrieval
of
the
value
of
a
certain
property
of
a
particular
segment
of
text
practical,
it
is
of
some
import
that
the
segment
in
questions
can
simply
be
pointed
to,
rather
than
referenced
by
name
.
As
we
have
already
seen,
TagLog
provides
a
built-in
predicate
for
this
purpose:
selection
1
binds
a
variable
to
the
segment
expression
denoting
the
currently
selected
segment,
and
that
expression
can
be
used
in
a
succeeding
call,
as
follows:
143
!-
selection
S
,
provable
syncat
S,X
.
X
conj
8.3
The
TagLog
Statement
Browser
The
Statement
Browser
is
designed
to
make
it
easy
to
inspect
the
current
description
of
a
particular
segment
of
a
text.
The
Statement
Browser
allows
the
user
to
select
a
segment
in
the
text,
and
by
clicking
a
button,
or
pressing
a
key,
get
a
full
description
of
this
segment.
FIGURE
34.
The
TagLog
Statement
Browser
The
Statement
Browser
can
also
be
used
for
monitoring
updates
of
the
current
theory
i.e.
additions
and
deletions
of
sentences
;
it
can
be
used
for
deleting
statements,
negating
statements,
demanding
explanations
of
statements,
and
so
on.
The
TagLog
Statement
Browser
149
The
Statement
Browser
is
dynamically
linked
with
the
Search
Tool
and
the
Concordance
Tool,
and
can
thus
be
used
to
inspect
descriptions
of
segments
found
by
means
of
these
tools.
The
Statement
Browser
contains
one
display
area,
three
checkboxes,
and
four
buttons.
The
display
area
displays
the
sentences
that
describe
the
currently
selected
segment,
in
the
mode
determined
by
the
settings
of
the
checkboxes
the
meaning
of
which
are
given
as
follows:
Observed:
Select
this
checkbox
to
display
the
observation
statements
that
describe
the
currently
selected
segment.
Predicted:
Select
this
checkbox
to
display
derived
statements
about
the
currently
selected
segment.
Internal:
Select
this
checkbox
to
display
the
statements
describing
every
subsegment
of
the
currently
selected
segment.
The
following
buttons
perform
various
operations
on
the
sentences
in
the
display
area:
Refresh:
Clicking
this
button
updates
the
information
in
the
display
area,
for
the
segment
currently
selected.1
Remove:
To
remove
the
statement
selected
in
the
display
area
from
the
current
theory,
click
this
button
or
push
the
Back
Space
key
.
The
sentence
becomes
unknown
in
a
sense
to
be
explained
in
Chapter
10.
This
operation
is
currently
possible
only
on
explicitly
represented
sentences.
Contradict:
To
add
a
sentence
which
is
the
explicit
negation
of
the
selected
sentence
to
the
current
theory,
click
this
button.
Hence,
if
the
selected
sentence
has
the
non-negated
form
A,
a
sentence
A
is
added
to
the
current
theory.
If
the
selection
is
A,
the
sentence
A
is
added.
In
many
cases,
an
inconsistency
will
arise,
although
not
always,
as
will
be
seen
in
Chapter
14:
the
effect
might
also
be
to
remove
the
contradicted
sentence
from
the
current
theory
in
the
sense
that
it
will
no
longer
be
derivable
.
1.
A
short-cut:
Pressing
the
Return
key
when
the
Text
window
is
active
has
the
same
effect.
So
by
selecting
a
segment,
pressing
the
Return
key,
selecting
a
new
segment,
pressing
the
Return
key,
and
so
on.
the
descriptions
of
many
segments
in
succession
can
be
browsed,
with
no
need
to
deactivate
the
Text
window
in
between.
150
Browsing
Theories
Explain:
Clicking
this
button
opens
the
Explanation
Tool
with
a
conjunction
of
the
sentences
selected
in
the
display
area,
or,
if
the
Explanation
Tool
is
already
open,
produces
an
explanation
in
the
currently
selected
mode.
8.3.1
Inspecting
the
statements
describing
a
segment
As
a
simple
example
of
the
use
of
the
Statement
Browser,
consider
the
text
fragment:
144
a
complex
Suppose
that
we
have
described
this
text
fragment
as
follows,
145
syncat
22-23,det
.
syncat
23-24,n
sg
.
and
that
we
later
on
want
to
recapitulate
what
we
have
said
about
it.
In
order
to
inspect
the
statements
describing
the
segment
corresponding
to
complex
we
select
the
segment
and
click
the
Refresh
button.
The
Statement
Browser
will
now
look
as
in
Figure
35.
FIGURE
35.
The
Statement
Browser
in
action
8.4
Parsing
as
Deduction
The
idea
of
parsing
as
deduction
was
originated
by
Kowalski
1979
,
and
Pereira
and
Warren
1983
.
Parsing
as
deduction
has
in
fact
already
been
Parsing
as
Deduction
151
exemplified
in
Chapter
4,
where
the
corresponding
notion
of
grammar
as
theory
was
also
explained.
The
current
section
will
demonstrate
how
parsing
as
deduction
by
means
of
the
Statement
Browser
is
performed.
In
the
TagLog
framework,
parsing
becomes
a
special
case
of
retrieval
of
a
description
of
a
segment
of
text.
Parsing
a
particular
text
segment
is
simply
a
question
of
asking
the
system
what
the
current
grammar
theory
predicts
of
the
segment.
The
Statement
Browser
is
the
right
tool
to
use
for
this
purpose:
make
sure
the
Predicted
checkbox
is
marked,
select
a
segment
to
parse
in
the
Text
window,
and
click
the
Refresh
button
or
push
the
Return
key
.
The
result
is
shown
in
the
display
area.
To
continue
the
above
example,
given
that
we
have
written
a
grammar
which
includes
the
statement
146
syncat
P0-P2,np
Num
:-
syncat
P0-P1,det
,
syncat
P1-P2,n
Num
.
the
browser
will
display
as
in
Figure
36,
if
the
Predicted
and
Internal
checkboxes
are
selected.
FIGURE
36.
Parsing
as
deduction
with
the
Statement
Browser
Note
that
if
we
for
some
reason
decide
to
delete
the
statement
syncat
2324,n
sg
from
the
current
theory,
for
example
by
placing
the
insertion
point
on
it
and
pressing
the
Backspace
key,
the
statement
syncat
2224,np
sg
will
disappear
as
well,
since
it
is
logically
dependent
on
the
former
statement.
152
Browsing
Theories
Let
me
also,
by
way
of
another
example,
introduce
one
of
the
greatest
problem
for
parsing,
namely
ambiguity.
Suppose
we
have
a
text
that
contains
a
segment
instantiating
the
string
in
147
,
and
that
we
have
selected
that
segment.
147
time
flies
Assume
also
we
have
the
theory
in
148
.
148
s
--
np,
vp.
np
--
n.
np
--
n,
n.
vp
--
v.
n
--
time
.
n
--
flies
.
v
--
flies
.
The
Statement
Browser
will
display
as
in
Figure
37.
FIGURE
37.
Ambiguity
in
the
eyes
of
the
Statement
Browser
Thus,
there
seem
to
be
two
statements
in
the
current
theory:
syncat
152154,s
and
syncat
152-154,np
,
that
describe
the
segment
152-154
in
mutually
incompatible
ways.
This
is
ambiguity.
To
distinguish
this
kind
of
ambiguity
from
structural
ambiguity,
I
will
use
the
term
categorial
ambiguity.
Whereas
the
term
structural
ambiguity
refers
to
the
case
of
a
particular
segment
having
more
than
one
analysis
tree,
categorial
ambiguity
is
the
case
of
one
segment
being
described
as
belonging
to
at
least
two
different
and
incompatible
grammatical
categories.2
2.
It
seems
to
me
that
the
distinction
between
categorial
and
structural
ambiguity
is
blurred
by
the
fact
that
if
parse
trees
are
treated
as
categories,
every
structural
ambiguity
is
turned
into
a
categorial
ambiguity,
and
by
the
fact
that,
strictly
speaking,
every
categorial
ambiguity
is
also
a
structural
ambiguity,
albeit
a
very
shallow
one.
Still,
I
think
the
distinction
is
useful.
Alternative
Theory
Presentation
Modes
153
Presumably,
one
of
the
predictions
made
above
must
be
false,
and
which
one
should
be
easy
to
determine
given
the
context.
As
will
be
seen
later,
it
is
in
line
with
the
TagLog
approach
to
regard
ambiguity
as
a
kind
of
error
that
a
parser
i.e.
a
parsing
procedure
plus
a
particular
grammar
makes.
Ambiguity
will
be
discussed
more
thoroughly
in
Chapter
14.
Let
me
just
end
this
section
by
pointing
to
the
possibility
of
using
the
Statement
Browser
to
disambiguate
the
segment
in
question.
Since
the
ambiguity
of
152-154
is
due
to
the
categorial
ambiguity
of
flies
,
the
most
natural
thing
to
do
in
this
case
is
to
select
the
segment
152-154,
select
the
erroneus
sentence
in
the
Statement
Browser,
and
remove
it
by
clicking
the
Remove
button.
Another
possibility
is
to
explicitly
contradict
the
erroneus
sentence
by
clicking
the
Contradict
button.
This
will
either
make
the
inconsistency
manifest,
or,
if
the
grammar
rules
are
interpreted
as
default
rules,
the
erroneus
sentence
will
no
longer
be
derivable
i.e.
the
inconsistency
is
resolved
in
favour
of
the
explicitly
added
sentence
.
More
on
this
in
Chapter
14.
8.5
Alternative
Theory
Presentation
Modes
Logic
is
extremely
useful
for
the
task
at
representing
data,
and
for
reasoning
about
them,
but
is
not
always
adequate
for
presentational
purposes.
This
shows
in
TagLog
too.
One
problem
is
that
TagLog
formulas
expressing
TagLog
theories
tend
to
be
rather
long,
and
what
is
more,
they
tend
to
be
numerous.
Hence,
the
Statement
Browser
is
able
to
show
only
a
small
part
of
the
current
theory
at
a
time.
Another
problem
is
that
it
can
be
hard
to
trace
the
relation
between
the
text
and
the
formulas
describing
the
text,
e.g.
it
can
be
hard
to
see
exactly
to
which
segments
particular
segment-expressions
refer.
The
TagLog
mechanism
for
highlighting
a
segment
denoted
by
a
particular
segmentexpression
helps,
of
course,
but
only
for
one
segment
expression
at
a
time.
In
this
respect,
the
way
traditional
tagging
by
labelling
approaches
use
spatial
relations
as
part
of
their
representational
means
has
a
certain
advantage
from
a
presentational
point
of
view.
154
Browsing
Theories
Viewing
a
traditionally
tagged
text
is
a
bit
like
looking
at
a
map
in
a
direct
sort
of
way,
whereas
inspecting
a
theory
in
the
Statement
Browser
is
more
like
looking
at
a
long
list
of
latitudes
and
longitudes,
through
a
long
and
narrow
tube.
Great
detail
and
precision,
but
not
much
overview.
It
is
important
to
stress,
however,
that
a
user
of
a
logic-based
system
for
corpus
analysis
is
not
confined
to
looking
at
the
result
of
analyses
in
the
form
of
sets
of
logical
formulas.
The
focus
of
this
section
is
on
the
use
of
systematic
and
consistent
mappings
between
TagLog
formulas
and
more
visual
forms,
that
convey
the
same
information
as
the
formulas.
8.5.1
Alternative
ways
of
presenting
singular
statements
Figure
38
indicates
the
mapping
between
TagLog
formulas
and
tablebased,
tier-based,
and
tail-based
representations.
As
should
be
clear
from
this
picture,
the
mapping
is
quite
straightforward,
although
it
may
not
be
quite
so
straightforward
in
cases
where
we
have
overlapping
tags,
for
example.
bstring
John
loves
Mary
:::
bstring
1-2,
John
.
bstring
2-3,
loves
.
bstring
3-4,
Mary
.
syncat
1-2,pn
.
syncat
2-3,v
.
syncat
3-4,pn
.
bstring
syncat
John
loves
Mary
...
pn
v
pn
...
syncat
pn
v
pn
::
John
pn
loves
v
Mary
pn
...
FIGURE
38.
Mapping
theories
to
other
modes
of
presentation
In
Section
8.6,
I
will
present
one
of
these
alternative
modes
of
presentation
in
detail:
table-based
presentation.
Alternative
Theory
Presentation
Modes
155
8.5.2
Alternative
ways
of
presenting
general
statements
Problems
of
presentation
are
not
confined
to
the
singular
statements.
Universal
statements
can
also
be
hard
to
read,
and
alternative
ways
of
presenting
the
same
information
are
of
great
interest.
We
have
already
taken
advantage
of
the
fact
that
there
is
a
well-known
and
natural
way
to
present
grammar
statements,
namely
as
grammar
rules.
syncat
P0-Pn,s
:syncat
P0-Pi,np
,
syncat
Pi-Pn,vp
.
syncat
P0-P1,np
:syncat
P0-P1,pn
.
syncat
P0-Pn,vp
:syncat
P0-Pi,v
,
syncat
Pi-Pn,np
.
s
--
np,
vp.
np
--
pn.
vp
--
v,
np.
FIGURE
39.
Mapping
from
clauses
to
grammar
We
might
also
take
advantage
of
the
fact
that
a
set
of
universally
quantified
sentences
of
a
certain
form
can
be
mapped
to
a
graph,
and
hence,
give
us
a
graphical
way
of
presenting
a
taxonomy.
a
a
X
:-
b
X
.
a
X
:-
c
X
.
b
X
:-
d
X
,
b
X
:-
e
X
.
b
d
e
c
FIGURE
40.
Mapping
from
clauses
to
a
graphically
represented
tree
Other
correspondences
might
exist,
but
this
is
not
the
place
to
explore
them.
156
Browsing
Theories
8.6
Table-Based
Presentation
In
corpus
linguistics
many
uses
are
found
for
tables
and
listings.
In
this
section,
we
will
be
looking
at
how
word
listings,
tables
of
collocations,
listings
of
statements,
listings
of
lexica,
etc.,
can
be
produced
in
TagLog.
The
next
chapter
will
demonstrate
the
building
of
frequency
tables.
Tables,
as
they
are
commonly
understood,
consist
of
columns
and
rows.
From
an
abstract
point
of
view,
a
TagLog
table
is
a
set
of
n-tuples,
where
n
is
the
number
of
columns
and
where
n,
in
principle,
is
any
number
.
Thus,
a
TagLog
table
can
be
specified
by
means
of
a
set-expression.
8.6.1
A
simple
table
in
the
abstract
As
a
first
simple
example
of
how
TagLog
abstractly
characterizes
a
table,
we
use
the
corpus
of
transcriptions
of
doctor-patient
dialogues
prepared
by
K
s-Dienes
forthcoming
,
where
feedback
expression
have
been
described
with
regard
to
their
position
in
a
larger
utterance
external
feedback
position
.3
Consider
the
following
expression:
149
W,P
function
S,feedback
,
string
S,W
,
ext
fb
pos
S,P
This
expression
denotes
the
set
of
pairs
W,P
such
that
W
is
a
word
type
and
P
is
a
structural
position
initial,
medial,
final,
or
single
and
there
exists
a
segment
S
which
is
an
instance
of
both
W
and
P
remember
that
variables
not
bound
by
the
matrix
are
existentially
quantified
.
In
more
concrete
terms,
it
denotes
a
two-column
table
where
the
first
column
consists
of
feedback
words
and
the
second
column
of
the
structural
positions
of
these
words.
The
expression
does
not
say
anything
about
how
to
compute
this
set,
or
how
or
where
to
present
it.
In
order
to
produce
the
actual
table,
such
an
expression
must
be
placed
and
put
to
use
in
the
right
environment.
3.
See
Appendix
C
for
more
detailed
presentations
of
this
and
other
corpora.
The
TagLog
Table
Tool
157
8.7
The
TagLog
Table
Tool
Selecting
Table
from
the
Tool
menu
brings
up
the
dialog
box
in
Figure
41,
containing
one
scrollable
text
entry
field,
two
pop-up
menus,
and
two
buttons.
The
set-expression
denoting
the
set
of
tuples
we
are
interested
in
is
entered
in
the
text
entry
field.
FIGURE
41.
The
TagLog
Table
Tool
The
meaning
of
the
buttons
and
menus
is
as
follows:
Sorting
order:
This
pop-up
menu
has
two
possible
values:
Ascending:
The
table
is
sorted
in
ascending
order
with
respect
to
the
first
column.
This
is
the
default
value.
Descending:
The
table
is
sorted
in
descending
order
with
respect
to
the
first
column.
Dump
to:
This
pop-up
menu
has
three
possible
values:
Dialog:
The
table
is
printed
in
the
dialog
window.
Clipboard:
The
table
is
sent
to
the
clipboard,
from
which
it
can
be
pasted
into
other
applications.
File:
The
table
is
printed
in
a
file
specified
by
the
user.
Build:
Clicking
this
button
will
start
building
the
table
specified
in
the
text
input
field,
sorted
according
to
the
setting
of
the
Sort
order
menu,
and
printed
to
the
target
specified
by
the
current
value
of
the
Output
To
menu.
We
change
the
order
between
the
columns
by
just
changing
the
order
between
the
variables
in
the
set
expression
s
matrix.
Also,
setting
the
158
Browsing
Theories
value
of
the
Sorting
order
pop-up
menu
to
Descending,
gives
us
a
different
table.
8.8
Using
the
Table
Tool
8.8.1
Simple
table
By
placing
the
following
set-expression
in
the
Table
Tool
s
text
entry
field
and
then
clicking
the
build
button,
we
express
a
request
for
a
table
of
positions
of
single
word
feedback
items:
150
W,P
function
S,feedback
,
bstring
S,W
,
ext
fb
pos
S,P
resulting
in
the
following
table:
...
...
d
ehh
ehh
hm
hmm
hmm
hmmm
ja
ja
ja
jah
jah
jaha
jaha
jaha
jaja
jass
jass
jo
mja
mjaa
nja
n
n
n
h
n
h
n
h
n
tack
va
va
vassa
h
initial
single
final
initial
single
initial
initial
single
single
final
initial
single
initial
single
final
initial
single
single
initial
single
initial
initial
single
initial
initial
single
initial
single
initial
initial
single
final
medial
single
initial
8.8.2
Table
with
list-valued
columns
Rather
than
having
multiple
occurrences
of
one
word
form
in
the
first
column,
we
may
prefer
to
have
only
one
occurrence,
and
instead
have
lists
of
values
for
feedback
position
in
the
second
column.
Then
the
built-in
pred-
Summary
and
Conclusion
159
icate
ext
2
comes
in
handy;
ext
SetExpression,List
is
true
iff
List
is
the
list
of
elements
in
the
set
denoted
by
SetExpression.
The
specification:
151
W,L
ext
C
function
S,feedback
,
bstring
S,W
,
ext
fb
pos
S,C
,L
gives
us
the
following
table:
...
d
ehh
hm
hmm
hmmm
ja
jah
jaha
jaja
jass
jo
mja
mjaa
nja
n
n
h
n
h
n
tack
va
vassa
h
initial,single
final
initial,single
initial
initial,single
single
final,initial,single
initial,single
final,initial,single
single
initial,single
initial
initial
single
initial
initial,single
initial,single
initial
initial
single
final,medial
single
initial
8.9
Summary
and
Conclusion
In
the
logical
approach
to
corpus
linguistics,
retrieval
of
information
about
certain
text
segments
the
current
description
of
the
segments
amounts
to
deduction
in
a
theory
encoding
all
the
available
information.
A
special
case
of
this
is
parsing
as
deduction.
The
TagLog
Statement
Browser
is
designed
to
support
inspection
of
the
current
description
of
a
particular
segment
of
text.
The
Statement
Browser
allows
the
user
to
browse
a
full
description
of
a
segment
selected
in
a
text.
It
can
also
be
used
for
monitoring
updates,
deleting
statements,
negating
statements,
and
for
demanding
explanations
of
statements.
The
description
is
always
presented
in
the
form
of
a
set
of
logical
formulas.
This
chapter
has
also
stressed,
however,
that
a
user
of
a
logic-based
system
for
corpus
analysis
is
not
confined
to
looking
at
the
result
of
analyses
in
the
form
of
sets
of
logical
formulas.
Systematic
and
consistent
mappings
between
TagLog
formulas
and
more
visual
forms
exist,
that
convey
the
same
information
as
the
formulas,
but
in
a
more
user
friendly
way.
CHAPTER
9
Quantitative
Analysis
9.1
Introduction
Although
it
is
for
the
support
of
qualitative
rather
than
quantitative
methods
for
description
that
TagLog
has
something
novel
to
offer,
it
is
my
conviction
that
TagLog
has
great
potential
to
become
a
very
flexible
tool
for
quantitative
analysis
as
well.
In
this
chapter,
it
is
will
be
demonstrated
how
traditional
quantitative
methods
for
analysis
of
tagged
corpora,
such
as
simple
counting,
frequency
listing
and
some
common
descriptive
statistic
measures
can
be
accounted
for,
and
in
fact
generalized,
in
terms
of
proof
and
set
theoretic
notions,
and
how
they
are
implemented
in
the
TagLog
system.
In
a
sense,
qualitative
analysis
is
logically
prior
to
quantitative
description.
Counting
the
instances
of
a
class
presupposes
a
classification
of
at
least
some
of
the
segments
of
a
text
in
terms
of
this
class.
Thus,
the
notion
of
quantitative
description
that
I
am
going
to
argue
for
in
this
chapter
presupposes
the
notion
of
qualitative
description
that
have
been
developed
throughout
the
preceding
chapters.
Counting
in
TagLog
161
The
TagLog
system
is
not,
and
will
probably
never
become,
a
general
statistics
tool
or
a
tool
for
creating
attractive
graphic
presentations.
But
it
supports
export
of
data
to
other
such
tools,
and
thus
provides
a
link
in
the
chain
of
tools
needed
by
the
linguist.
The
exporting
capability
of
the
TagLog
system
will
also
be
demonstrated
in
this
chapter.
9.2
Counting
in
TagLog
To
count
in
TagLog
means
to
determine
the
cardinality
of
a
set.
So
counting,
like
many
other
things
in
TagLog,
involves
deduction
and
some
very
elementary
set
theory.
The
cardinality
of
a
set
is
the
number
of
distinct
elements
it
contains.1
To
talk
about
the
cardinality
of
a
set,
we
use
the
relation
card
2,
which
holds
between
the
set
and
its
cardinality,
represented
by
an
integer.
For
example,
both
card
a,b
,2
and
card
a,a,b
,2
are
true
statements,
while
card
a,b,c
,2
is
false.
In
order
to
count
the
elements
of
a
particular
set,
all
we
need
to
do
is
to
frame
a
set-expression
denoting
the
set
in
question,
instantiate
the
first
argument
of
card
2
to
that
expression,
and
query
TagLog
about
the
value
of
the
second
argument.
9.2.1
Counting
types
or
counting
tokens
As
a
simple
example,
consider
the
utterance
in
152
.2
152
det
r
kallt
det
r
det
ja
On
the
one
hand,
if
we
count
the
particular
tokens,
this
utterance
contains
seven
words.
The
way
to
count
them
in
TagLog
is
to
ask:
153
!-
card
S
bstring
S,W
,N
.
which
in
this
case
will
bind
the
variable
N
to
7.
On
the
other
hand,
if
we
count
the
number
of
different
word
types,
the
utterance
contains
only
four
words.
This
is
the
way
to
ask
for
it:
1.
2.
At
least
this
is
true
for
finite
sets.
it
is
cold,
that
it
is,
yes
162
Quantitative
Analysis
154
!-
card
W
bstring
S,W
,N
.
which
binds
N
to
4.
Note
that
the
difference
between
the
expression
in
153
and
the
expression
in
154
is
which
variable
is
placed
in
the
set
expression
s
matrix
and
which
variable
is
existentially
quantified.
9.3
The
TagLog
Count
Tool
Selecting
the
Count
Tool
from
the
Tool
menu
brings
up
the
dialog
box
in
Figure
42,
containing
one
scrollable
text
entry
field,
and
one
button.
FIGURE
42.
The
TagLog
Count
Tool
Count:
A
click
on
this
button
will
compute
the
cardinality
of
the
set
denoted
by
the
set-expression
entered
in
the
text
entry
field.
9.4
Frequency
Tabling
It
often
happens
that
we
want
to
summarize
categorial
data
by
giving
the
frequency
distribution
over
classes.
This
information
is
often
presented
in
the
form
of
a
frequency
listing.
From
a
TagLog
point
of
view,
a
frequency
listing
with
respect
to
a
property
P
is
a
set
of
ordered
pairs
C,N
where
C
is
a
possible
value
of
P,
and
N
is
the
cardinality
for
the
set
of
instances
of
C.
As
a
concrete
example,
consider
the
expression
155
C,N
card
S
lexcat
S,C
,N
Frequency
Tabling
163
This
expression
denotes
the
set
of
pairs
C,N
such
that
C
is
a
lexical
category
and
N
is
the
cardinality
of
the
set
of
instances
of
that
category.
Thus,
what
we
have
here
is
a
word
class
frequency
list.
As
another
example,
the
number
of
word
tokens
uttered
by
each
participant
in
a
dialogue
can
be
specified
as
follows:
156
R,N
card
S
bstring
S,W
,
utterance
S1,R
,
subsegment
S1,S
,N
As
a
third
example,
consider
the
task
of
compiling
a
table
of
biclass
frequencies
e.g.
for
use
in
a
part-of-speech
tagger
.
For
our
sample
Brown
corpus
text,
the
specification
157
N,C1,C2
card
P0-P2
lexcat
P0-P1,C1
,
lexcat
P1-P2,C2
,N
produces
a
table
of
the
following
kind
only
the
first
fifteen
rows
are
shown
:
108
101
98
85
85
82
77
57
55
54
52
42
37
36
35
prep
adj
n
sg
det
adj
det
n
sg
prep
prep
n
pl
n
sg
comma
adj
n
sg
n
pl
det
n
sg
prep
adj
n
pl
n
sg
comma
adj
n
sg
prep
punct
conj
adj
v
sg3
pres
comma
As
usual
in
TagLog,
we
do
not
have
to
have
our
data
explicitly
tagged
in
order
to
refer
to
them.
We
may
write
universal
statements
by
means
of
which
certain
facts
about
segments
in
our
corpus
follow
deductively
from
other
facts
about
them.
In
the
case
of
athe
corpus
of
transcriptions
of
doctor-patient
dialogues
prepared
by
K
s-Dienes
ibid.
,
certain
segments
have
been
described
as
having
a
feedback
function,
and
certain
segments
have
been
described
as
utterances.
Since
the
role
of
the
position
of
feedback
item
in
utterances
was
felt
to
raise
interesting
questions,
a
relation
ext
fb
pos
2,
exploiting
the
ordering
between
points,
was
defined:
158
ext
fb
pos
S,single
:function
S,feedback
,
utterance
S,
.
ext
fb
pos
P0-Pi,initial
:function
P0-Pi,feedback
,
utterance
P0-Pn,
,
Pi
Pn.
164
Quantitative
Analysis
ext
fb
pos
Pi-Pj,medial
:function
Pi-Pj,feedback
,
utterance
P0-Pn,
,
Pi
P0,
Pj
Pn.
ext
fb
pos
Pj-Pn,final
:function
Pj-Pn,feedback
,
utterance
P0-Pn,
,
Pj
P0.
This
made
it
possible
to
count
the
number
of
instances
of
each
type
of
feedback
position:
159
Pos,N
card
S
ext
fb
pos
S,Pos
,N
Tables
like
the
following
could
then
easily
be
generated:
final
initial
medial
single
20
58
10
50
Sometimes
we
want
to
compare
the
way
in
which
the
frequencies
of
different
categories
are
distributed
over
two
or
more
groups.
A
multi-dimensional
table
might
be
a
good
solution
for
this.
Such
tables
do
not
pose
any
special
difficulties
for
TagLog.
A
three-dimensional
table
just
means
that
the
set-expression
s
matrix
must
be
a
triple,
a
four-dimensional
table
means
it
has
to
be
a
quadruple,
etc.
For
example,
consider
the
following
frequency
list
of
feedback
positions:
160
R,P,N
card
S
ext
fb
pos
S,P
,
subsegment
S1,S
,
utterance
S1,R
,N
The
result
is
a
table
of
the
following
kind:
doctor
doctor
doctor
doctor
patient
patient
patient
patient
final
initial
medial
single
final
initial
medial
single
7
26
3
14
13
32
7
36
Often,
a
better
way
to
present
such
a
listing
is
the
following:
doctor
final
initial
medial
single
7
26
3
14
patient
13
32
7
36
Fortunately,
such
multi-dimensional
layouts
of
multi-dimensional
tables
is
also
supported
by
the
current
version
of
TagLog.
In
many
cases,
this
is
the
required
form
of
input
to
a
statistics
or
graphics
program.
Frequency
Tabling
165
9.4.1
Exporting
tables
from
TagLog
to
other
programs
Diagrams
can
be
helpful
as
a
means
of
presenting
a
summary
version
of
a
collection
of
data.
TagLog
excels
at
collection
and
tabulation
of
data,
but
the
current
version
of
TagLog
cannot
produce
high-quality
diagrams,
nor
can
it
perform
inferential
statistical
computations.
But
TagLog
tables,
such
as
the
ones
we
have
introduced
so
far,
can
be
used
as
the
basis
for
contructing
attractive
graphs,
and
nice
looking
tables,
or
whatever
the
user
wants
to
do
with
them,
since
TagLog
tables
can
easily
be
exported
to
other
programs.
There
are
two
ways
to
export
a
table:
via
the
clipboard,
or
via
a
file.
In
Figure
43,
for
example,
the
user
has
entered
a
set-expression
in
the
Table
Tool
s
query
box,
denoting
the
set
of
tuples
C,N
such
that
C
is
a
syntactic
category,
and
N
is
the
cardinality
of
the
set
of
segments
that
are
instances
of
C.
The
user
has
then
clicked
the
Build
button
in
order
to
build
the
table,
dumped
it
to
the
clipboard,
and
exported
it
to
a
charting
program,
where
the
graph
has
been
produced.
FIGURE
43.
Tabling
in
TagLog
166
Quantitative
Analysis
9.4.2
A
word
length
graph
For
showing
dependencies
between
numerical
values,
a
vertical
bar
diagram
is
usually
a
good
choice.
The
relation
between
word
length
x-axis
and
frequency
of
words
of
that
length
y-axis
in
the
doctor-patient
corpus
referred
to
above,
i.e.
the
relation
161
L,N
card
S
bstring
S,W
,
string
length
W,L
,N
which
is
not
particularly
clear
to
present
in
a
table,
is
easy
to
apprehend
here:
Word
freq.
Word
length
FIGURE
44.
Diagram
produced
by
TagLog
and
a
graphing
program
The
following
steps
have
been
taken
in
order
to
produce
this
diagram:
The
set-expression
denoting
the
table
was
entered
in
the
Table
Tool
s
text
input
field;
the
Sorting
Order
menu
was
set
to
Ascending;
the
Output
To
menu
was
set
to
Clipboard;
the
Build
button
was
clicked;
the
graphing
program
was
launched;
the
table
was
pasted
into
its
spreadsheet,
and
the
diagram
was
produced.
9.5
Descriptive
Statistics
in
TagLog
Once
we
have
a
set
of
numerical
data,
we
usually
want
to
be
able
to
summarize
and
present
it
in
such
a
way
that
if
there
were
any
interesting
pat-
Descriptive
Statistics
in
TagLog
167
terns
to
be
found,
we
would
observe
them
in
just
one
quick
glance.
This
is
the
business
of
descriptive
statistics.3
The
current
version
of
TagLog
includes
a
simple
but
general
tool
for
computing
the
following
descriptive
statistical
measures:
Count
Mean
Standard
deviation
Median
Maximum
Minimum
This
tool,
like
most
other
tools
in
TagLog,
takes
as
its
input
a
set-expression,
only
this
time
it
is
not
a
real
set,
but
what
is
usually
called
a
bag.
Given
a
bag
of
integers,
this
tool
computes
the
values
for
the
above
measures
and
presents
them
to
the
user
in
the
form
of
a
table.
Selecting
the
Statistics
Tool
from
the
Tool
menu
brings
up
the
dialog
box
in
Figure
45,
containing
one
scrollable
text
entry
field,
one
pop-up
menu,
and
one
button.
FIGURE
45.
The
TagLog
Statistics
Tool
The
set-expression
denoting
a
bag
of
numbers
is
entered
in
the
text
entry
field.
Output
to:
This
pop-up
menu
has
two
possible
values:
Dialog:
The
table
is
printed
in
the
dialog
window.
3.
A
distinction
is
often
made
between
descriptive
statistics,
which
is
concerned
with
the
summary
of
numerical
data,
and
inferential
statistics,
which
tries
to
draw
inferences
from
such
data.
168
Quantitative
Analysis
Clipboard:
The
table
is
sent
to
the
clipboard,
from
which
it
can
be
pasted
into
other
applications.
Run:
Clicking
this
button
will
compute
the
statistics
mentioned
above
and
print
a
presentation
of
them,
in
the
form
of
a
table,
to
the
target
specified
by
the
current
value
of
the
Output
to
menu.
9.5.1
Descriptive
statistics,
an
example
The
expression
162
N
utterance
S,doctor
,
segment
length
S,N
produces
the
table
Count
Mean
Std.Dev.
Median
Min
Max
83
13.2
13.9
10.0
1
73
and
163
N
utterance
S,patient
,
segment
length
S,N
produces
Count
Mean
Std.Dev.
Median
Min
Max
95
5.9
9.0
3.0
1
69
9.6
Other
Uses
of
Tables
In
this
section,
we
use
the
Table
Tool
in
order
to
generate
listings
of
high
frequency
words,
and
tables
of
collocations,
from
a
description
of
the
doctor-patient
dialogue
introduced
earlier
in
this
chapter.
Thus
we
add
to
the
already
very
long
list
of
functionalities
that
TagLog
provides.
9.6.1
Listing
high
frequency
words
It
is
sometimes
useful
to
be
able
to
determine
the
set
of
the
most
frequently
occurring
words
in
a
text.
Such
a
set
is
defined
relative
to
a
frequency
threshold
F,
so
that
all
words
that
appear
more
than
F
times
in
a
text
are
included
in
the
set.
In
TagLog,
such
a
set
for
F
25
can
be
characterized
as
follows:
Other
Uses
of
Tables
169
164
W
card
S
bstring
S,W
,N
,
N
25
r
s
och
men
jag
ja
har
du
det
de
att
But
often,
for
example
when
the
purpose
of
the
query
is
to
get
a
rough
picture
of
the
content
of
a
text,
it
is
necessary
to
exclude
all
form
words
from
the
list.
Given
an
appropriate
definition
of
form
word
1,
we
can
simply
say:
165
W
card
S
bstring
S,W
,
not
form
word
W
,N
,
N
4
v
l
viktigt
vet
tror
tack
par
ont
lite
hur
huden
g
r
g
ng
doktorn
b
ttre
benen
Now,
it
might
be
more
elegant
instead
to
define
the
notion
of
content
word
more
explicitly,
166
content
word
S,W
:bstring
S,W
,
not
form
word
W
.
and
then
to
use
this
predicate
in
order
to
specify
the
table:
167
W
card
S
content
word
S,W
,N
,
N
4
The
result
will
be
the
same.
9.6.2
Collocations
Collocations
can
be
thought
of
as
arbitrary,
often
domain-dependent,
recurrent
word
combinations
that
are
lexically
clustered
in
the
sense
that
one
or
several
words
of
the
collocations
often
suggests
the
rest
of
the
collocation
cf.
Smadja,
1993
.
In
order
to
find
collocations
in
a
corpus,
a
reasonable
strategy
is
to
look
for
words
that
co-occur
more
frequently
170
Quantitative
Analysis
than
would
be
expected
on
statistical
grounds
only.
For
example,
if
we
assume
that
a
pair
of
words
that
appear
within
the
span
M
more
than
F
times
are
collocations,
then
the
following
expression
defines
the
set
of
collocational
pairs
for
M
10
and
F
2:
168
W1,W2
card
S1,S2
content
word
S1,W1
,
within
span
S1,S2,10
,
content
word
S2,W2
,
N
,
N
2
The
within
span
3
predicate
is
a
built-in
TagLog
predicate
defined
in
a
way
that
makes
within
span
S1,S2,N
true
iff
the
length
of
the
segment
determined
by
the
first
point
in
the
segment
S1
and
the
second
point
in
the
segment
S2
is
less
or
equal
to
N.
With
K
s-Dienes
text
loaded,
and
by
using
168
in
the
TagLog
Table
Tool,
we
are
able
to
generate
the
following
table:4
verkar
verkar
tror
ibland
hela
g
r
g
r
kroppen
hela
b
ttre
finns
kroppen
g
r
fint
It
would
be
quite
straightforward
to
work
with
a
more
constrained
notion
of
collocation,
e.g.
to
require
that
S1
and
S2
stand
in
a
particular
syntactic
relation.
We
would
just
have
to
add
the
relevant
condition
to
the
expression
in
168
.
9.7
Summary
and
Conclusion
In
this
chapter,
theory-building
,
qualitative
approaches
have
met
quantitative
analysis.
Counting,
frequency
tabling,
and
descriptive
statistics,
have
been
demonstrated,
and
we
have
used
the
Table
Tool
in
order
to
generate
listings
of
high
frequency
words
and
collocations.
4.
Here
is
a
translation
into
English:
seems
seems
thinks
sometimes
whole
goes
goes
body
whole
better
is
body
goes
well
CHAPTER
10
Logical
Extensions
10.1
Introduction
For
some
of
the
ideas
that
we
want
to
develop
in
the
TagLog
framework,
it
turns
out
that
we
will
need
to
be
able
to
deal
with
negative,
incomplete
and
inconsistent
information.
In
this
chapter,
we
will
therefore
extend
the
Horn
clause
logic,
very
carefully,
so
that
no
efficiency
is
lost
in
the
process,
by
adding
a
limited
form
of
classical
negation
and
various
metalogical
operators.
These
extensions
will
provide
us
with
more
expressive
power,
but
on
the
negative
side,
we
will
lose
completeness,
so
that
a
great
many
conclusions
that
follow
logically
from
a
theory
expressed
in
the
language
cannot
be
drawn.
172
Logical
Extensions
10.2
Hard
to
Say
in
Horn
Clause
Logic
A
major
problem
with
the
expressive
power
of
pure
Horn
clause
logic
is
that
only
positive
information
can
be
dealt
with.
Positive
information
can
be
explicitly
represented
and
from
the
representations
more
positive
information
can
be
derived.
Information
that
is
not
derivable
is
treated
as
unknown
or
negative,
depending
on
whether
an
open
world
or
a
closed
world
is
assumed.
The
result
of
making
the
so
called
Open
World
Assumption
can
be
depicted
as
follows:
positive
A
unknown
That
is,
a
sentence
A
can
either
be
determined
to
be
true,
or
to
have
a
truth
value
that
is
unknown.
Most
ways
of
dealing
with
negative
information
within
logic
programming
is
related
to
the
Closed
World
Assumption
CWA
.
The
most
wellknown
among
these
is
negation
by
failure.
The
CWA
can
be
formulated
as
follows:
Everything
is
known!
A
full
description
of
the
relevant
part
of
the
world
is
present!
The
knowledge
of
our
chosen
domain
is
complete!
It
then
follows,
under
the
CWA,
that
if
A
cannot
be
inferred,
then
the
truth
value
of
A
does
not
have
to
be
treated
simply
as
unknown.
Rather,
the
negation
of
A
can
be
safely
assumed
to
hold.
This
is
negation
by
failure.
Thus,
to
say
that
John
is
not
a
student,
it
suffices
to
say
that
he
is
a
teacher,
or
in
fact,
to
say
nothing
at
all.
As
long
as
student
john
is
not
provable,
the
negation
of
student
john
can
be
assumed
to
hold.
In
other
words,
and
with
reference
to
the
figure
above
and
to
the
following
figure,
we
have
traded
unknown
information
for
negative
information.
positive
A
negative
Hard
to
Say
in
Horn
Clause
Logic
173
Under
the
CWA
however,
nothing
can
be
left
unsaid
or
undetermined,
or
deemed
irrelevant;
the
unknown
no
longer
exists
as
an
epistemic
category.
But,
as
noted
by
DeRaedt
1992
,
the
CWA
is
indeed
strange
in
a
system
for
interactive
theory
revision!
Why
learn
if
we
know
everything?
Why
update
a
theory
assumed
to
be
complete?
Clearly,
in
the
case
of
TagLog,
the
CWA
cannot
be
maintained,
at
least
not
in
its
unrestricted
form.
Rather,
we
have
to
accept
that
at
a
certain
point
in
time,
a
given
theory
about
a
text
may
be
incomplete
in
the
sense
explained
above:
there
are
certain
sentences
in
our
language
that
are
not
part
of
the
theory,
either
because
we
do
not
know
the
truth
value
of
them
yet
,
or
because
we
do
not
care.
There
are,
however,
a
couple
of
problems
connected
with
negation
by
failure,
as
well.
First,
negation
by
failure,
regarded
as
a
kind
of
negation,
is
rather
limited.
Only
queries
and
antecedents
of
conditional
clauses
i.e.
rules
may
contain
negation.
Unit
clauses
i.e.
facts
cannot
be
negated,
nor
can
conditional
clauses
have
negative
conclusions.
As
a
consequence,
there
are
lots
of
things
we
simply
cannot
say.
A
second
problem
is
that
we
cannot
capture
the
difference
between
no
and
don
t
know
.
As
shown
above,
should
A
not
be
derivable,
we
can
choose
to
regard
that
as
a
proof
of
the
negation
of
A
if
we
make
the
closed
world
assumption
or
we
can
choose
to
regard
A
as
unknown
if
we
assume
an
open
world
,
but
we
cannot
capture
the
difference.
This
is
unfortunate,
since
it
seems
reasonable
to
assume
that
a
system
for
interactive
theory
revision
must
be
able
to
admit
its
ignorance
,
and
be
clear
and
explicit
about
what
it
does
not
yet
know.
Thirdly,
in
a
framework
where
the
only
negation
is
negation
by
failure,
inconsistency
cannot
arise,
since
a
formula
can
never
be
provable
and
not
provable
at
the
same
time.
That
inconsistency
cannot
arise
may
sound
like
a
good
thing,
but
it
s
not:
inconsistency
signals
error
erroneous
predictions,
for
example
and
the
detection
of
error,
in
its
turn,
is
the
most
important
motivation
for
revising
a
theory.
Thus,
to
say
no
to
the
possibility
of
inconsistency
is
to
say
no
to
a
major
driving
force
behind
theory
development.
Hence,
it
seems
that
there
are
four
kinds
of
information
that
we
want
the
TagLog
system
to
be
able
to
distinguish:
positive,
negative,
unknown,
and
inconsistent
information.
What
we
require
can
be
depicted
as
follows:
174
Logical
Extensions
unknown
positive
A
negative
inconsistent
To
sum
up,
the
distinction
between
the
negative
and
the
unknown
presupposes
the
availability
of
explicit
negation,
as
well
as
making
the
open
world
assumption.
Moreover,
inconsistency
is
only
possible
in
a
framework
where
explicit
negative
information
is
representable,
so
that
a
clash
with
explicit
positive
information
becomes
possible.
Thus,
Section
10.3
introduces
a
useful
notion
of
explicit
negation,
and
Section
10.4
introduces
a
battery
of
meta-predicates
to
reason
about
the
known
unknown
and
the
consistent
inconsistent.
Their
uses
might
be
depicted
as
follows:
.
unprovable
A
unprovable
A
provable
A
A
provable
A
provable
A
provable
A
10.3
Explicit
Negation
In
this
section
we
enhance
the
TagLog
formalism
with
a
symbol
for
a
limited
form
of
classical
negation
called
explicit
negation.
The
idea
comes
from
Gelfond
and
Lifschitz
1990
.1
1.
Gelfond
and
Lifschitz
speak
of
their
proposed
extension
as
classical
negation.
But
since
the
laws
of
contraposition
do
not
hold
for
this
kind
of
negation
i.e.
p
:-
q
and
p
does
not
automatically
give
you
q
,
Kakas,
Kowalski
and
Toni
1993
argue
that
the
term
classical
is
inappropriate.
They
prefer
the
term
explicit
negation
instead,
and
so
do
I.
Explicit
Negation
175
From
the
theorem
prover
s
point
of
view,
A
and
A
are
completely
unrelated
and
each
must
be
queried
separately.
In
particular,
a
failure
to
prove
A
doesn
t
mean
that
A
is
proven,
rather,
A
can
be
proven
only
if
A
is
explicitly
given
as
an
axiom,
or
if
there
is
a
clause
A
:-
B,
and
B
can
be
proven.
Gelfond
and
Lifschitz
provide
a
formal
semantics
for
this
extension.
Moreover,
and
this
is
of
great
importance
for
our
purpose,
they
show
that
the
extension
hardly
brings
any
new
computational
difficulties
.2
In
TagLog,
there
are
at
least
two
cases
where
the
introduction
of
negated
facts
fits
in
quite
naturally.
First,
if
we
know
that
a
segment
S
has
a
property
value
V1
for
a
property
P,
i.e.
that
P
S,V1
holds,
we
are
often
able
to
conclude,
if
V2
is
different
from
V1,
that
S
does
not
have
the
value
V2
for
P,
i.e.
that
P
S,V2
holds.
In
particular,
this
happens
when
properties
are
restricted
to
unique
values
for
single
segments
for
example,
we
may
want
to
stipulate
that
a
segment
cannot
be
an
instance
of
more
than
one
lexical
category
.
What
we
need
for
this
kind
of
conclusion
to
be
drawn
formally
is
a
way
to
express
such
unique-value
restrictions.
In
TagLog,
we
solve
that
problem
by
introducing
denials
of
the
following
kind:
169
lexcat
S,V1
:-
lexcat
S,V2
,
V1
V2.
Given
such
a
sentence,
lexcat
2-3,n
is
derivable
if,
for
example,
lexcat
2-3,det
is
derivable.
Second,
it
is
clear
that
if
a
property
P
involved
in
a
statement
A
is
declared
as
being
closed,
and
A
is
not
derivable
from
the
current
theory,
the
negation
of
A
can
be
derived.
In
TagLog
we
say:
170
A
:-
current
text
T
,
functor
A,P,N
,
closed
T,P
N
,
unprovable
A
.
For
example,
if
we
feel
that
we
have
ascribed
a
lexical
category
to
every
segment
in
a
text
text1
that
can
possibly
have
a
lexical
category,
we
may
declare
the
property
lexcat
to
be
closed.
In
order
to
do
so,
we
add
the
following
statement
to
our
theory:
2.
Gelfond
and
Lifschitz
1990,
p.
581
.
In
fact,
they
show
that
under
rather
general
conditions,
simply
by
replacing
each
occurrence
of
P
with
a
new
predicate
P
,
a
theory
containing
explicit
negation
can
be
transformed
into
a
an
essentially
equivalent
theory
that
does
not
contain
explicit
negation.
What
this
shows,
in
fact,
is
that
explicit
negation
does
not
give
us
more
expressive
power,
but
only
a
greater
naturalness
of
expression,
a
greater
modularity,
etc.
176
Logical
Extensions
171
closed
text1,lexcat
2
.
Then,
lexcat
2-3,n
,
lexcat
2-3,adv
,
lexcat
2-3,conj
,
etc.
are
derivable
if
lexcat
2-3,n
,
lexcat
2-3,adv
,
lexcat
2-3,conj
,
etc.
are
not.
This
is
a
special
case
of
what
in
the
literature
has
sometimes
been
called
selective
CWA
a
restricted
form
of
the
Closed
World
Assumption.
10.4
Meta-Logic
A
meta-logic
is
a
logic
by
means
of
which
we
may
refer
to
sentences
of
another
language
the
object-language
.
In
this
section,
we
discuss
a
meta-logical
extension
of
the
Horn
clause
language
given
in
Chapter
3
that
allows
us
to
talk
about
and
manipulate
Horn
clause
language
sentences.
What
I
have
in
mind
here
is
what
has
been
called
a
mono-lingual
metalogic
van
Harmelen,
1989
.
The
object-language
and
the
meta-language
are
one
and
the
same
language.
Object-level
predicates
are
regarded
as
meta-level
function
symbols.
No
distinction
between
object-level
variables
ranging
over
object-level
terms
and
meta-level
variables
ranging
over
object-level
formulas
is
made.3
It
will
be
demonstrated
in
this
section,
and
in
the
next,
that
such
a
metalogical
extension
plays
an
important
role
in
the
TagLog
approach
to
integrated
syntax,
semantics
and
pragmatics,
and
that
it
yields
a
simple
implementation
of
non-monotonic
reasoning.
The
idea
is
to
make
the
provability
and
unprovability
relations
of
the
object
language
available
at
the
meta-language
level.
These
are
the
basic
relations:
provable
A
is
true
if
A
is
provable
in
the
current
theory
unprovable
A
is
true
if
A
is
not
provable
in
the
current
theory
3.
There
are
certain
theoretical
problems
connected
with
this
kind
of
meta-logic
that
does
not
make
a
clear
distinction
between
object
level
and
meta-level
Hill
Lloyd,
1989
.
For
one
thing,
we
want
?-
provable
p
X
to
mean
?-
provable
X
p
X
,
but
it
means
?-
X
provable
p
X
!
.
In
practice,
however,
this
kind
of
meta-programming
is
widely
used,
and
does
not
lead
to
problems
if
one
is
careful.
Meta-Logic
177
Then,
in
terms
of
these
relations,
or
as
modifications
of
them,
other
useful
predicates
are
defined,
such
as:
predicted
A
is
true
iff
A
is
predicted
by
the
current
theory
observed
A
is
true
iff
A
is
a
description
of
an
observation
consistent
A
is
true
iff
A
is
consistent
with
the
current
theory
unknown
A
is
true
iff
A
is
unknown,
or
undecidable,
in
the
current
theory
provable
A,Proof
is
true
iff
A
is
provable
in
the
current
theory
and
Proof
is
a
proof
of
A
card
X
A
,
N
is
true
iff
N
is
the
cardinality
of
the
set
of
X
such
that
A
and
note
that
X
occurs
in
A
is
provable
in
the
current
theory
10.4.1
The
provable
1
predicate
Below,
a
TagLog
theorem-prover
written
in
Prolog
is
given.
Readers
familiar
with
Prolog
programming
techniques
will
recognize
this
as
a
so
called
vanilla
meta-interpreter.4
172
provable
true
.
provable
A
:builtin
predicate
A
,
!,
call
A
.
provable
A
:clause
A,B
,
provable
B
.
provable
A,B
:provable
A
,
provable
B
.
Here
s
the
declarative
meaning
of
the
four
clauses
comprising
the
interpreter:
The
formula
true
is
provable,
unconditionally.
A
built-in
predicate
is
provable
if
it
can
be
executed.
A
literal
A
is
provable
if
a
clause
A
:-
B
can
be
found
and
B
is
provable.
A
conjunction
A,B
is
provable
if
A
is
provable,
and
B
is
provable.
From
a
procedural
point
of
view,
it
can
be
understood
as
follows:
In
the
call
of
the
clause
2
predicate,
B
gets
bound
to
the
body
of
a
non-unit
4.
A
major
reason
for
presenting
this
meta-interpreter
here
is
that
in
sections
to
come,
it
will
be
extended
in
various
ways
in
order
to
handle
the
generation
of
proofs
needed
for
explanations,
and
in
order
to
handle
abductive
reasoning.
178
Logical
Extensions
clause
the
head
of
which
matches
unifies
with
with
A,
or
B
gets
bound
to
true
if
there
is
a
unit
clause
that
matches
with
A.
Unification
is
handled
implicitly.
The
clause
2
predicate
also
takes
care
of
the
systematic
top-tobottom
search
through
the
set
of
clauses
constituting
the
current
theory.
Note
that
the
last
clause
of
the
provable
1
predicate
implements
the
Prolog
leftmost
literal
computation
rule.
Given
the
theory
in
173
,
an
example
of
the
use
of
the
prover
is
given
in
174
.
173
human
X
:-
featherless
X
,
biped
X
.
featherless
socrates
.
biped
socrates
.
174
!-
provable
human
socrates
.
yes
Note
that
although
the
program
in
172
is
able
to
handle
all
the
deductive
reasoning
required
in
TagLog,
it
will
not
be
used
for
this
purpose.
As
pointed
out
before,
one
may
regard
the
TagLog
system
as
an
extension
of
a
Prolog
system,
and
most
of
the
required
reasoning
takes
place
on
the
level
where
TagLog
as
such
is
implemented:
the
Prolog
level.
For
the
basic
reasoning
tasks,
TagLog
cannot
afford
the
overhead
of
a
meta-interpreter.
Hence,
for
most
tasks,
the
definition
in
175
is
used
instead,
transforming
a
meta-level
query
into
an
object-level
query.
175
provable
A
:-
A.
This
is
what
is
usually
referred
to
as
a
reflection
rule.
10.4.2
The
unprovable
1
predicate
TagLog,
just
like
most
implementations
of
Prolog,
provides
a
meta-logical
operator
unprovable
1
called
somewhat
misleading,
as
we
shall
see
negation
by
failure.
The
semantics
of
this
operator
is
as
follows:
176
unprovable
A
succeeds
iff
A
fails
finitely
This
rule
can
be
implemented
very
efficiently
to
prove
a
goal
unprovable
A
,
the
prover
merely
tries
to
prove
A
and
responds
to
the
outcome
as
described
in
176
.
The
Prolog
code
in
177
implements
this
behaviour.5
5.
In
order
to
guarantee
soundness
and
completeness
of
negation
by
failure,
A
must
be
variable
free
when
selected
for
execution
cf.
Lloyd,
1987
.
Meta-Logic
179
177
unprovable
A
:-
provable
A
-
fail
;
true.
The
unprovable
can
be
regarded
as
a
form
of
negation
only
under
the
closed
world
assumption.
Again,
the
difference
between
unprovable
A
and
provable
A
is
essential
whenever
we
cannot
assume
that
the
available
positive
information
about
A
is
complete.
This
happens
all
the
time
in
TagLog.
For
example,
A
can
fail
to
be
complete
when
the
description
of
a
particular
text
is
still
under
development.
10.4.3
The
predicates
observed
1
and
predicted
1
The
provable
1
predicate
can
distinguish
what
is
provable
from
what
is
not.
But
should
A
be
an
explicit
description
of
a
fact,
or
A
follow
from
the
universal
theory,
provable
A
succeeds
just
the
same.
Hence,
provable
1
does
not
distinguish
data
descriptions
from
the
theorems
of
a
universal
theory.
But
a
separation
between
theory
and
data
stricter
than
in
Prolog
can
be
maintained,
so
that
predictions
generated
by
a
theory
on
the
one
hand
and
descriptions
of
data
on
the
other
can
be
kept
apart
when
the
need
arises.
We
distinguish
between
two
ways
in
which
a
sentence
A
can
be
provable:
A
can
be
predicted,
or
A
can
be
observed.
These
are
mutually
exclusive
cases.
Consider
the
following
clauses:
178
p
X
:-
q
X
.
q
a
.
Two
sentences
are
provable
from
178
:
q
a
and
p
a
.
But
these
sentences
differ
in
that
q
a
is
an
explicit
representation
of
data,
describing
an
observed
fact,
whereas
p
a
is
a
prediction
generated
by
means
of
a
universally
quantified
implication.
Predicates
that
check
these
properties
of
sentences
can
be
defined
as
follows:
179
observed
A
:clause
A,true
.
180
Logical
Extensions
180
unobserved
A
:not
clause
A,true
,
not
clause
A,true
.
predicted
A
:provable
A
,
not
observed
A
.
unpredicted
A
:clause
A,B
,
unprovable
B
.
181
182
Thus,
with
respect
to
the
theory
in
178
we
have:
183
184
185
186
observed
q
a
.
yes
observed
p
a
.
no
predicted
q
a
.
no
predicted
p
a
.
yes
Note
that
every
sentence
that
is
provable
is
either
predicted
or
observed.
10.4.4
The
consistent
1
predicate
A
theory
T
is
consistent
iff,
for
no
formula
,
both
and
are
derivable
from
it.
A
Horn
clause
theory
cannot
be
inconsistent,
but
as
soon
as
we
introduce
explicit
negation,
we
invite
inconsistencies
to
arise
as
well.
To
deal
with
consistency,
the
current
implementation
of
TagLog
has
two
built-in
meta
predicates:
consistent
1
and
consistent
0.
Often,
a
modification
of
the
current
theory
introduces
an
inconsistency
that
was
not
there
before.
The
idea
behind
consistent
1
is
to
try
to
detect
an
inconsistency
introduced
by
a
modification
before
actually
performing
the
modification.
The
semantics
of
the
consistent
1
predicate
is
as
follows:
consistent
A
is
true
iff
the
explicit
negation
of
A
cannot
be
proved.
This
immediately
suggests
the
implementation
in
187
.
187
consistent
A
:-
unprovable
A
.
A
couple
of
comments
are
in
order
at
this
point.
First,
note
that
A
must
be
variable
free,
since
the
soundness
of
unprovable
1
cannot
be
guaranteed
otherwise
cf.
footnote
on
page
178
.
Meta-Logic
181
Secondly,
note
that
the
specification
above
and
thus
the
implementation
of
it
is
not
complete.
A
better
specification
might
be
the
following:
consistent
A
is
true
iff
the
explicit
negation
of
none
of
the
consequences
of
A
can
be
proved.
The
latter
specification
suggests
a
forward
chaining
implementation,
more
complicated
and
less
efficient
than
the
implementation
in
187
.
It
would
be
beyond
the
scope
of
this
thesis
to
present
such
an
implementation
here,
and
in
any
case,
the
implementation
in
187
is
good
enough
for
our
purposes.
Note
that
consistent
1
does
not
check
consistency
of
the
whole
current
theory.
Rather,
the
idea
is
that
if
the
current
theory
is
consistent
before
adding
A
to
it,
then,
if
consistent
A
is
true,
it
is
guaranteed
to
be
consistent
afterwards
as
well.
TagLog
also
contains
a
predicate
for
the
global
checking
of
consistency:
consistent
0
is
true
iff
the
current
theory
does
not
allow
a
statement
and
its
negation
to
be
proved.
10.4.5
The
known
and
the
unknown
The
known
1
and
the
unknown
1
predicates
are
not
particularly
useful,
as
far
as
the
present
work
is
concerned.
Nevertheless,
for
completeness,
the
definitions
are
presented
below.
188
known
A
:
provable
A
;
provable
A
.
unknown
A
:unprovable
A
,
unprovable
A
.
189
10.4.6
Meta-logic
summary
Finally,
Table
2
summarizes
the
semantics
of
the
meta-predicates
explained
in
this
section.
For
each
meta-predicate
P
discussed
above,
the
182
Logical
Extensions
calls
!-
P
p
a
and
!-
P
q
a
are
executed
in
three
different
theories,
and
the
outcome
yes
or
no
is
registered.
Meta-logical
test
?-
provable
p
a
.
?-
provable
q
a
.
?-
unprovable
p
a
.
?-
unprovable
q
a
.
?-
observed
p
a
.
?-
observed
q
a
.
?-
predicted
p
a
.
?-
predicted
q
a
.
?-
falsified
p
a
.
?-
falsified
q
a
.
?-
consistent
p
a
.
?-
consistent
q
a
.
?-
known
p
a
.
?-
known
q
a
.
?-
unknown
p
a
.
?-
unknown
q
a
.
TABLE
2.
An
p
X
:-
q
X
.
q
a
.
yes
yes
no
no
no
yes
yes
no
no
no
yes
yes
yes
yes
no
no
p
X
:-
q
X
.
q
a
.
p
a
.
yes
yes
no
no
yes
yes
no
no
no
no
yes
yes
yes
yes
no
no
p
X
:-
q
X
.
q
a
.
p
a
.
yes
yes
no
no
no
yes
yes
no
yes
no
no
yes
yes
yes
no
no
overview
of
the
semantics
of
TagLog
meta-predicates
10.5
Non-Monotonic
Reasoning
In
a
non-monotonic
inference
scheme,
the
validity
of
certain
conclusions
may
depend
on
certain
sentences
not
being
theorems
of
the
current
theory.
Thus,
in
a
non-monotonic
inference
scheme,
the
acquisition
of
new
knowledge
can
cause
old
theorems
to
be
discarded.
On
the
other
hand,
in
a
monotonic
inference
scheme,
new
axioms
never
invalidate
previous
theorems.
In
this
section,
a
technique
for
performing
non-monotonic
reasoning
in
TagLog
is
presented,
the
applications
of
which
will
be
presented
in
sections
to
come.
By
means
of
the
meta-logical
extensions
of
the
TagLog
formalism,
it
is
possible
to
refer
to
the
state
of
the
current
theory
and
thus
to
what
is
currently
known.
In
particular,
by
means
of
the
consistent
1
predicate,
we
Non-Monotonic
Reasoning
183
can
check
the
consistency
of
adding
a
new
variable
free,
atomic
formula
to
the
current
theory.
Furthermore,
by
inserting
a
call
to
consistent
A
in
the
antecedent
of
a
conditional,
it
is
possible
to
make
the
applicability
of
the
conditional
depend
on
whether
the
current
theory,
updated
with
A,
would
be
consistent
or
not.
Typically
then,
clauses
with
positive
consequents,
and
with
an
occurrence
of
consistent
1
in
the
antecedent,
represent
general
rules,
whereas
clauses
with
negative
conclusions
represent
exceptions.
For
example,
in
the
following
theory:
190
can
fly
X
:-
bird
X
,
consistent
can
fly
X
.
can
fly
X
:-
penguin
X
.
bird
tweety
.
bird
pablo
.
penguin
pablo
.
it
is
possible
to
conclude
191
can
fly
tweety
.
can
fly
pablo
.
but
not
192
can
fly
pablo
.
10.5.1
Describing
communicative
functions
in
TagLog
In
this
section,
as
an
example
of
the
use
of
the
meta-logical
approach
to
non-monotonic
reasoning
explained
in
Section
10.5,
a
set
of
simple
rules,
similar
in
spirit
to
the
rules
discussed
by
Robbert-Jan
Beun
in
Beun,
1989
for
recognizing
the
communicative
function
of
an
utterance,
are
given.
Beun
s
rules
are
formulated
in
Default
Logic
Reiter,
1980
but
such
rules
can
easily
be
expressed
using
the
technique
given
above.
A
rule
saying
that
a
declarative
sentence
is
a
statement
by
default
can
be
expressed
as
follows:
193
function
S,statement
:sentence
type
S,declarative
,
consistent
function
S,statement
.
If
we
want
to
say
that,
by
default,
a
declarative
sentence
which
is
not
a
statement,
is
a
question,
we
can
simply
write:
184
Logical
Extensions
194
function
S,question
:sentence
type
S,declarative
,
function
S,statement
,
consistent
function
S,question
.
With
the
following
rule,
we
are
saying
that
an
interrogative
sentence
is,
by
necessity,
a
question
i.e.
this
is
not
a
default
rule
:
195
function
S,question
:sentence
type
S,interrogative
.
A
segment
S
with
content
C
uttered
by
X
to
Y
is
not
a
statement
if
Y
is
believed
by
X
to
be
an
expert
on
C:
196
function
S,statement
:utterance
S,X
,
partner
X,Y
,
content
S,C
,
believes
X,expert
on
Y,C
.
We
also
want
to
stipulate
that
the
system
believes
that
everyone
is
an
expert
on
his
own
desires:
197
believes
sys,expert
on
X,want
X,
.
Here
is
a
simple
made-up
two-turn
dialogue
an
idealized
version
of
a
piece
of
a
PLUS
dialogue
between
an
information
provider
and
an
information
seeker
a
system
and
a
user.
Thus,
the
system
and
the
user
are
communicative
partners
:
198
partner
sys,user
.
partner
user,sys
.
Here
is
the
dialogue:
199
S
:
you
want
a
car
U:
yes
Here
is
a
some
more
information
about
the
dialogue:
200
utterance
1-5,sys
.
utterance
5-6,user
.
sentence
type
1-5,declarative
.
sentence
type
5-6,declarative
.
content
1-5,want
user,car
.
content
5-6,want
user,car
.
Now,
even
though
it
is
declarative
in
its
form,
we
may
use
the
rule
in
194
to
infer
that
this
occurrence
of
you
want
a
car
is
a
question:
Extensions
Applied
185
201
!-
provable
function
1-5,question
.
yes
Thus,
what
we
have
proved
is
that,
since
it
is
a
declarative
sentence,
since
it
is
not
a
statement,
and
since
it
is
consistent
with
the
rest
of
what
we
know,
this
instance
of
you
want
a
car
is
a
question.
We
could
prove
that
it
is
not
a
statement
since
it
is
uttered
by
someone
the
system
who
believes
that
his
communicative
partner
the
user
is
an
expert
on
the
content
of
the
utterance
the
user
s
own
desires
.
10.6
Extensions
Applied
Meta-logical
extensions,
explicit
negation,
as
well
as
the
ability
to
explicitly
represent
negative
information
in
combination
with
the
meta-logical
extensions,
have
many
uses.
Although
I
did
not
explicitly
identify
them
as
such
then,
we
have
in
fact
seen
some
of
these
uses
already.
And
more
is
to
come.
Let
me
list
them
all
here,
with
pointers
to
the
relevant
sections.
Explicit
negation
enables
the
user
to
formulate
some
negative
informa-
tion
about
a
text,
even
before
any
positive
information
is
described
see
the
present
chapter
.
Explicit
negation
enables
the
user
to
explicitly
contradict
a
prediction
made
by
the
system
see
Section
8.3
.
Explicit
negation
is
used
to
represent
exceptions
to
general
rules
see
Section
10.5
.
The
provable
1
meta-predicate
can
be
used
in
support
of
an
approach
to
integrated
grammar
in
the
spirit
of
Hobbs
et
al.
1993
See
Section
4.8.2
.
The
consistent
1
meta-predicate
is
used
to
avoid
unintended
updates
during
hand-coding
of
a
text
see
Section
6.5.2
.
The
consistent
1
meta-predicate
is
used
in
order
to
restrict
abduction
see
Section
13.5
.
The
meta-predicates
are
used
to
allow
the
distinction
between
predic-
tive
and
falsifying
instances.
This
is
a
special
case
of
allowing
the
distinction
between
don
t
know
and
no
see
Chapter
12
.
186
Logical
Extensions
The
meta-predicate
unprovable
1
is
used
to
implement
a
restricted
ver-
sion
of
the
Closed
World
Assumption,
called
selective
CWA
see
the
present
chapter
.
The
meta-logical
approach
to
non-monotonic
reasoning
is
used
to
infer
communicative
function
speech-act
from
sentence
type
mood
see
the
present
chapter
.
The
non-monotonic
reasoning
capability
is
used
to
make
grammars
sensitive
to
context,
for
parsing,
searching
and
automatic
tagging
see
Section
11.6
and
Section
14.4.2
.
This
list
emphasizes
the
claim
made
by
Kakas,
Kowalski
and
Toni
1993
,
that
the
combination
of
explicit
negation
and
negation
as
failure
is
very
useful
for
knowledge
representation
in
general
.
10.7
Summary
and
Conclusion
Certain
things
that
we
would
like
to
be
able
to
do
in
TagLog
require
extensions
of
the
available
means
of
expressing
theories
about
texts,
especially
with
regard
to
negative,
incomplete
and
inconsistent
information.
This
chapter
has
discussed
two
such
extensions:
explicit
negation
meta-level
reasoning
Thus,
the
expressive
power
of
the
logic
presented
in
Chapter
3
is
significantly
raised
for
a
very
low
cost
in
terms
of
loss
of
efficiency.
Kowalski
1990
argues
that
Horn
clause
logic,
augmented
with
such
extensions
as
abduction,
integrity
constraints,
and
meta-level
reasoning
is
sufficient
for
knowledge
representation
and
knowledge
assimilation
purposes,
and
that
the
extension
to
full
first
order
logic
may
be
unnecessary.
It
is
in
that
spirit
the
TagLog
approach
has
been
developed.
CHAPTER
11
Automatic
Tagging
11.1
Introduction
Hand
coding
of
text
is
a
laborious,
error
prone,
and
time-consuming
task,
and
there
are
a
number
of
advantages,
especially
regarding
speed
and
consistency,
with
developing
ways
to
perform
the
tagging,
as
far
as
possible,
automatically.1
While
we
are
far
from
being
able
to
do
automatic
tagging
on
the
level
of
semantics
or
pragmatics
fully
automatic
speech-act
tagging
is
out
of
the
question,
for
example
the
current
state
of
the
art
of
automatic
tagging
allows
part-of-speech
tagging
with
some
success.
Consequently,
this
section
will
concentrate
on
automatic
tagging
methods
applied
to
part-ofspeech
tagging.
Keep
in
mind,
however,
that
they
can,
at
least
in
principle,
be
used
for
other
things
as
well.
1.
As
regards
consistency,
it
should
be
noted
however,
that
there
is
nothing
in
principle
that
stops
us
from
checking
this
property
automatically
even
for
a
hand
coded
text.
188
Automatic
Tagging
A
part-of-speech
tagged
text
is
a
text
in
which
each
word
occurrence
is
classified
in
accordance
with
some
system
of
lexical
categories.
Usually,
the
idea
is
to
have
one
and
only
one
tag
per
word
occurrence.
Now,
the
large
number
of
homographs
in
most
natural
languages
poses
great
problems
for
an
automatic
part-of-speech
tagging
approach.
To
see
these
problems,
consider
the
statement
in
84
,
repeated
here
as
202
.
This
can
be
seen
as
a
rule
for
inheritance
of
lexical
category
from
a
string
to
the
segments
of
the
current
text
that
are
instances
of
this
string.
202
lexcat
S,C
:-
bstring
S,W
,
lexicon
W,C
.
Suppose
that
we
have
a
lexicon
encoded
as
a
predicate
lexicon
2,
possibly
assigning
to
each
word
form
more
than
one
lexical
category.
Some
of
the
clauses
of
this
predicate
is
shown
in
203
.
203
lexicon
to
,inf
.
lexicon
to
,prep
,
lexicon
the
,det
,
lexicon
walks
,v
.
Given
this,
and
given
the
statement
in
204
,
we
can
derive
either
or
both
of
the
statements
in
205
.
204
205
bstring
34-35,
to
.
lexcat
34-35,inf
.
lexcat
34-35,prep
.
Needless
to
say,
because
of
the
remaining
ambiguity,
this
is
not
satisfactory.
And
clearly,
the
problem
with
the
statement
in
202
is
that
it
does
not
look
at
the
context.
As
we
shall
see
in
Chapter
14,
another
way
to
put
it
is
to
say
that
the
statement
in
202
is
false!
.
To
handle
disambiguation,
we
need
to
look
at
the
context.
Now,
it
is
in
fact
possible
to
look
at
only
the
local
co-text
of
the
segments
to
be
disambiguated,
and
to
consider
only
shallow
surface
properties
of
this
co-text.
This
observation
is
the
basis
for
many
techniques
for
automatic
tagging,
and
in
particular
for
the
ones
that
will
be
discussed
here.
The
Elements
of
Automatic
Tagging
189
11.2
The
Elements
of
Automatic
Tagging
Automatic
tagging
can
be
described
as
the
process
of
mapping
an
untagged
corpus
into
a
tagged
version
of
that
corpus.
In
order
to
do
this,
a
tagger
needs
to
draw
on
some
kind
of
knowledge
base.
The
process
is
depicted
in
Figure
46.
Untagged
corpus
Knowledge
base
Tagger
Tagged
corpus
FIGURE
46.
Automatic
tagging
Thus,
automatic
tagging
is
a
matter
of
representation,
acquisition,
and
application
of
knowledge.
That
is,
at
least
the
following
are
needed:
some
strategy
for
representing
the
knowledge
needed
for
automatic
tagging,
typically
in
the
form
of
a
lexicon,
and
a
system
of
rules,
constraints,
or
transitional
probabilities;
some
method
for
acquiring
the
knowledge,
either
by
manual
work,
or
by
learning
from
a
corpus;
some
method
of
applying
the
knowledge
to
untagged
corpora.
11.2.1
Requirements
With
respect
to
the
above
diagram,
the
following
requirements
of
methods
for
automatic
tagging
might
be
formulated.
It
is
required
that:
the
output
of
the
tagger
i.e.
the
tagged
corpus
is
complete
in
the
sense
that
each
word
token
has
received
a
tag
,
correct
in
the
sense
that
every
tag
is
correct
,
and
non-ambiguous
in
the
sense
that
no
word
token
has
received
more
than
one
tag
;
the
tagger
does
not
use
up
too
much
computational
resources,
i.e.
that
it
is
reasonably
efficient;
190
Automatic
Tagging
it
is
reasonably
non-problematic
to
acquire
the
knowledge
base
that
the
tagger
needs.
From
a
TagLog
point
of
view,
we
will
also
require
of
a
method
for
automatic
tagging
that
it
is
reasonably
in
line
with
the
rest
of
the
TagLog
philosophy,
and
that
it
uses,
as
far
as
possible,
TagLog
logic
in
order
to
represent
the
knowledge
needed
for
tagging.
In
the
following,
I
will
consider
four
different
approaches
to
automatic
part-of-speech
tagging:
a
statistics-based
approach,
a
transformationbased
approach,
a
constraint-based
approach,
and
a
novel
logic-based
approach.
11.3
Statistical
Part-of-Speech
Tagging
Typically,
a
statistics-based
method
for
automatic
part-of-speech
tagging
represents
the
required
knowledge
as
a
probabilistic
model.
Most
popular
among
such
models
is
the
hidden
Markov
model
HMM
.
In
a
HMM
there
are
states,
transitions
between
states,
and
symbols
emitted
by
the
states.
There
are
two
kinds
of
probabilities
associated
with
a
HMM:
transition
probabilities;
i.e.
the
probability
of
a
transition
from
one
state
to
another,
and
output
probabilities,
i.e.
the
probability
of
a
certain
state
emitting
a
certain
symbol.
The
model
is
called
hidden
since
from
watching
the
string
of
symbols
that
it
outputs,
we
cannot
in
general
determine
which
states
it
passes
through.
I
will
illustrate
here
with
a
simple
HMM
in
which
the
states
represent
parts-of-speech,
and
the
symbols
emitted
by
the
states
are
words.
The
assumption
is
that
a
word
depends
probabilistically
on
just
its
own
partof-speech
i.e.
its
tag
which
in
turn
depends
on
the
part-of-speech
of
the
preceding
word
or
on
the
start
state
in
case
there
is
no
preceding
word
.2
Our
sample
HMM
can
be
displayed
as
in
Figure
47.
2.
This
is
the
assumption
of
a
so
called
biclass
model.
It
is
more
common
in
fact
to
use
a
triclass
model,
i.e.
to
assume
that
the
part-of-speech
of
a
word
depends
on
the
parts-of-speech
of
the
preceding
two
words.
Statistical
Part-of-Speech
Tagging
191
0.20
0.10
State:
det
Out
Prob.
a
0.300
the
0.250
...
...
0.01
0.18
State:
aux
Out
Prob.
can
0.010
do
0.110
...
...
0.50
0.01
0.36
0.30
0.01
0.21
0.01
0.20
0.01
0.77
0.25
State:
v
Out
Prob.
saw
0.009
can
0.005
...
...
Start
0.10
0.10
0.01
0.01
0.30
0.01
0.52
0.45
0.26
0.36
0.39
State:
pron
Out
Prob.
she
0.070
he
0.070
...
...
State:
n
0.01
0.01
Out
Prob.
pie
0.002
can
0.001
...
...
Spaghetti
and
HMMeatballs
0.01
0.34
TL-95
FIGURE
47.
A
Hidden
Markov
Model
Needless
to
say,
in
a
realistic
application,
the
HMM
will
have
to
be
very
much
bigger.
Still,
this
particular
HMM
is
capable
of
generating,
among
other
strings,
the
sentence
he
can
can
a
can
.
It
passes
from
the
start
state
to
the
state
pron,
from
there
to
aux,
then
to
v,
and
from
v
to
det
and
finally
to
n,
and
outputs
the
words
as
it
moves
from
state
to
state.
The
probability
of
this
happening
is
easy
to
compute:
we
simply
multiply
all
the
transition
probabilities
and
output
probabilities
along
the
path.
This
particular
HMM
could
generate
the
same
string
of
words
by
going
through
other
states
e.g.
start-pron-aux-aux-det-aux
but
that
would
be
less
probable.
192
Automatic
Tagging
The
above,
however,
is
a
good
description
of
string
generation,
but
not
of
part-of-speech
tagging.
In
part-of-speech
tagging,
the
string
of
symbols
is
given
and
the
task
is
to
find
the
sequence
of
state
transitions
most
likely
to
have
generated
this
string.
So
to
tag
a
given
text,
we
need
a
program
that
computes
the
sequence
of
state
transitions
that
has
the
highest
probability
of
being
the
one
that
generated
the
text.
In
principle,
the
simple
Prolog
program
in
206
would
do
just
fine.3
206
most
probable
sequence
Words,P-Ss
:findall
PS,sequence
Words,1-
start
,PS
,PSs
,
max
key
PSs,P-Ss1
,
reverse
Ss1,
start
Ss
.
sequence
,PSs,PSs
.
sequence
Word
Words
,P1-
S1
Ss
,PSs
:outprob
Word,S2,Po
,
transprob
S1,S2,Pt
,
P2
is
Po
Pt
P1,
sequence
Words,P2-
S2,S1
Ss
,PSs
.
It
is
assumed
that
the
output
probabilities
are
represented
as
Prolog
clauses
of
the
following
kind
only
two
are
shown
here,
the
others
are
listed
in
Appendix
B
:
207
outprob
a,det,0.300
.
outprob
can,aux,0.010
.
...
Moreover,
it
is
assumed
that
the
transitional
probabilities
are
represented
as
follows
again
not
all
of
them
are
shown
:
208
transprob
det,det,0.20
.
transprob
det,aux,0.01
.
transprob
det,v,0.05
.
...
Then
the
most
probable
sequence
of
tags
corresponding
to
he
can
can
a
can
is
computed
as
follows:
209
?-
most
probable
sequence
he,can,can,a,can
,P-Seq
.
P
1.965e-11
Seq
pron,aux,v,det,n
This
program
is
horrendously
inefficient,
however,
since
it
examines
every
sequence
of
states
that
could
possibly
generate
the
text,
which
3.
See
appendix
B
for
some
of
the
implementation
tricks
used
here.
Statistical
Part-of-Speech
Tagging
193
indeed
can
be
very
many.
In
our
HMM
example
there
are
only
27
different
sequences
of
states
that
generate
he
can
can
a
can
but
that
sentence
is
very
short
and
with
longer
sentences
and
bigger
HMMs
we
are
looking
at
thousands
or
even
hundreds
of
thousands
of
possible
sequences.
Hence,
a
naive
algorithm
will
not
do.
Fortunately,
there
are
algorithms
that
will
find
the
most
probable
sequences
very
fast,
without
having
to
calculate
the
probabilities
for
all
sequences.
In
Appendix
B,
I
have
given
a
Prolog
implementation
of
one
such
algorithm
the
Viterbi
algorithm
Viterbi,
1967
.
In
our
sample
HMM,
the
probabilities
have
been
invented.
There
are
two
well-known
methods
for
training
the
model,
i.e.
for
acquiring
the
required
probabilities
automatically.
The
first
method
involves
collecting
statistics
from
a
hand
coded
training
corpus.
For
example,
for
each
state
t
and
each
word
form
w,
the
output
probabilities
can
be
estimated
by
taking
the
number
of
occurrences
of
w
tagged
as
t
in
a
training
corpus
and
divide
it
by
the
number
of
occurrences
of
t.
And
for
each
pair
of
states
t1
and
t2
the
transitional
probabilities
can
be
estimated
by
taking
the
number
of
occurrences
of
t1
being
followed
by
t2,
and
divide
it
by
the
number
of
occurrences
of
t1.
As
I
have
shown
in
Section
9.4,
such
data
would
be
simple
to
collect
in
TagLog.
The
second
method
for
training
a
HMM
is
to
estimate
these
probabilities
without
a
hand
coded
corpus
by
means
of
the
forward-backward
algorithm
see
e.g.
Charniak,
1993
.
As
for
the
evaluation
of
methods
for
statistics-based
tagging,
researchers
are
reporting
success
rates
of
90-95
Church,
1988;
DeRose,
1988;
Charniak,
1993
.
For
such
a
simple
approach,
this
is
surprisingly
good.
Tagging
by
means
of
this
method
is
rather
efficient
as
well,
provided
a
good
algorithm
is
used.
Learning
can
be
very
fast,
at
least
with
the
first
method
described
above,
since
it
merely
involves
counting.
On
the
other
hand,
to
work
well,
both
learning
methods
require
a
lot
of
training
material.
It
is
sometimes
claimed
that
it
is
hard
to
read
and
grasp
the
significance
of
knowledge
of
the
kind
used
by
this
approach.
I
disagree.
If
there
are
problems
with
this
method,
they
are
not
of
a
semantic
nature.
The
information
represented
in
these
tables
is
order-independent,
meaningful,
although
perhaps
a
bit
overwhelming
when
there
is
a
lot
of
it
but
that
should
be
possible
to
take
care
of
by
means
of
browsers
that
can
hide
information
194
Automatic
Tagging
considered
insignificant
.
Moreover,
the
method
is
both
simple
and
elegant.
However,
although
it
is
clear
that
TagLog
is
compatible
with
a
statisticsbased
approach
to
automatic
tagging,
and
that
it
would
not
be
out
of
the
question
to
develop
a
tagger
module
in
TagLog
based
on
such
principles,
we
will,
in
the
sections
to
follow,
continue
to
search
for
something
more
in
the
spirit
of
TagLog.
The
fact
that
we
have
implemented
Viterbi
tagging
in
a
logic
programming
language
does
not
make
Viterbi
tagging
part
of
the
logical
approach
to
computational
corpus
linguistics.
11.4
Transformation-Based
Part-of-Speech
Tagging
Part-of-speech
tagging
had
for
some
years
been
an
area
of
natural
language
processing
where
statistical
techniques
were
more
successful
than
rule-based
methods.
Then
Brill
1992
presented
a
simple
rule-based
partof-speech
tagger
which
performs
with
an
accuracy
comparable
to
statistics-based
taggers.4
In
this
section,
I
will
demonstrate
that
a
system
of
TagLog
condition-action
rules
see
Section
3.3.3
is
a
powerful
enough
device
to
implement
Brill
s
method.
Brill
s
method,
too,
involves
a
lexicon.
Suppose
we
have
a
lexicon,
accessed
via
a
predicate
lexicon
2,
that
assigns
only
the
most
likely
tag
to
a
word
form.
For
example,
this
lexicon
would
list
run
as
a
verb,
but
not
as
a
noun,
despite
the
fact
that
run
sometimes
occur
as
in
The
run
lasted
thirty
minutes
.
One
obvious
way
to
acquire
such
a
lexicon
would
be
to
extract
it
from
a
large
hand
coded
corpus,
perhaps
using
TagLog
for
that
purpose.
Brill
s
strategy
also
involves
rules
that
will
assign
tags
to
segments,
based
on
what
is
in
the
lexicon,
the
endings
and
capitalization
of
words,
and
cotext.
We
will
use
TagLog
condition-action
rules
for
this
purpose.
4.
By
way
of
comparing
transformation-based
on
the
one
hand
with
statistical
approaches
like
the
HMM
method
explained
in
Section
11.3
on
the
other,
one
should
note
that,
in
a
sense,
the
former
approaches
rely
on
contextual
transitional
probabilities
too,
but
only
in
an
indirect
way:
the
statistics
is
calculated
during
learning
time,
and
is
not
part
of
the
resulting
description.
Transformation-Based
Part-of-Speech
Tagging
195
For
example,
the
rule
in
210
has
the
effect
that
all
segments
that
are
instances
of
strings
that
are
in
the
lexicon
receive
the
most
likely
tags.
Instances
of
word
forms
that
are
not
in
the
lexicon
do
not
receive
any
tag
at
all.
210
bstring
S,W
,
lexicon
W,C
add
lexcat
S,C
.
With
the
rule
in
211
,
we
take
advantage
of
the
fact
that
words
not
in
the
lexicon
tend
to
be
proper
nouns
if
they
are
capitalized.
211
bstring
S,W
,
not
lexcat
S,
,
capitalized
W
add
lexcat
S,pn
.
The
rule
in
212
has
the
effect
that
words
that
are
ending
in
ous
and
that
have
not
already
received
a
tag
for
lexcat,
are
tagged
as
adjectives,
because
that
is
the
most
common
tag
for
words
ending
in
that
way.
212
bstring
S,
ous
,
not
lexcat
S,
add
lexcat
S,adj
.
It
is
easy
to
think
of
other
rules
along
similar
lines.
The
meaning
of
the
rule
in
213
is
that
if
a
segment
is
tagged
inf
meaning
infinitive
marker
and
the
following
segment
is
tagged
det
meaning
determiner
,
then
switch
the
tag
from
inf
to
prep
meaning
preposition
.
213
lexcat
P0-P1,inf
,
lexcat
P1-P2,det
replace
lexcat
P0-P1,inf
,
lexcat
P0-P1,prep
.
The
rationale
for
this
rule
is
that
a
noun
phrase
is
much
more
likely
to
follow
immediately
after
a
preposition
than
after
the
infinitive
marker
to.
The
rule
in
214
replaces
all
noun
tags
immediately
preceded
by
inf
tags
with
verb
tags.
214
lexcat
P0-P1,inf
,
lexcat
P1-P2,n
replace
lexcat
P1-P2,n
,
lexcat
P1-P2,v
.
A
sequence
of
condition-actions
rule
are
executed,
in
order,
by
calling
ca
run
0
defined
in
Section
3.3.3
.
Thus,
the
first
three
rules
just
add
tags
so
that,
if
all
goes
well,
every
basic
segment
has
received
exactly
one
tag
when
these
three
rules
have
executed.
The
fourth
and
fifth
rules
patches
the
result
of
these
executions,
by
replacing
certain
tags
with
other
tags,
if
certain
co-textual
conditions
hold.
These
rules
are
called
transformation-rules
.
Note
how
important
the
ordering
of
the
rules
are,
since
each
rule
may
radically
change
the
conditions
under
which
the
rules
that
follow
work.
196
Automatic
Tagging
11.5
The
Learning
of
Transformation-Rules
One
of
the
main
points
in
Brill
s
work
is
that
transformation-rules
for
automatic
tagging
can
be
automatically
acquired.
Brill
calls
his
technique
error
driven
learning
.
The
distribution
of
errors
produced
by
an
imperfect
tagger
i.e.
a
tagger
making
use
of
all
available
rules
except
transformation
rules
is
examined
to
learn
an
ordered
list
of
transformation-rules
that
can
be
applied
to
provide
more
accurate
tagging.
The
method
is
depicted
in
Figure
48.
Untagged
corpus
Imperfect
Tagger
Tagged
corpus
Hand
coded
version
of
corpus
Learner
Transformationrules
Transformationrule
templates
FIGURE
48.
Error-driven
learning
Adapted
from
Brill,
1992
At
each
stage
of
learning,
the
learner
finds
the
transformation
whose
application
to
the
tagged
corpus
results
in
the
best
scoring
tagged
text.
The
transformations
considered
are
instances
of
templates,
defining
a
set
of
allowable
transformations.
The
scoring
is
calculated
by
comparing
the
result
of
transformations
with
a
small
hand
coded
text
which
is
assumed
to
be
tagged
correctly.
Learning
proceeds
on
the
tagged
text
that
results
from
applying
the
learned
transformation.
This
continues
until
no
more
transformations
can
be
found
whose
application
results
in
an
improvement.
Once
an
ordered
list
of
transformation-rules
has
been
learned,
The
Learning
of
Transformation-Rules
197
unseen
text
is
tagged
by
simply
applying
the
imperfect
tagger,
followed
by
each
transformation,
in
order,
to
the
entire
text.5
This
learning
method
can
be
implemented
as
in
215
.
The
procedure
learn
1
generates
all
possible
instantiations
of
all
templates,
and
then
calls
learn
2
which
performs
an
exhaustive
search
through
this
space,
finds
the
transformation-rule
that
maximizes
the
increase
in
truthlikeness
of
the
current
theory,
adds
it
to
the
other
rules,
runs
it,
and
then
calls
itself
recursively
in
order
to
proceed
with
learning
giving
the
new
current
theory.
The
procedure
terminates
when
the
best
rule
does
not
increase
performance
above
a
threshold
set
by
the
user.
215
learn
Threshold
:setof
Rule,template
Rule
,RuleInstances
,
learn
Threshold,RuleInstances
.
learn
Threshold,RuleInstances
:find
best
RuleInstances,WinnerRule,WinnerScore
,
assert
WinnerRule
,
execute
WinnerRule
,
WinnerScore
Threshold,
learn
Threshold,RuleInstances
.
learn
Threshold,
RuleInstances
.
Templates
are
specified
by
the
user
of
the
system,
in
the
form
exemplified
in
216
.
Templates
typically
involve
segments
within
a
few
positions
on
each
side
of
the
segment
whose
description
will
be
affected
by
the
transformations.
216
template
lexcat
P0-P1,A
,
lexcat
P1-P2,B
replace
lexcat
P0-P1,A
,
lexcat
P0-P1,C
:tag
value
lexcat,A
,tag
value
lexcat,B
,tag
value
lexcat,C
.
template
lexcat
P0-P1,B
,
lexcat
P1-P2,A
replace
lexcat
P1-P2,A
,
lexcat
P1-P2,C
:tag
value
lexcat,A
,tag
value
lexcat,B
,tag
value
lexcat,C
.
Given
a
list
of
rule
instances,
the
procedure
in
217
evaluates
all
of
them
and
finds
the
one
that
scores
best.
217
find
best
NewRule
Rules
,Winner,Score
:evaluate
NewRule,NewScore
,
find
best
Rules,NewRule,NewScore,Winner,Score
.
5.
It
might
be
interesting
to
regard
this
method
as
an
instance
of
Popper
s
schema:
P1
TT
EE
P2.
198
Automatic
Tagging
find
best
,Winner,Score,Winner,Score
.
find
best
NewRule
Rules
,Leader,HighScore,Winner,Score
:evaluate
NewRule,NewScore
,
NewScore
HighScore
-
find
best
Rules,NewRule,NewScore,Winner,Score
;
find
best
Rules,Leader,HighScore,Winner,Score
.
The
predicate
evaluate
2
evaluates
a
rule
with
respect
to
the
current
text.
The
number
of
cases
where
the
current
tagging
is
incorrect
and
the
transformation-rule
will
correct
it,
and
the
number
of
cases
where
the
current
tagging
is
correct,
and
therefore
the
transformation
will
cause
some
damage,
are
calculated.
The
score
of
the
rule
is
returned
as
the
difference
between
the
former
number
and
the
latter.6
218
evaluate
Condition
replace
A,
B
,
Score
:count
B,
Condition
,
Positive
,
count
A,
Condition
,
Negative
,
Score
is
Positive
-
Negative.
Now,
unfortunately,
this
implementation
is
far
too
inefficient
to
be
practically
useful.
The
problem
is
that
we
are
trying
to
generate
all
instances
of
all
templates
before
doing
anything
else,
and
usually
the
number
of
instances
is
very
large
for
example,
given
6
templates
and
20
different
tags,
48.000
rules
will
be
generated!
To
find
the
one
that
scores
best,
the
procedure
would
have
to
evaluate
each
of
them
with
respect
to
the
whole
training
corpus!
.
Instead,
as
pointed
out
by
Brill,
a
better
strategy
is
to
begin
by
computing
a
list
of
all
error
types
that
the
current
sequence
of
condition-action
rules
makes,
and
then
generate
and
test
only
those
rules
that
may
be
used
to
correct
some
of
these
errors.
Thus,
only
a
very
small
percentage
of
possible
transformation-rules
really
need
to
be
examined.
I
have
implemented
a
procedure
that
has
been
improved
in
this
way
and
in
other
ways
as
well
.
It
is
given
in
Appendix
B.
When
tested
with
6
templates
and
20
different
tags
on
a
2000
word
corpus,
it
was
able
to
find
20
rules
within
a
couple
of
hours
on
a
Sparcstation
10
running
Sicstus
Prolog.
This
is
not
fast,
but
it
is
fast
enough
to
be
practical.
With
71
patch
rules
along
the
lines
of
213
and
214
,
Brill
obtained
an
error
rate
of
5.1
,
when
testing
the
tagger
on
5
of
the
Brown
Corpus.
6.
Note
that
the
clauses
in
the
supposedly
correctly
hand
coded
theory
are
assumed
to
be
clauses
of
the
form
Clause,
where
is
a
prefix
functor.
This
is
just
to
distinguish
them
from
clauses
produced
by
the
automatic
tagging
procedure.
Logic-Based
Part-of-Speech
Tagging
199
This
is
on
par
with
the
best
stochastic
taggers.
Also,
tagging
is
efficient.
Learning,
though,
is
rather
slow.
On
the
other
hand,
learning
does
not
require
a
very
large
pre-tagged
corpus
to
learn
from,
a
rather
small
one
will
suffice.
It
is
argued
by
Brill
that
rule-based
taggers
have
important
advantages
over
stochastic
taggers:
they
are
smaller,
more
perspicuous,
and
more
portable.
I
believe,
however,
that
this
claim
should
be
balanced
against
the
fact
that
condition-actions
rules
can
be
very
hard
to
understand
since
what
they
actually
do
is
dependent
on
what
rules
that
were
executed
earlier
have
already
done.
Nevertheless,
it
is
probably
true
that
Brill
s
transformations
are
more
open
for
manual
tuning,
and
thus
more
suitable
for
a
balanced
cooperative
approach,
than
are
the
matrices
of
statistical
data.
How
well
does
Brill
s
approach
fit
in
with
TagLog?
What
about
connectivity?
Since
the
conditions
of
condition-action
rules
are
logical
conditions,
a
Brill
tagger
implemented
as
above
will
in
principle
be
sensitive
to
knowledge
produced
by
other
TagLog
tools.
Furthermore,
since
what
is
asserted
or
retracted,
or
replaced
by
the
action
parts
of
condition-action
rules
are
logical
sentences,
other
tools
will
be
aware
of
the
result
of
using
a
Brill
tagger.
But
the
condition-action
rules
as
such
will
be
of
no
use
to
other
tools.
Moreover,
TagLog
tools
such
as
the
Explanation
Tool
or
the
Hypothesis
Testing
Tool
tools
that
do
very
useful
things
with
universally
quantified
conditionals
do
not
understand
condition-action
rules,
so
special
purpose
tools
for
debugging
and
analysis
of
condition-action
rules
would
probably
have
to
be
implemented
as
well.
11.6
Logic-Based
Part-of-Speech
Tagging
Brill
s
strategy
is
promising:
it
is
simple
and
powerful,
and
it
can
be
implemented
efficiently
in
TagLog
by
means
of
condition-action
rules.7
But
it
is
anything
but
declarative,
and,
as
always,
a
declarative
account
would
be
preferable.
So
in
this
section,
I
sketch
a
rather
more
declarative
and
logical
solution
to
the
problem
of
part-of-speech
tagging.
7.
For
an
extremely
efficient
implementation
by
means
of
finite-state
transducers,
see
Roche
Schabes,
1995
.
200
Automatic
Tagging
In
Section
11.1,
we
saw
that
a
simple
inheritance
rule
cannot
adequately
describe
the
relation
between
a
lexicon
and
tokens
in
a
text.
Fortunately,
as
we
have
already
seen,
there
seems
to
exist
rather
distinct
contextual
constraints
on
when,
for
example,
a
segment
can
be
an
instance
of
an
infinitive
marker.
An
infinitive
marker
can
never
immediately
precede
a
determiner
whatever
immediately
precedes
a
determiner
is
not
an
infinitive
marker.
Nothing
can
be
more
natural
than
to
capture
such
constraints
by
means
of
negative
statements,
such
as
the
statement
in
219
.
219
lexcat
P0-P1,inf
:-
lexcat
P1-P2,det
.
In
order
to
make
our
lexical
inheritance
rule
sensitive
to
constraints,
we
need
to
modify
it
as
follows:
220
lexcat
S,C
:bstring
S,W
,
lexicon
W,C
,
consistent
lexcat
S,C
.
Thus,
what
we
are
looking
at
here
is
a
kind
of
default
inheritance
rule,
that
will
allow
inheritance
as
long
as
it
does
not
give
rise
to
an
inconsistency.
Typically,
inconsistencies
arise
when
constraints
apply,
and
thus,
inheritance
is
not
allowed
in
these
cases.
Ideally,
our
system
of
constraints
will
allow
only
one
part-of-speech
to
be
assigned
to
each
segment
the
correct
one
although
in
practice
we
will
probably
have
to
be
content
with
a
substantial
overall
reduction
in
ambiguity.
Now,
given
the
statements
in
221
,
only
222
can
be
derived,
not
223
.
221
222
223
bstring
34-35,
to
.
bstring
35-36,
the
.
lexcat
34-35,prep
.
lexcat
34-35,inf
.
Which
is
exactly
what
we
wanted.
We
can
see
this
as
a
kind
of
ambiguity
resolution
through
consistency
recovery
.
Before
we
add
the
constraint
in
219
to
the
current
theory,
it
follows,
from
the
sentence
in
220
,
and
from
bstring
34-35,
to
,
that
both
lexcat
34-35,prep
and
lexcat
34-35,inf
hold.
Given
a
single-valued
constraint,
these
theorems
are
inconsistent
with
one
another.
But
each
of
them
holds
by
default
only.
So
when
we
add
the
constraint
in
219
,
one
of
them,
namely
lexcat
34-35,inf
,
is
explicitly
negated
and
Logic-Based
Part-of-Speech
Tagging
201
thus
overridden,
so
that
now
only
the
other,
namely
lexcat
34-35,prep
,
still
holds.
Hence,
by
the
addition
of
the
constraint,
the
full
description
of
34-35
was
disambiguated
in
favour
of
lexcat
34-35,prep
.
This
is
a
non-monotonic
process:
by
adding
an
axiom,
we
invalidated
a
previous
theorem.
Since
that
theorem
happened
to
contradict
another
theorem,
consistency
was
recovered,
and
thus
the
ambiguity
was
resolved.
Let
us
look
at
a
couple
of
other
constraints.
An
infinitive
marker
can
never
precede
a
proper
noun
either,
so
let
us
reformulate
the
relevant
constraint
by
means
of
a
disjunction.
Moreover,
verbs
do
not
succeed
determiners.
Formally:
224
lexcat
P0-P1,inf
:-
lexcat
P1-P2,det
;
lexcat
P1-P2,pn
.
lexcat
P1-P2,v
:-
lexcat
P0-P1,det
.
Several
hundred
such
constraints
would
probably
be
very
useful.
The
problem
is
to
find
them.
Here
too,
of
course,
the
ideal
would
be
a
procedure
that
allows
us
to
acquire
them
automatically
from
a
hand
coded
corpus.
Perhaps
the
distribution
of
errors
produced
by
the
lexical
inheritance
rule
could
be
examined
to
learn
a
set
of
constraints
in
the
form
of
negative
statements
that
could
be
added
to
the
current
theory
to
provide
more
accurate
tagging.
I
have
placed
this
on
the
agenda
for
future
work.
All
things
considered,
logic-based
tagging
seems
to
be
a
both
viable
and
elegant
approach,
well
worth
exploring
further.
What
remains
is
to
assess
its
practical
merits
and
disadvantages,
and
to
compare
it
with
other
approaches
when
tried
on
real
corpora.
This
is
outside
the
scope
of
the
present
thesis,
however.
In
the
mean
time,
before
such
an
investigation
has
been
performed,
we
can
only
assess
and
compare
it
from
a
theoretical
point
of
view.
The
logical
approach
to
automatic
tagging
then
compares
very
favourable
with
Brill
s
approach.
The
logic-based
approach
to
automatic
tagging
could
be
characterized
as
non-modular,
implicit,
and
demand-driven,
whereas
we
might
want
to
characterize
Brill
s
approach
and
many
other
approaches
as
modular,
explicit,
all-at-once,
and
once-and-for-all.
I
think
that
the
non-modular
approach
of
logic-based
tagging
is
more
elegant
than
Brill
s
two-stage
approach.
The
strategy
of
adding
a
tag
say
inf
,
only
to
swap
it
for
another
tag
say
prep
the
moment
after,
is
neither
elegant
nor
efficient
why
add
it
in
the
first
place?
.
202
Automatic
Tagging
Furthermore,
with
a
large
on-line
lexicon
in
the
background
there
should
be
no
need
to
tag
segments
explicitly
for
parts-of-speech,
all
at
once
and
once
and
for
all.
In
the
logic-based
approach,
information
about
parts-ofspeech
follows
implicitly
from
the
lexicon,
the
lexical
inheritance
rule,
and
the
negative
statements
encoding
the
constraints,
and
can
be
derived
automatically
when
there
is
a
demand
for
it.
Of
course,
we
could
still
be
interested
in
caching
these
statements
as
lemmas,
by
deriving
them
and
then
storing
them
explicitly,
once
and
for
all,
but
in
the
logic-based
approach
it
no
longer
constitutes
a
necessary
step
in
a
procedure
for
automatic
tagging.
The
logic-based
approach
to
automatic
tagging
is
formulated
wholly
within
the
TagLog
logical
framework.
It
is
therefore
easy
to
integrate
or
rather,
it
is
already
integrated
with
other
TagLog
functionalities,
such
as
searching,
counting,
concordancing,
hypothesis
testing,
and
explanation.
For
example,
as
will
be
demonstrated
in
Chapter
13,
the
TagLog
Explanation
Tool
can
be
used
for
debugging
of
theories
used
for
tagging,
as
it
stands.
As
I
have
mentioned
already,
since
transformation-based
tagging
can
only
be
implemented
outside
the
logical
framework,
with
conditionaction
rules,
there
is
a
need
for
special
purpose
mechanisms
for
maintenance
and
debugging
of
such
rules.
It
is
this
last
point
in
particular
that
shows
that
it
is
the
logic-based
path
we
want
to
tread.
So
even
if
future
practical
tests
show
logic-based
tagging
to
be
inefficient
and
therefore
impractical,
a
standard
would
have
been
set,
that
we
ought
to
try
to
live
up
to.
In
the
mean
time,
I
believe
that
a
transformation-based
tagger
implemented
by
condition-action
rules
is
a
very
useful
tool
to
have
in
TagLog.
In
contrast
to
logic-based
tagging,
Brill
s
approach
has
already
been
proved
to
work
very
well
in
practice
with
big
corpora
and
non-trivial
tagging
tasks.
11.7
Constraint-Based
Part-of-Speech
Tagging
The
logic-based
tagging
method
shows
some
similarities
with
the
Constraint
Grammar
CG
approach
Karlsson,
1990;
Karlsson
et
al.,
1994
.
Constraint-Based
Part-of-Speech
Tagging
203
In
CG,
a
constraint
is
a
quadruple.
Karlsson
1990
gives
the
example
in
225
.
225
w
0
PREP
-1
DET
This
constraint
has
the
semantics
that
if
a
word
w
has
a
reading
out
of
a
set
of
several
possible
readings
with
the
feature
PREP,
this
very
reading
is
discarded
0
iff
the
preceding
word
i.e.
the
word
in
position
-1
has
a
reading
with
the
feature
DET
Karlsson
ibid.,
p.
170
.
Although
the
constraints
in
CG
are
claimed
to
be
declarative,
and
thus
to
embody
true
statements,
reading
them
in
the
above
manner
gives
them
a
procedural
ring.8
In
TagLog,
a
constraint
with
the
same
effect
as
the
one
in
225
can
be
given
as
follows
instead:
226
syncat
P1-P2,prep
:-
syncat
P0-P1,det
.
This,
indeed,
is
a
declarative
constraint.
To
see
what
it
would
mean
to
use
TagLog
s
condition-action
rules
for
part-of-speech
tagging
in
the
CG
manner,
consider
the
following
rule:
227
syncat
P0-P1,det
delete
syncat
P1-P2,prep
.
The
use
of
condition-action
rules
imposes
a
certain
processing
strategy
on
the
CG
method,
however,
and
an
order-dependence
between
rules
that
CG
people
would
perhaps
claim
that
CG
ought
not
to
have,
but
it
might
still
be
a
method
that
a
TagLog
user
would
find
useful,
in
certain
circumstances.
Karlsson
presents
a
number
of
mechanisms
that
are
supposed
to
make
it
easier
for
the
rule
writer
to
construct
effective
rules
indeed,
as
we
will
see
below,
Karlsson
s
approach
presupposes
a
rule
writer
.
In
my
view,
these
mechanisms
look
unprincipled
and
ad
hoc.
If
they
are
reconstructed
8.
Another
problem
is
that
it
is
not
sufficiently
clear
what
is
meant
by
a
set
of
readings
and
how
they
relate
to
conditions.
Since
the
CG
framework
is
supposed
to
be
declarative,
we
should
be
able
to
interpret
them
as
statements
the
TagLog
way
.
Now
if
there
is
more
than
one
reading
for
a
word
which
is
often
the
case
,
we
could
either
interpret
them
disjunctively,
or
we
could
interpret
them
conjunctively.
Suppose
we
interpret
them
disjunctively.
It
would
not
then
be
clear
what
is
meant
by
a
condition
being
satisfied
by
the
local
context.
A
disjunction
of
statements
cannot
of
course,
by
themselves,
satisfy
an
atomic
condition.
Suppose
we
interpret
them
conjunctively.
This
would
be
the
TagLog
approach,
which
works
well,
but
is
this
the
interpretation
that
Karlsson
intends?
204
Automatic
Tagging
in
logic
we
might
be
able
to
see
how
they
can
be
improved,
or
at
least
more
clearly
formulated.
For
example,
the
C
in
the
constraint
in
228
is
supposed
to
mean
that
the
assignment
of
TO
to
the
preceding
word
is
unique:
228
w
0
VFIN
-1C
TO
Needless
to
say,
this
is
not
hard
to
say
in
logic
either:
229
syncat
P1-P2,vfin
:syncat
P0-P1,inf
,
not
syncat
P0-P1,V
,
V
inf
.
In
CG
the
operator
!
indicates
that
the
target
reading
is
the
correct
one
iff
all
context
conditions
are
satisfied,
all
other
readings
are
discarded
Karlsson,
1990,
p.
170
.
It
seems
to
me,
though,
that
a
constraint
such
as
230
w
!
VINF
-1
TO
would
be
better
expressed
as
231
syncat
P1-P2,C
:syncat
P0-P1,to
,
C
vinf.
so
that
the
uniform
negative
format
of
the
constraints
is
kept.
In
CG,
a
template
mechanism
is
available
for
expressing
partial
generalizations.
Karlsson
s
example
is
that
a
template
NP
could
be
declared
to
contain
the
alternatives
N
,
A
N
,
DET
N
,
DET
A
N
,
etc.
This
is
clumsy
and
ad
hoc,
and
it
does
not
allow
recursion.
In
TagLog,
the
user
can
write
grammar
rules,
and
define
some
of
the
conditions
of
his
constraints
in
terms
of
what
follows
deductively
from
such
grammars.
So
instead
of
the
template
above,
we
can
have
the
recursive
X-bar
style
grammar
in
233
.
232
233
syncat
P0-P1,det
:-
syncat
P1-P2,np
.
np
--
det,
n1.
np
--
n1.
n1
--
n.
n1
--
adj,
n1.
This
is
more
general
and
what
is
more
it
is
pure
logic.
Voutilainen
et
al.
1992
tested
the
CG
system
on
running
English
text,
with
a
rule
base
consisting
of
1,100
constraints.
They
report
that
while
roughly
one
word
out
of
two
is
morphologically
ambiguous,
after
disam-
Summary
and
Conclusion
205
biguation
94-97
were
fully
unambiguous.
Of
all
word
forms
in
the
output,
99.7
have
the
most
appropriate
reading
left.
This
is
impressive.
The
system
is
efficient
too,
according
to
tests
performed
by
the
developers.
I
think
I
have
shown
above,
however,
that
certain
logical
unclarities
about
the
whole
approach
remain.
The
Constraint
Grammar
approach
is
in
need
of
a
logical
reconstruction,
and
perhaps
my
treatment
in
this
chapter
could
be
regarded
as
a
first
attempt
at
doing
this.
Creating
a
system
of
constraints
in
CG
is
hard
manual
work
since
no
learning
mechanism
is
available.
However,
I
believe
that
a
comparable
result
can
be
achieved
with
a
system
of
constraints
automatically
acquired
from
corpora.
Using
surface-directed,
local
constraints
but
without
using
automatic
learning
seems
to
me
to
be
a
less
than
optimal
way
of
doing
things.
In
this
respect,
CG
as
well
as
the
logic-based
approach
is
unbalanced.
11.8
Summary
and
Conclusion
The
TagLog
framework
is
compatible
with
many
methods
of
automatic
tagging.
I
have
concentrated
on
two
techniques
that
seem
to
be
more
suitable
than
others:
Brill
s
transformation-based
technique,
and
a
novel
logic-based
technique.
Although
it
cannot
be
said
to
be
logic-based
in
itself,
Brill
s
transformation-based
technique
works
very
well
in
a
logic-based
environment.
Implementation
is
very
simple,
and
the
result
is
a
more
flexible,
highlevel
system
than
Brill
s
own.
This
chapter
also
took
a
small
step
towards
a
logic-based
approach
to
tagging,
and
showed
that
this
approach
is
well
worth
exploring
further.
CHAPTER
12
Hypothesis
Testing
All
grammars
leak.
Edward
Sapir
12.1
Introduction
How
good
is
my
hypothesis?
How
good
is
my
theory?
Should
I
keep
it?
Should
I
throw
it
away?
The
testing
of
hypotheses
and
theories
is
a
very
important
aspect
of
the
work
of
the
scientist,
and
no
less
so
if
he
happens
to
be
a
linguist.
According
to
Popper,
it
is
testing,
the
possibility
of
falsification,
that
distinguishes
science
from
non-science.
This
chapter
is
about
a
tool
in
TagLog
the
TagLog
Hypothesis
Testing
Tool
that
to
some
extent
automates
the
testing
of
hypotheses
and
theories,
gives
a
comprehensive
overview
of
the
results
of
the
testing,
and
supports
the
revision
of
hypotheses
and
theories
in
the
light
of
these
results.
The
Logic
of
Hypothesis
Testing
207
12.2
The
Logic
of
Hypothesis
Testing
I
will
assume
that
four
different
kinds
of
instances
of
a
universally
quantified
conditional
are
relevant
to
its
status
as
a
hypothesis.
Given
a
statement
S
of
the
form
A
:-
B,
and
given
that
A
and
B
are
ground
instances
of
A
and
B,
respectively,
A
:-
B
is
a
confirming
instance
of
S
iff
both
A
and
B
are
true,
a
falsifying
instance
of
S
iff
B
is
true
but
A
is
false,
a
predictive
instance
of
S
iff
B
is
true
and
A
is
unknown,
and
a
non-covered
instance
of
S
iff
A
is
true
but
B
is
not.
If
we
assume
the
truth
of
what
is
provable,
and
if
we
assume
that
what
has
been
described
as
observed
is
also
true,
then,
in
terms
of
the
meta-predicates
defined
in
Chapter
10,
these
notions
might
be
defined
as
follows:
234
confirming
instance
A
:-
B
:-
provable
B
,
observed
A
.
falsifying
instance
A
:-
B
:-
provable
B
,
observed
A
.
predictive
instance
A
:-
B
:-
provable
B
,
unobserved
A
.
noncovered
instance
A
:-
B
:-
observed
A
,
unprovable
B
.
Now,
as
their
form
may
suggest,
the
definitions
in
234
will
be
used
in
the
TagLog
system:
for
searching
and
concordancing,
of
course,
but
mainly
for
counting.
12.3
Measuring
the
Status
of
a
Hypothesis
Based
on
the
distribution
of
confirming,
falsifying,
predictive,
and
noncovered
instances
of
a
universal
statement,
I
have
made
an
attempt
to
operationalize
the
notions
of
coverage
and
truthlikeness.
As
we
will
see,
I
will
also
define,
and
operationalize,
a
notion
of
conclusiveness.
The
way
I
will
do
this
is
certainly
subject
to
change,
but
it
should
be
clear
that
what
I
want
to
find
is
a
set
of
measurable
notions
that
the
system
can
use
in
order
to
communicate
the
status
of
a
hypothesis
to
the
user
so
as
to
bring
about
the
further
development
of
the
current
theory,
and
possibly
to
give
that
development
a
push
in
the
right
direction.
Such
notions
must
be
intuitive,
be
possible
to
operationalize,
and
preferably
be
easy
to
visualize.
I
have
designed
them
in
such
a
way
that
each
measure
can
be
208
Hypothesis
Testing
expressed
as
a
real
number
between
0
and
1.
Thus,
as
we
will
see,
they
can
be
vizualized
as
circle
diagrams.
These
tests
certainly
do
not
tell
us
all
there
is
to
know
about
a
universal
statement.
A
statement
may
score
100
in
truthlikeness,
100
in
coverage,
and
100
in
conclusiveness,
but
still
be
trivial
and
uninteresting.
Unfortunately,
I
can
think
of
no
way
to
measure
interestingness
that
would
fit
here.
12.3.1
Coverage
Intuitively,
the
coverage
of
a
universal
statement
is
the
extent
to
which
it
predicts
or
explains
the
occurrences
of
the
particular
phenomenon
that
it
purports
to
predict
or
explain.
A
measure
of
coverage
of
a
universal
conditional
is
simply
defined
by
taking
the
number
of
its
covered
instances,
i.e.
the
number
of
confirming
instances
plus
the
number
of
falsifying
instances
plus
the
number
of
predictive
instances,
and
divide
it
by
the
total
number
of
all
its
instances,
i.e.
by
the
number
of
covered
instances
plus
the
number
of
non-covered
instances.
12.3.2
Truthlikeness
In
Chapter
2,
some
intuitions
behind
the
notion
of
nearness
to
truth,
or
truthlikeness,
were
briefly
discussed.
In
an
attempt
to
make
this
more
precise,
we
say
that
the
truthlikeness
of
a
universally
quantified
conditional
is
obtained
by
dividing
the
number
of
its
confirming
instances
with
the
number
of
its
confirming
instances
plus
the
number
of
its
falsifying
instances.
12.3.3
Conclusiveness
Note
that
our
definition
of
truthlikeness
did
not
say
anything
about
predictive
instances.
This
is
because
as
long
as
a
universal
conditional
has
predictive
instances,
we
do
not
really
know
its
truthlikeness,
since
we
do
not
know
whether
the
predictive
instances
would
turn
out
to
be
confirming
or
falsifying,
if
we
were
to
investigate
them
more
closely.
Thus
it
seems
that
we
need
some
measure
of
confidence
in
the
measure
of
truthlikeness
that
we
have
obtained
in
a
particular
case.
I
have
designed
such
a
measure,
and
I
have
called
it
conclusiveness.
The
TagLog
Hypothesis
Testing
Tool
209
This
measure
is
defined
as
the
ratio
between
the
number
of
confirming
instances
plus
the
number
of
falsifying
instances,
and
the
number
of
covered
instances
i.e.
the
number
of
confirming
instances
plus
the
number
of
falsifying
instances
plus
the
number
of
predictive
instances
.
12.4
The
TagLog
Hypothesis
Testing
Tool
The
TagLog
Hypothesis
Testing
Tool
is
designed
to
provide
the
researcher
with
a
measure
of
the
degree
of
success
with
which
a
general
hypothesis
describes
the
relevant
facts
of
a
text,
based
on
the
proportions
between
the
numbers
of
confirming,
falsifying,
predictive
and
non-covered
instances
of
the
universal
statement
expressing
the
hypothesis.
The
proportions
are
presented
in
two
ways:
in
the
form
of
a
table,
and
in
three
circle
diagrams.
The
tool
also
allows
the
researcher
to
search
for
the
segments
involved
in
these
different
kinds
of
instances.
The
dynamic
link
to
the
Statement
Browser
makes
it
easy
to
inspect
the
descriptions
of
the
segments,
and
modify
them
if
necessary.
Selecting
the
Hypothesis
Testing
Tool
from
the
Tool
menu
brings
up
the
dialog
box
in
Figure
49,
containing
one
scrollable
text
entry
field,
one
table,
four
buttons
and
one
diagram
drawing
area.
FIGURE
49.
The
TagLog
Hypothesis
Testing
Tool
The
universal
statement
for
which
the
user
wishes
to
run
statistics
is
entered
in
the
text
entry
field.
210
Hypothesis
Testing
Run:
A
click
on
this
button
runs
the
statistics
for
the
universal
statement
in
the
text
entry
field,
prints
the
resulting
figures
in
the
table,
and
draws
the
corresponding
circle
diagrams
in
the
diagram
area.
First:
Clicking
this
button
finds
the
first
occurrence
of
the
instance
type
highlighted
in
the
table
i.e.
confirming,
falsifying,
predictive,
or
noncovered
and
selects
the
occurrence
in
the
Text
window.
Besides,
clicking
this
button
enables
two
other
buttons:
Next:
Finds
the
next
occurrence
of
the
instance
type
highlighted
in
the
table.
Subsequent
clicks
on
this
button
will
find
the
rest
of
the
instances,
one
at
a
time.
Reset:
Returns
to
top
level.
Explain:
Activates
the
link
to
the
TagLog
Explanation
Tool
see
Section
13.7
.
An
attempt
to
explain
the
consequent
of
the
current
hypothesis
as
it
applies
to
the
current
segment
is
then
made.
12.5
Interpreting
the
Results
of
Hypothesis
Testing
The
result
of
a
Hypothesis
Testing
run
says
something
about
the
universal
statement
in
question,
and
it
suggests
actions
to
be
taken
in
order
to
improve
the
current
theory,
actions
such
as
checking,
modifying
or
extending.
12.5.1
The
significance
of
predictive
instances
As
defined
above,
the
predictive
instances
are
the
instances
that
claim
something
about
some
of
the
segments
of
the
corpus,
but
where
the
system
does
not
know
whether
what
they
claim
is
true
or
not.
It
is
up
to
the
researcher
to
check
the
predictions,
i.e.
it
is
the
responsibility
of
the
researcher
to
determine,
on
the
basis
of
observation,
whether
a
particular
prediction
is
successful
or
not,
i.e.
whether
the
consequent
of
a
particular
predictive
instance
of
a
universal
statement
is
true
or
not.
If
it
is
found
to
be
true,
the
instance
becomes
a
confirming
instance,
but
if
it
is
found
to
be
false,
it
becomes
a
falsifying
instance.
If
the
prediction
is
false,
the
researcher
should
probably
consider
the
kind
of
action
suggested
for
falsifying
instances,
explained
below.
Interpreting
the
Results
of
Hypothesis
Testing
211
Suppose
that
we
have
run
the
statistics
for
a
particular
universal
statement
and
that
we
have
selected
predictive
in
the
tool
s
table.
In
order
to
check
and
possibly
confirm
disconfirm
the
predictions
made,
perform
the
following
steps:
1.
Click
First.
The
system
will
select
the
segment
involved
in
the
first
predictive
instance
found
by
the
system.
2.
Determine,
on
the
basis
of
observation,
whether
the
consequent
is
true
or
false
of
this
segment.
3.
Regardless
of
whether
the
prediction
is
true
or
not,
it
might
be
a
good
idea
at
this
stage
to
explicitly
confirm
or
disconfirm
it,
by
introducing,
in
the
current
theory,
an
explicit
observation
statement
describing
the
selected
segment.
The
consequence
is
that,
if
we
run
the
statistics
again,
the
instance
will
be
counted
as
confirming
or
falsifying
rather
than
predictive.
4.
Click
Next
in
order
to
check
the
rest
of
the
predictive
instances.
12.5.2
The
significance
of
confirming
instances
By
definition,
a
confirming
instance
of
a
universal
statement
is
an
instance
where
both
the
antecedent
and
the
consequent
are
true.
But
as
Karl
Popper
1959
,
among
others,
has
told
us,
not
even
a
zillion
confirming
instances
of
each
of
its
statements
can
conclusively
verify
a
theory.
There
is
always
a
chance
that
we
will
find
an
instance
that
breaks
the
rule,
and
smashes
our
theory
to
pieces.
Still,
a
statement
that
is
sufficiently
confirmed
is
usually
accepted,
at
least
tentatively,
and
our
confidence
in
a
hypothesis
is
likely
to
increase
as
the
number
of
confirming
instances
increases.
Moreover,
as
the
number
of
confirming
instances
increases
other
things
being
constant
the
degree
of
truthlikeness
increases
as
well.
In
contrast
to
the
other
types
of
instances,
confirming
instances
do
not
really
prompt
the
researcher
to
any
action
at
all.
He
may
rest
in
the
confidence
created
by
them.
12.5.3
The
significance
of
falsifying
instances
Again
by
definition,
the
falsifying
instances
of
a
universal
statement
are
the
instances
that
are
false,
i.e.
where
the
antecedent
is
true
but
the
consequent
false.
212
Hypothesis
Testing
If
a
theory
is
falsified
because
one
of
its
statements
has
a
falsifying
instance,
the
researcher
does
not
have
to
throw
all
of
it
away.
But
the
existence
of
a
falsifying
instance
suggests
that
it
would
be
wise
to
modify
something.
There
are
at
least
three
possibilities:
1.
Change
the
theory.
Remove
the
universal
statement
which
has
a
falsifying
instance
from
the
theory,
and
thus
save
the
rest
of
it.
Or
find
some
way
of
specializing
the
universal
statement
in
question,
so
that
it
no
longer
has
falsifying
instances.
2.
Change
the
description
of
data.
The
description
contradicting
the
conclusion
of
the
universal
statement
can
be
a
incorrect.
Correct
it,
and
thus
make
the
falsifying
instance
disappear.
3.
Of
course,
practically
speaking,
there
is
also
a
third
possibility
open
to
the
researcher,
namely
to
ignore
the
fact
that
the
theory
is
falsified.
He
can
even
justify
this
manoeuvre
by
pointing
out
the
fruitfulness
of
what
he
has,
in
spite
of
its
falsity,
or
he
can
promise
that
soon,
very
soon
now,
this
apparent
contradiction
will
be
resolved,
but
until
then,
let
s
hang
on
to
this
idea...
.
As
Popper
1992
puts
it
,
some
degree
of
dogmatism
is
useful,
even
in
science
.
12.5.4
The
significance
of
non-covered
instances
A
confirming
outcome
of
the
confirming
and
predictive
components
of
testing
is
not
sufficient
in
itself,
because
it
may
be
that
the
statements
confirm
or
successfully
predict
only
trivial
cases
and
leave
the
not
so
trivial
cases
still
to
be
explained.
This
is
the
problem
of
insufficient
coverage.
A
maximally
successful
theory
must
generate
only
true
statements
of
the
relevant
domain,
but
it
must
also
generate
all
true
statements
of
this
domain,
at
least
as
long
as
they
are
relevant.
The
Hypothesis
Testing
Tool
supports
a
practical
method
for
extending
the
coverage
of
a
particular
universal
statement.
The
first
step
is
to
check
the
cases
where
it
fails
to
cover.
1.
Select
Non-covered
in
the
table.
2.
Click
the
Find
button
in
order
to
find
the
first
segment
involved
in
a
non-covered
instance
of
the
statement.
3.
Extend
the
theory
so
as
to
cover
the
selected
case.
4.
Repeat
until
there
are
no
non-covered
instances
left
to
be
found.
Testing
a
Simple
Hypothesis
213
12.5.5
Summing
up
significance
In
practice,
we
will
probably
have
to
learn
to
live
with
theories
that
are
known
to
contain
some
false
statements
and
known
to
fail
to
contain
some
true
statements.
Most
theories
are
falsified.
Most
theories
have
less
than
total
coverage.
The
important
thing
for
the
researcher
is
to
be
aware
of
falsifying
and
non-covered
instances
if
there
are
any.
It
is
only
if
the
researcher
is
aware
of
them
that
they
can
prompt
further
development
of
the
theory.
In
this
sense,
TagLog
can
free
the
researcher
from
ignorance,
but
I
do
not
think
TagLog,
or
any
such
system,
can
ever
free
the
researcher
from
the
responsibility
of
deciding
and
acting
correctly
on
the
basis
of
what
he
knows.
Science
is
not
only
a
question
of
logic.
12.6
Testing
a
Simple
Hypothesis
In
order
to
demonstrate
the
idea
of
automatic
hypothesis
testing,
we
will
work
with
a
simple,
almost
trivial,
example.
The
example
is
based
on
the
PLUS
corpus
text,
but
I
have
deliberately
introduced
some
errors
and
a
certain
amount
of
incompleteness
in
the
description,
in
order
to
provide
us
with
more
interesting
results.
Suppose
the
hypothesis
that
we
want
to
test
can
be
expressed
in
natural
language
as
in
235
,
and
thus
be
formalized
as
in
236
.
235
236
Every
instance
of
the
string
and
is
a
conjunction
syncat
S,conj
:-
bstring
S,
and
We
enter
the
formula
in
236
in
the
Hypothesis
Testing
Tool
s
text
entry
field,
and
then
click
the
Run
button
to
run
the
statistics.
The
outcome
is
shown
in
Figure
50.
214
Hypothesis
Testing
FIGURE
50.
Testing
a
simple
hypothesis
The
table
tells
us
that
there
are
two
segments
in
the
current
text
that
are
instances
of
the
string
and
and
at
the
same
time
instances
of
the
lexical
category
conjunction
.
Thus,
these
are
two
cases
that
confirm
the
hypothesis.
But
there
is
also
one
occurrence
of
and
which
is
provably
not
a
conjunction,
and
which
therefore
falsifies
the
hypothesis.
Furthermore,
there
is
one
predictive
instance
of
the
hypothesis,
i.e.
one
case
where
the
hypothesis
predicts
of
an
occurrence
of
and
that
it
is
a
conjunction.
In
this
case,
however,
the
hypothesis
can
neither
be
confirmed,
nor
falsified,
on
the
basis
of
what
the
system
knows.
Upon
closer
inspection,
it
may
turn
out
to
be
either
confirming
or
falsifying.
Finally,
the
high
number
of
non-covered
instances,
or
rather
the
ratio
between
the
number
of
covered
instances
and
the
number
of
covered
instances
plus
the
number
of
noncovered
instances,
tells
us
that
the
coverage
is
low,
which
should
come
as
no
surprise
since
there
are
conjunctions
other
than
and
.
With
only
two
confirming
cases
and
one
falsifying
case,
one
predictive
case,
and
three
cases
not
covered
at
all,
our
hypothesis
appear
to
be
rather
weak.1
Either
the
hypothesis
does
not
get
the
facts
of
the
text
right,
or
these
facts
have
not
been
completely
and
truthfully
described.
Aided
by
the
table
and
the
diagrams,
we
are
in
a
position
to
make
wellinformed
decisions
of
how
to
modify
either
the
hypothesis,
and
hopefully
1.
The
figures
are
very
small
since
we
are
working
with
only
one
short
text,
but
it
will
do
for
the
purpose
of
demonstration.
Testing
a
Simple
Hypothesis
215
improve
upon
it,
or
the
description
of
the
facts,
in
the
hope
of
showing
that
the
hypothesis
is
not
as
bad
as
it
first
seemed.
We
begin
by
looking
at
the
confirming
cases.
After
all,
we
ought
to
check
that
they
have
not
been
confirmed
for
the
wrong
reasons!
We
select
Confirming
in
the
table
and
click
the
First
button
telling
the
system
to
select
the
first
confirming
instance.
The
result
shows
in
the
screen
in
Figure
51.
FIGURE
51.
Testing
a
simple
hypothesis
cont.
By
observing
the
selected
segment
as
it
appears
in
the
text,
and
by
looking
at
the
Statement
Browser,
we
are
able
to
confirm
that
the
instance
in
question
is
a
confirming
one.
Now,
let
us
inspect
the
falsifying
instances
of
our
hypothesis,
to
see
if
we
can
modify
the
hypothesis,
or
perhaps
the
description
of
data,
so
as
to
make
the
falsifying
instance
disappear,
or
if
we
can
leave
it
as
it
is,
fully
aware
that
it
is
false,
and
remain
happy
anyway.
When
we
select
Falsifying
in
the
table,
and
click
the
Find
button,
the
screen
displays
what
is
shown
in
Figure
52.
216
Hypothesis
Testing
FIGURE
52.
Testing
a
simple
hypothesis
cont.
Note
how
clearly
the
Statement
Browser
reveals
the
source
of
the
falsifying
instance:
Due
to
a
mistake
during
the
description
phase
a
slip
of
the
mouse
during
selection
from
a
menu,
say
,
the
selected
instance
of
and
was
wrongly
described
as
an
adjective!
Thus,
the
action
called
for
here
is
not
a
modification
of
the
hypothesis,
but
a
modification
of
the
description
of
data.
We
delete
the
false
statement
by
selecting
it
in
the
Statement
Browser
and
pressing
the
Backspace
key
and
we
add
a
new
statement
saying
that
the
selected
segment
is
a
conjunction
by
selecting
from
the
Value
menu
the
normal
procedure
.
If
we
re-run
the
statistics
this
is
not
necessary
at
this
stage,
but
let
us
do
it
anyway
,
the
table
in
Figure
53
is
displayed.
Testing
a
Simple
Hypothesis
217
FIGURE
53.
Testing
a
simple
hypothesis
cont.
In
order
to
check
the
prediction,
i.e.
in
order
to
inspect
the
predictive
instance
and
determine
the
truth
value
of
what
it
predicts,
we
perform
the
same
steps
as
in
the
case
of
the
falsifying
instance,
but
this
time
with
Predictive
selected
in
the
table.
Figure
54
shows
the
resulting
screen.
FIGURE
54.
Testing
a
simple
hypothesis
cont.
The
Statement
Browser
gives
us
a
reason
to
suspect
that
we
have
simply
forgotten
to
describe
the
selected
instance
of
and
!
We
repair
the
omis-
218
Hypothesis
Testing
sion,
and
confirm
the
prediction,
by
means
of
a
selection
from
the
Value
menu
again.
Now,
if
we
re-run
the
test,
the
table
and
diagrams
looks
as
in
Figure
55.
FIGURE
55.
Testing
a
simple
hypothesis
cont.
Finally,
suppose
that
we
want
to
improve
upon
the
coverage
of
the
hypothesis.
The
first
step
is
to
check
the
cases
where
it
fails
to
cover.
We
select
Non-covered
in
the
table,
and
click
the
Find
button
in
order
to
find
the
first
segment
which
is
described
as
a
conjunction,
but
which
is
not
an
instance
of
the
string
and
.
The
screen
now
looks
as
in
Figure
56.
FIGURE
56.
Testing
a
simple
hypothesis
cont.
Testing
a
Grammar
Rule
219
We
have
found
an
instance
of
the
string
or
which
is
described
as
a
conjunction,
and
if
we
continue
by
clicking
the
Next
button
we
will
find
instances
of
but
described
as
conjunctions
too.
This
suggests
that
we
should
modify
our
hypothesis
so
as
to
cover
these
cases:
237
syncat
S,conj
:-
bstring
S,
and
;
bstring
S,
or
;
bstring
S,
but
When
we
re-run
the
statistics
with
237
in
the
text
entry
field,
the
tables
and
diagrams
display
as
in
Figure
57.
FIGURE
57.
Testing
a
simple
hypothesis
cont.
What
we
have
seen,
through
this
simple
example,
is
how
the
TagLog
Hypothesis
Testing
Tool
is
able
to
support
an
interactive
theory
revision
process,
in
which
the
system
does
the
searching
and
counting,
and
presents
the
results
in
a
way
that
provides
hints
for
what
should
be
done
next,
whereas
it
is
the
user
who
is
responsible
for
the
checking
and
confirmation
disconfirmation
of
predictions,
and
for
performing
modifications
when
the
need
for
modifications
arises.
12.7
Testing
a
Grammar
Rule
Figure
58
shows
a
slightly
less
trivial
example,
in
which
grammar
rule
notation
is
used
to
express
the
hypothesis
to
be
tested.
A
rule
describing
the
hypothesis
that
a
segment
which
is
an
instance
of
a
pronoun
or
a
proper
noun,
or
which
consists
of
a
determiner
followed
by
a
noun,
is
220
Hypothesis
Testing
always
a
noun
phrase,
has
been
entered
in
the
query
box
and
the
Run
button
has
been
clicked.
Note
that
the
X-bar
grammar
in
the
background
is
consulted
during
testing.
The
relevant
part
of
the
background
theory,
i.e.
the
current
theory
minus
the
hypothesis,
must
be
assumed
to
be
true,
or
else
there
would
be
no
guarantee
that
the
figures
and
the
pie-charts
reflect
the
status
of
the
hypothesis
correctly.
FIGURE
58.
Grammar-rule
testing
in
TagLog
As
before,
the
resulting
table
and
circle
diagrams
show
the
degree
of
success
of
the
hypothesis.
The
user
has
decided
to
inspect
the
predictive
instances
in
order
to
check
the
prediction,
and
has
therefore
selected
the
corresponding
row
in
the
table
and
clicked
the
First
button,
so
that
the
instance
in
question
has
been
selected
in
the
Text
window.
The
next
logical
step
is
to
confirm
or
disconfirm
the
prediction
by
selecting
the
appropriate
tag
from
the
menus
in
the
usual
way.
Tracing
through
the
cases
not
covered
by
the
grammar,
the
user
would
find
cases
of
noun
phrase
coordination,
noun
groups
forming
noun
Summary
and
Conclusion
221
phrases,
noun
phrases
containing
adjective-modifying
adverbs,
and
chains
of
determiners.
He
would
have
to
add
rules
to
cover
these.
12.8
Summary
and
Conclusion
In
this
chapter,
the
significance
of
confirming,
falsifying,
predictive,
and
non-covered
instances
of
a
hypothesis
expressed
as
a
universal
statement
was
explained,
and
related
to
the
notions
of
truthlikeness,
conclusiveness,
and
coverage,
and
to
the
notions
of
checking,
confirmation,
disconfirmation,
and
modification
of
hypotheses.
I
have
also
presented
one
of
the
tools
available
in
the
TagLog
system
the
TagLog
Hypothesis
Testing
Tool
a
tool
for
measuring
the
empirical
support
for
universal
statements.
I
have
shown
how
such
a
tool,
based
on
the
notions
mentioned
above,
implemented
in
a
framework
of
computational
logic,
and
equipped
with
a
graphical
user
interface,
can
support
the
linguist
in
his
work.
CHAPTER
13
Explanation
Men
do
not
think
they
know
a
thing
till
they
have
grasped
the
why
of
it.
Aristotle
13.1
Introduction
As
was
noted
in
the
introductory
chapters,
the
theoretical
linguist
is
not
necessarily
content
with
knowing
what
syntactic
properties
a
particular
text
segment
has,
knowing
what
it
means,
or
knowing
its
function.
The
peculiar
linguistic
attitude
towards
features
of
texts
amounts
to
trying
to
explain
them,
rather
than
just
stopping
at
simple
descriptions.
Explanation
is
one
of
the
most
important
notions
in
the
philosophy
of
science.
In
this
section,
I
discuss
and
demonstrate
the
role
played
by
explanation
in
the
TagLog
system,
a
discussion
that
to
a
certain
extent
will
draw
on
the
philosophy
of
science.
It
is
a
fact
of
human
psychology
that
we
are
not
capable
of
knowing
all
the
logical
consequences
of
what
we
claim.
In
particular,
the
use
of
universal
implicational
statements
in
TagLog
means
that
the
truth
of
certain
Some
Preliminaries
of
Explanation
223
statements
may
come
as
a
surprise
to
the
researcher,
even
when
they
follow
deductively
from
claims
that
he
has
made
himself.
Another
fact
of
human
psychology
is
that
we
tend
to
demand
explanations
of
statements
that
surprise
us.
Thus,
a
system
such
as
TagLog
ought
to
be
able
to
explain
its
conclusions
to
the
user.
Fortunately,
not
only
is
a
computer
good
at
reasoning
from
premises
to
conclusions,
it
can
also
be
made
to
produce
explanations
of
how
it
reached
the
conclusions.
Such
explanations
can
take
the
form
of
presentations,
in
one
form
or
another,
of
the
proofs
of
the
conclusions.
Indeed,
as
we
will
see
in
this
chapter,
the
user
of
TagLog
is
always
free
to
demand
an
explanation
of
a
particular
statement
implied
by
the
current
theory,
and
it
is
the
task
of
the
system
to
provide
it.
Note
that
the
term
explanation
is
used
in
two
ways
here:
the
theorist
explaining
a
phenomenon
to
the
rest
of
the
scientific
community,
and
the
TagLog
system
explaining
a
statement
to
the
user.
They
have
things
in
common,
no
doubt,
but
they
should
not
be
confused.
From
a
practical
point
of
view,
explanation
in
the
latter
sense
is
clearly
something
that
is
useful,
for
theory
debugging
purposes
including
debugging
of
grammars
,
and
in
general
for
tracing
out
the
connectedness
between
observations.
Moreover,
since
a
facility
for
explaining
statements
to
the
user
seems
to
be
able
to
promote
an
awareness,
and
perhaps
also
a
deeper
understanding,
of
some
of
the
implications
of
particular
theoretical
formulations,
it
may
be
able
to
help
the
user
to
develop
explanatory
theories
in
the
former
sense
as
well.
13.2
Some
Preliminaries
of
Explanation
The
view
I
take
of
explanation
is
a
fairly
formal
one,
and
in
essence
it
is
the
one
taken
by,
for
instance,
Hempel
Oppenheim
1948
.
According
to
this
view,
explanation
is
related
to
deducibility;
it
involves
the
deduction
of
the
explanandum
from
a
body
of
statements
explanans
.
More
specifically,
the
TagLog
notion
of
explanation
is
the
following:
Given
is
a
set
of
TagLog
statements,
forming
a
theory.
Among
these
statements
is
a
set
that
describes
a
particular
situation
e.g.
a
particular
object,
event,
or
state,
or
a
particu-
224
Explanation
lar
segment
of
text
.
One
or
more
of
these
statements
are
singled
out
as
demanding
explanation.
Explanation
is
provided
by
showing,
by
deduction,
that
a
theory
just
like
the
one
in
question,
but
which
does
not
contain
the
statements
at
issue,
would
entail
these
statements.
Let
us
for
the
moment
assume
that
the
description
of
the
particular
situation
is
complete,
and
that
every
relevant
fact
is
specified.
Later
in
Section
13.5
,
I
intend
to
show
that
this
assumption
can
be
abandoned,
if
abduction
rather
than
deduction
is
recognized
as
the
major
reasoning
strategy
behind
explanation
and
that
then,
in
fact,
deductive
explanation
becomes
a
special
case
of
abductive
explanation
.
Since
the
TagLog
notion
of
explanation
in
some
respects
covers
more,
but
in
other
respects
less,
than
some
other
notions
of
explanation,
implicit
in
common
sense
or
explicated
by
philosophers
of
science,
a
number
of
comments
are
in
order.
First,
it
must
be
made
clear
that
is
not
meant
to
capture
the
common
sense
notion
of
explanation.
How
people
actually
explain
things
is
of
little
concern
to
us
here.
Our
notion
of
explanations
is
a
rational
reconstruction
cf.
Kim,
1967
,
and
should
not
be
expected
to
conform
to
our
ordinary
notion
of
explanation
at
every
point.
Nevertheless,
238
is
given
as
an
example
of
a
kind
of
explanation
that
is
meant
to
rationally
reconstruct
.
238
U:
What
s
that?
S
:
That
s
an
NP
U:
WHY
is
that
an
NP?
S
:
It
consists
of
a
DET
followed
by
a
NOUN
To
come
back
to
,
the
formulation
but
which
does
not
contain
the
statements
at
issue
is
there
to
rule
out
trivial
proofs,
of
the
from
A
follows
A
kind,
that
cannot
be
said
to
explain
anything
at
all.
That
is,
the
idea
is
to
dispense
with
explanations
of
the
kind
given
by
S
in
the
last
utterance
in
239
.
239
U:
What
s
that?
S
:
That
s
an
NP
U:
WHY
is
that
an
NP?
S
:
It
is
an
NP
Some
Preliminaries
of
Explanation
225
An
explanation
in
the
sense
of
can
of
course
only
be
relative
to
the
formal
system
that
provides
the
framework
for
description
and
questioning,
in
our
case,
a
TagLog
theory.
Attempts
to
transcend
the
framework,
such
as
the
attempt
made
by
U
in
the
last
utterance
of
240
,
obviously
cannot
be
supported.
S
provides
the
only
explanation
there
is,
given
say
a
simple
phrase
structure
grammar.
240
U:
What
s
that?
S
:
That
s
an
NP
U:
WHY
is
that
an
NP?
S
:
It
consists
of
a
DET
followed
by
a
NOUN
U:
Yes,
but
WHY
is
it
an
NP?
Another
thing
worth
noting
is
that
the
formulation
in
does
not
mention
the
notion
of
causality,
or
the
notion
of
law.
Hempel,
and
many
other
philosophers
of
science,
assume
that
explanation
is
very
intimately
connected
to
these
notions.
But
then
grammars,
and
many
other
kinds
of
theories
proposed
by
theoretical
linguists,
would
have
no
explanatory
power
at
all.
Others
feel
that
would
mean
ruling
out
too
much.
It
seems
to
us
that
when
one
is
diagnosing
physical
devices,
of
course
explanations
must
be
in
terms
of
physical
causality.
But
when
we
are
working
within
an
informational
system,
such
as
language
or
mathematics,
then
the
relations
are
implicational
and
not
necessarily
causal.
Hobbs
et
al.
1993
As
I
have
already
mentioned,
the
distinction
between
descriptive
generalizations
and
definitions
is
not
visible
to
TagLog.
This
extends
to
the
explanation
mechanism;
TagLog
is
blind
as
to
whether
it
uses
a
descriptive
generalization
or
a
definition
in
order
to
explain
a
statement
in
terms
of
other
statements.
Thus,
a
dialogue
such
as
the
one
in
241
is
perfectly
possible.
241
S
:
John
is
a
bachelor.
U:
Why?
S
:
He
is
a
man
and
he
is
not
married.
Explanations
such
as
these
are
sometimes
called
pseudo
explanations.
Not
filtering
out
such
explanations
is
consistent
with
the
strategy
of
not
making
hard-to-apply
distinctions
visible
to
TagLog.
Another
hard-to-apply
distinction
that
does
not
matter
to
the
TagLog
explanation
facility
is
the
distinction
between
what
is
observable
and
226
Explanation
what
is
not.
Indeed,
I
oppose
the
idea
that
explanation
always
has
to
relate
some
observed
phenomenon
to
something
which
is
not
observable,
some
hidden
part
of
nature,
a
theoretical
construct,
or
whatever.
In
my
view,
an
explanation
of
something
observable
by
something
else
observable
is
still
a
full-blown
explanation.
The
purpose
of
an
explanation
could
be
described
as
one
of
connecting
a
set
of
seemingly
disparate
observations
making
the
set
look
coherent
so
to
speak
and
that
can
involve
something
unobservable,
but
does
not
have
to.
And
indeed,
the
TagLog
notion
of
explanation
does
not
even
preclude
an
explanation
of
something
unobservable
by
something
unobservable,
or,
for
that
matter,
something
unobservable
by
something
observable.
The
formulation
in
imposes
other
restrictions
on
what
can
be
explained
in
TagLog.
Whereas
in
principle
almost
any
statement
in
a
theory
may
be
doubted,
and
may
be
explained
by
being
shown
to
be
consequences
of
the
rest
of
the
theory,
TagLog
cannot
handle
all
cases.
In
particular,
universal
statements
cannot
be
explained
in
TagLog,
at
least
not
in
a
direct
way
they
can
be
tested,
but
not
explained
.
242
S
:
An
NP
can
consist
of
a
DET
followed
by
a
N
U:
Why?
S:?
Moreover,
by
definition,
the
axioms
of
a
particular
TagLog
formalization
cannot
be
explained
remember,
they
cannot
explain
themselves!
.
This
is
not
a
shortcoming
of
TagLog;
no
formal
system
provides
a
framework
within
which
its
own
axioms
can
be
explained.
In
TagLog,
some
of
these
axioms
are
typically
accepted
primarily
on
an
ostensive
basis
this
is
a
noun,
that
is
a
verb
phrase
so
one
could
claim
that
these
axioms
are
explained
by
something
outside
the
framework,
but
not
in
the
standard
sense
of
explanation.
Also,
note
that
far
from
all
singular
statements
in
TagLog
are
axioms:
many
observations
find
explanations
within
the
theory.
13.3
The
Logic
of
Deductive
Explanations
Naturally,
showing,
by
deduction
amounts
to
the
giving
of
a
proof.
The
TagLog
notion
of
explanation,
and
the
different
modes
of
explanation
that
The
Logic
of
Deductive
Explanations
227
TagLog
support,
are
based
on
the
notion
of
a
proof,
in
the
form
of
a
proof
tree.
It
is
straightforward
to
modify
the
reasoning
engine
presented
in
Chapter
10
so
that
it
generates
a
proof
tree
if
the
evaluation
of
a
goal
succeeds.
Collecting
successful
branches
of
the
search
tree
as
a
proof
tree
is
a
well-known
technique,
described
in
Sterling
Shapiro,
1986
for
example.
Here
is
one
version
of
such
a
modified
meta-interpreter:
243
prove
true,true
.
prove
A,A
true
:builtin
predicate
A
,
!,
call
A
.
prove
A,A
BProof
:clause
A,B
,
prove
B,BProof
.
prove
A,B
,
ProofA,ProofB
:prove
A,ProofA
,
prove
B,ProofB
.
The
code
in
243
can
be
read
as
follows:
To
prove
the
formula
true,
do
nothing;
the
resulting
proof
is
true.
To
prove
a
built-in
predicate,
execute
it;
again,
the
proof
is
true.
To
prove
a
literal
A,
find
a
clause
A
:-
B
in
the
current
theory,
and
prove
B;
the
resulting
proof
is
A
BProof,
where
BProof
is
the
proof
of
B.
Finally,
to
prove
a
conjunction
A,B
,
prove
A,
and
then
prove
B;
the
resulting
proof
is
a
structure
of
the
form
AProof,
BProof
,
where
AProof
is
the
proof
of
A
and
BProof
is
the
proof
of
B.
For
example,
given
the
theory
in
244
,
the
proofs
in
245
are
produced.
244
p.
p
:-
q,
r.
q.
r.
!-
prove
p,
Proof
.
Proof
p
true
;
Proof
p
q
true,r
true
245
As
another
example,
grammatical
this
time,
consider
the
theory
in
246
.
246
s
--
np,
vp.
np
--
pn.
vp
--
v,
np.
syncat
1-2,pn
.
syncat
2-3,v
.
syncat
3-4,pn
.
228
Explanation
The
proof
tree
in
247
is
produced.
247
!-
prove
syncat
1-4,s
,
Proof
.
Proof
syncat
1-4,s
syncat
1-2,np
syncat
1-2,pn
true,
syncat
2-4,vp
syncat
2-3,v
true,
syncat
3-4,np
syncat
3-4,pn
true
In
order
to
implement
a
procedure
for
explanation,
we
must
remember
as
was
noted
in
Section
13.2
that
trivial
proofs
are
not
considered
explanations,
and
thus
what
we
require
from
an
explanation
is
that
no
conjunct
in
the
explanandum
is
part
of
the
explanans.
The
first
clause
in
248
guarantees
that
no
statement
in
the
explanandum
is
proved
in
just
one
trivial
step
from
a
singular
statement
in
the
theory.
The
second
clause
is
there
since
we
must
be
able
to
explain
conjunctions
of
goals.
We
explain
a
conjoined
goal
by
explaining
each
of
its
conjuncts,
resulting
in
a
forest
of
trees.
248
explain
A,Proof
:prove
A,Proof
,
Proof
A
true.
explain
A,B
,
AProof,BProof
:explain
A,AProof
,
explain
B,BProof
.
For
instance,
given
the
theory
in
244
,
the
explanation
in
249
produces
only
one
solution,
and
the
attempt
at
explanation
in
250
fails
altogether.
249
250
!-
explain
p,
Proof
.
Proof
p
q
true,r
true
!-
explain
q,
Proof
.
no
13.4
Explanation
and
Presentation
A
proof
tree
is
a
rather
abstract
and
terse
representation
of
an
explanation.
To
be
useful,
some
thought
must
be
given
to
the
problem
of
presenting
an
explanation
in
a
clear
and
persuasive
way.
In
TagLog,
explanations
can
be
presented
in
different
modes.
The
choice
of
mode
is
made
by
the
user,
depending
on
the
situation
and
on
the
task
at
hand.
In
this
section,
I
Explanation
and
Presentation
229
present
the
different
modes
that
are
available
in
the
current
implementation
of
TagLog,
and
motivate
their
existence.
The
modes
that
will
be
discussed
are
the
following:
Graphical
proof
tree
Explanation
dialogue
Analysis
tree
General
statement
General
rule
13.4.1
Graphical
proof
tree
A
graphical
proof
tree
is
perhaps
the
most
direct
way
of
presenting
an
explanation.
It
is
a
picture
of
the
derivation,
with
the
conclusion
at
the
top,
and
with
the
branches
containing
premises
stretching
downwards.
The
leaves
of
a
proof
tree
consist
of
either
the
atom
true
or
the
atom
assumed
the
meaning
of
branches
terminating
in
assumed
will
be
explained
in
Section
13.6
.
Readers
familiar
with
logic
or
logic
programming
have
presumably
seen
such
trees
many
times
before.
For
example,
the
proof
trees
produced
in
245
and
247
display
as
in
Figure
59,
to
the
left
and
to
the
right,
respectively.
p
q
true
r
true
syncat
1-2,np
syncat
1-2,pn
true
syncat
1-4,s
syncat
2-4,vp
syncat
2-3,v
true
syncat
3-4,np
syncat
3-4,pn
true
FIGURE
59.
Graphical
proof
trees
One
potential
disadvantage
with
proof
trees
is
that
they
may
grow
very
large.
Proof
trees
are
always
very
detailed
and
explicit
about
every
step
in
a
derivation.
This
means
that
for
any
but
the
smallest
proofs,
huge
trees
are
produced.
Such
wordy
and
detailed
explanations
often
work
against
their
own
purpose,
which
is
to
clarify
matters,
not
to
blur
them.
What
can
be
done
about
this?
The
following
strategies
are
well-known
in
the
literature:
1
we
can
restrict
the
explanation
to
one
level
at
a
time
and
allow
the
user
to
ask
for
more
if
necessary;
2
we
can
hide
certain
230
Explanation
aspects
of
the
information
in
each
step;
3
very
simple
and
to
the
user
obvious
steps
in
the
derivation
do
not
have
to
be
explained;
they
can
simply
be
bypassed.
The
modes
presented
in
the
following
sections
implement
or
at
least
contain
elements
from
these
strategies.
13.4.2
Explanation
dialogue
Another
way
for
the
TagLog
system
to
explain
a
statement
a
way
that
implements
strategy
1
above
is
through
a
dialogue.
The
user
has
the
initiative
in
such
a
dialogue.
He
asks
for,
and
is
presented
with,
instances
of
the
universal
statements
that
were
used
to
directly
derive
the
explanandum,
and
is
invited
to
inspect
one
at
a
time
the
statements
used
in
order
to
derive
the
premises
of
these
statements,
and
so
on.
Basically,
this
is
a
way
to
present
the
user
with
a
folded
version
of
a
proof
tree,
and
to
let
him
unfold
it
himself,
step
by
step,
and
in
the
particular
order
that
he
desires.
As
an
example,
the
dialogue
beginning
in
Figure
60
has
been
initiated
by
the
user
selecting
the
statement
syncat
20-23,pp
in
the
Statement
Browser,
and
clicking
the
Explain
button.
The
:-
symbol
after
the
statement
syncat
21-23,np
on
the
third
row
indicates
that
it
can
be
further
explained.
FIGURE
60.
Explanation
dialogue
The
user
decides
to
look
closer
at
the
internals
of
the
noun
phrase
and
double-clicked
on
the
third
row
in
order
to
do
so.
The
screen
is
changed
into
Figure
61.
FIGURE
61.
Explanation
dialogue
cont.
Explanation
and
Presentation
231
In
Figure
62,
the
user
has
selected
the
statement
syncat
22-23,n
and
the
system
has
selected
the
corresponding
segment
in
the
text.
The
dot
after
the
statement
indicates
that
it
cannot
be
explained
any
further.
FIGURE
62.
Explanation
dialogue
cont.
Note
the
important
role
played
by
deixis
in
this
dialogue.
The
segment
involved
in
the
explanation
is
always
automatically
selected
in
the
Text
window.
13.4.3
Analysis
tree
Some
proof
trees
the
ones
involving
grammar
statements
only
can
be
transformed
into
something
that
linguists
are
more
used
to,
namely
analysis
trees.
In
a
proof
tree
corresponding
to
a
proof
involving
grammar
statements
only,
all
nodes
have
the
form
P
S,V
.
The
nodes
in
the
corresponding
analysis
tree
are
formed
by
V
only.
An
analysis
tree
is
required
to
have
leaves
that
are
strings,
so
if
the
pre-terminal
nodes
of
the
proof
tree
do
not
involve
strings
i.e.
if
P
is
not
bstring
or
string
the
strings
corresponding
to
the
segments
S
are
used
instead.
As
an
example,
the
analysis
tree
in
Figure
63
corresponds
to
the
proof
tree
produced
in
247
.
s
np
pn
v
vp
np
pn
John
loves
Mary
FIGURE
63.
Analysis
tree
Note
that
since
analysis
trees
in
TagLog
are
just
notational
variants
of
proof
trees,
then,
contrary
to
some
of
the
basic
assumptions
of
some
modern
theories
of
syntax
e.g.
GB
,
analysis
trees
do
not
have
any
special
ontological
status.
They
do
not
reflect
any
property
of
the
linguistic
real-
232
Explanation
ity;
rather,
they
are
reflections
of
our
way
of
representing
and
reasoning
about
this
reality.1
13.4.4
General
statement
A
very
natural
way
to
give
an
explanation
is
to
provide
it
in
the
form
of
one
universal
statement
that
implies
the
explanandum,
i.e.
in
the
form
indicated
in
252
rather
than
in
the
form
given
in
251
.
251
U:
What
s
that?
S
:
That
s
an
S
U:
WHY
is
that
an
S?
S
:
It
consists
of
an
NP
which
consists
of
a
PN
followed
by
a
VP
consisting
of
a
V
followed
by
an
NP
consisting
of
a
PN
U:
What
s
that?
S
:
That
s
an
S
U:
WHY
is
that
an
S?
S
:
A
PN
followed
by
a
V
followed
by
a
PN
is
always
an
S.
252
Such
a
universal
statement
can
be
obtained
by
means
of
a
machine
learning
technique
called
Explanation-based
generalization
Mitchell,
Keller,
Kedar-Cabelli,
1986
.
Explanation-based
generalization
is
a
technique
for
automatically
analysing
and
generalizing
explanations
consisting
of
proof
trees
in
order
to
produce
universal
statements
whose
antecedents
consist
only
of
predications
that
are
deemed
operational.
In
TagLog,
predications
are
operational
if
they
involve
built-in
predicates,
or
if
they
are
declared
as
operational
by
means
of
the
operational
1
predicate.
For
example,
with
a
declaration
of
operational
predicates
as
in
253
,
253
operational
syncat
,pn
.
operational
syncat
,v
.
the
statement
in
254
corresponds
to
the
proof
tree
in
247
.
1.
Note
that
there
are
modern
theories
of
grammar
that
are
not
so
much
interested
in
constituent
structure
as
such,
notably
HPSG
and
most
versions
of
categorial
grammar.
Explanation
and
Presentation
233
254
syncat
P0-P3,s
:syncat
P0-P1,pn
,
syncat
P1-P2,v
,
syncat
P2-P3,pn
.
The
statement
in
254
is
derived
from
a
proof
involving
grammar
statements
only,
but
the
procedure
works
for
any
kind
of
TagLog
proof.
Note
that
statements
produced
by
explanation-based
generalization
follow
deductively
from
the
current
theory,
and
therefore
do
not
add
any
new
information
to
it.
However,
since
the
use
of
specialized
statements
in
derivations
give
shorter
proofs
than
the
use
of
more
general
statements,
the
output
from
explanation-based
generalization
may
very
well
be
used
to
replace
other
more
general
statements
in
the
current
theory,
in
order
to
trade
a
loss
of
generality
for
efficiency
of
use.
See
Samuelsson
Rayner
1991
for
an
application
of
this
technique
to
NLP
systems.
Moreover,
it
is
certainly
also
conceivable
that
a
theoretician
finds
that
the
specialized
statement
is
sufficient,
in
the
sense
that
it
captures
all
relevant
empirical
regularities,
and
that
the
general
formulation
was
too
general,
and
can
be
dropped.
13.4.5
General
grammar
rule
When
it
comes
to
the
explanation
of
grammatical
facts,
it
is
natural
to
give
an
explanation
of
why
a
segment
must
be
regarded
as
an
instance
of
a
particular
syntactic
construction
as
one
grammar
rule
that
accounts
for
the
construction,
regardless
of
whether
the
rule
is
explicitly
given
in
the
grammar
or
not.
For
example,
the
rule
corresponding
to
the
proof
tree
in
247
is
shown
in
255
.
255
s
--
pn,
v,
pn.
Note
that
this
rule
in
analogy
to
the
general
statement
case
is
not
necessarily
explicit
in
the
grammar;
it
may
only
be
the
case
that
it
follows
deductively
from
the
grammar.
Such
a
rule
may
not
be
what
we
want
to
have
in
our
grammar;
often,
grammar
statements
ought
to
be
more
general.
But
as
an
explanation,
it
is
perfectly
adequate,
and
very
natural,
to
present
the
user
with
a
specific
rule
like
this.
234
Explanation
13.5
Abductive
Explanations
We
will
now
consider
the
kind
of
explanation
where
the
explanandum
is
deducible
only
if
certain
assumptions
are
made.
Let
256
illustrate
this
kind
of
explanation.
256
U:
What
s
that?
S
:
That
s
an
NP
U:
WHY
is
that
an
NP?
S
:
Assuming
the
instance
of
the
is
a
DET,
it
consists
of
a
DET
followed
by
a
NOUN
This
is
a
kind
of
abductive
inference.
It
can
be
argued
that
the
inference
process
that
underlies
explanation
is
abductive,
and
that
explanation
as
deduction
is
just
that
special
case
of
explanation
as
abduction
where
the
number
of
assumptions
being
made
is
zero.
modifies
in
order
to
account
for
the
possibility
of
assuming
statements
instead
of
proving
them.
Given
is
a
set
of
TagLog
statements,
forming
a
theory.
Among
these
statements
is
a
set
that
describes
a
particular
situation
e.g.
a
particular
object,
event,
or
state,
or
a
particular
segment
of
text
.
One
or
more
of
these
statements
are
singled
out
as
demanding
explanation.
Explanation
is
provided
by
showing,
by
deduction,
that
a
theory
just
like
the
one
in
question,
which
does
not
contain
the
statements
at
issue,
but
which
may
contain
a
number
of
additional
assumptions,
would
entail
these
statements.
A
brief
explanation
of
the
notion
of
abduction
is
in
order
at
this
point.
Abduction
can
be
characterized
as
a
deductive
process
that
simultaneously
is
able
to
generate
the
assumptions
that
are
needed
for
deduction
to
succeed.
Formally:
when
trying
to
prove
A,
hypothesize
B
if
A
:-
B
is
known.
Abduction
can
sometimes
be
conclusive.
This
is
the
case,
for
instance,
when
we
are
dealing
with
a
closed
world.
But
in
general,
abduction
is
not
conclusive.
There
may
be
multiple
competing
explanations,
many
candidate
hypotheses.
The
Logic
of
Abductive
Explanations
235
The
evaluation
of
the
goodness
of
candidate
hypotheses
is
an
open
research
problem.
Among
the
criteria
for
such
goodness
we
find
both
absolute
constraints
and
mere
preferences:
Sets
of
abductive
assumptions
inconsistent
with
the
rest
of
the
theory
are
disregarded
altogether.
Explanations
requiring
the
fewest
abductive
assumptions,
the
shortest
possible
proofs,
etc.
are
preferred.
Since
TagLog
is
designed
as
an
interactive
system,
constraints,
but
not
preferences,
are
automatically
enforced.
It
is
up
to
the
user
to
express
his
preferences
by
manually
selecting
one
explanation
from
a
set
of
alternative
ones.
As
we
will
see
in
the
sections
to
come,
TagLog
is
designed
to
present
the
user
with
the
alternative
explanations,
and
to
provide
the
means
of
choosing
between
them.
13.6
The
Logic
of
Abductive
Explanations
The
TagLog
abductive
prover
is
implemented
as
a
predicate
prove
2.
This
predicate
will
be
called
with
the
first
argument
instantiated
to
a
goal,
and
will
bind
the
last
argument
to
the
resulting
proof
tree.
As
a
side-effect,
the
current
theory
will
contain
those
singular
statements
that
the
abducer
needed
to
assume
in
order
to
make
the
goal
follow
from
the
background
theory.
In
fact,
only
a
rather
small
change
is
needed
in
order
to
extend
the
deductive
prover
in
243
to
allow
abductive
proofs:
an
extra
clause
is
added
so
that
instead
of
failing
when
a
singular
statement
cannot
be
proved,
the
statement
is
assumed,
by
being
added
to
the
current
theory.
The
code
for
such
a
simple
abducer
can
be
given
as
follows:
257
prove
true,true
.
prove
A,A
true
:builtin
predicate
A
,
!,
call
A
.
prove
A,A
BProof
:clause
A,B
,
prove
B,BProof
.
236
Explanation
prove
A,B
,
ProofA,ProofB
:prove
A,ProofA
,
prove
B,ProofB
.
prove
A,A
assumed
:not
A,
assumable
A
,
ensure
grounded
A
,
consistent
A
,
assume
A
.
The
first
four
clauses
are
explained
in
Section
13.3.
The
fifth
clause
says
that
in
order
to
prove
A
abductively
and
build
a
branch
A
assumed
in
the
proof
tree,
check
that
A
is
not
already
in
the
current
theory,
make
sure
that
A
is
assumable,
that
it
is
ground
or
can
be
grounded
,
and
that
it
would
be
consistent
to
add
it
to
the
current
theory,
then
assume
it.
The
predicate
ensure
grounded
1
must
implement
an
appropriate
strategy
for
instantiating
variables
before
literals
containing
them
are
assumed.
The
consistent
1
predicate
is
explained
in
Chapter
10.
The
assume
1
predicate
must
implement
a
soft
,
backtrackable
assert.
For
example,
given
the
theory
in
258
,
and
the
declaration
in
259
,
258
p.
p
:-
q,
r.
r.
assumable
q
.
259
the
proof
in
260
is
produced.
260
!-
explain
p,
Proof
.
Proof
p
q
assumed,r
true
Note
that
257
is
by
no
means
an
optimal
implementation
of
an
abductive
prover.
First,
it
relies
on
the
database
manipulation
predicates
of
Prolog
hidden
inside
assume
1
,
which
are
known
to
be
computationally
costly.
Secondly,
the
consistency
checking
procedure
is
very
expensive
too.
These
factors
do
perhaps
contribute
to
making
the
abductive
prover
too
inefficient
to
be
of
any
real
practical
use.
A
description
of
the
actual
implementation
and
optimization
of
an
abductive
prover
suitable
for
use
in
the
TagLog
system
is
beyond
the
scope
of
this
thesis,
however.
The
TagLog
Explanation
Tool
237
13.7
The
TagLog
Explanation
Tool
13.7.1
The
Explanation
Tool
layout
The
TagLog
Explanation
Tool
supports
explanation
of
a
ground
goal,
or
a
conjunction
of
ground
goals.
FIGURE
64.
The
TagLog
Explanation
Tool
The
Explanation
Tool
contains
one
text
entry
field,
one
button,
one
popup
menu,
and
one
checkbox.
The
statements
to
be
explained
are
entered
in
the
text
field.
They
can
be
entered
by
hand
in
the
text
field,
but
they
are
usually
placed
there
automatically,
via
a
link
from
the
Statement
Browser.
Explain:
Clicking
this
button
produces
an
explanation
of
the
statements
in
the
text
entry
field,
and
presents
it
in
the
mode
currently
selected.
If
the
Allow
Assumptions
checkbox
is
selected,
the
explanation
can
be
abductive.
Allow
Assumptions:
Select
this
checkbox
if
you
want
to
allow
the
explanations
to
be
based
on
abductive
proofs.
Mode:
The
current
mode
of
explanation
is
displayed.
Select
the
mode
you
want
from
the
menu.
Currently,
the
TagLog
system
supports
five
modes
of
explanation:
Proof
tree:
The
proof
of
the
conclusion
is
displayed
in
its
entirety
in
the
form
of
a
graphical
tree.
Explanation
dialogue:
The
user
is
presented
with
the
instance
of
the
universal
statement
that
was
used
to
directly
derive
the
conclusion,
and
is
invited
to
inspect
one
at
a
time
the
statements
used
in
order
to
derive
the
premises
of
this
statement,
and
so
on.
238
Explanation
Analysis
tree:
The
proof
of
the
conclusion
is
displayed
as
an
analysis
tree.
General
statement:
The
user
is
presented
with
a
specialized
universal
statement
that
allows
the
derivation
of
the
conclusion
in
just
one
step
from
the
singular
statements
of
the
theory.
General
rule:
This
is
the
general
statement
again,
but
formulated
in
the
grammar
notation.
13.7.2
How
to
use
the
Explanation
Tool
Suppose
that
a
segment
in
the
current
text
is
selected,
that
the
Statement
Browser
is
active
and
that
it
displays
the
statements
describing
the
selected
segment.
1.
Among
the
statements
in
the
Statement
Browser,
select
one
or
more
statements
that
you
want
the
system
to
explain.
2.
Click
the
Statement
Browser
s
Explain
button.
A
conjunction
of
the
statements
selected
in
the
Statement
Browser
is
sent
to
the
Explanation
Tool,
and
is
explained
in
the
mode
selected
there.
That
is,
a
dialogue
is
initiated,
or
a
proof
tree,
a
parse
tree,
a
general
statement,
or
a
general
rule
is
displayed.
3.
Should
more
than
one
explanation
be
available,
only
the
first
one
generated
is
displayed
in
step
2.
The
others
can
be
generated
on
demand.
4.
Should
an
explanation
involve
assumptions,
they
can
be
asserted
by
selecting
the
Assert
Assumptions
command
from
the
Tag
menu.
13.8
Explaining
Syntax
Figure
65
shows
the
TagLog
Explanation
Tool
in
action.
The
segment
corresponding
to
In
which
area
has
been
selected
in
the
text.
The
Statement
Browser
shows
this
segment
to
be
a
prepositional
phrase,
a
fact
derived
from
the
grammar
in
the
window
on
the
left.
The
user
has
selected
the
corresponding
statement
in
the
Statement
Browser,
and
clicked
the
Explain
button
in
order
to
open
the
Explanation
Tool.
Three
kinds
of
explanations
have
been
generated,
one
at
a
time,
according
to
different
settings
of
the
Mode
menu:
one
explanation
dialogue,
one
proof
Explanation
and
Logic-Based
Tagging
239
tree,
and
one
analysis
tree.
In
this
case,
the
General
statement
mode
and
the
General
rule
mode
not
shown
above
would
have
presented
the
user
with
a
grammar
statement
syncat
P0-P3,p
:-
syncat
P0-P1,prep
,
syncat
P1-P2,det
,
syncat
P2-P3,n
.,
and
a
grammar
rule
pp
--
prep,
det,
n.,
respectively.
FIGURE
65.
Explanation
in
TagLog
13.9
Explanation
and
Logic-Based
Tagging
One
of
the
advantages
with
the
logic-based
approach
to
automatic
tagging
sketched
in
Chapter
11
is
that
the
TagLog
explanation
facility
may
be
used
for
debugging
theories
from
which
the
assignments
of
tags
follow
as
logical
consequences.
Suppose
our
current
theory
contains
the
following
clauses:
261
lexcat
S,C
:-
bstring
S,W
,
lexicon
W,C
,
consistent
lexcat
S,C
.
lexcat
P0-P1,inf
:-
lexcat
P1-P2,det
.
240
Explanation
lexicon
to
,inf
.
lexicon
to
,prep
.
lexicon
the
,det
.
In
Figure
66,
a
segment
in
the
Brown
corpus
text
has
been
selected
so
that
a
description
of
it
has
been
displayed
in
the
Statement
Browser.
One
of
the
statements
of
this
descriptions
has
been
selected
and
the
Explain
button
has
been
clicked.
The
Explanation
Tool
explains
why
the
selected
segment
is
a
described
as
preposition:
The
selected
segment
is
an
instance
of
to
,
the
lexicon
lists
to
as
a
preposition,
and
the
description
of
the
selected
segment
as
a
preposition
is
consistent
with
the
rest
of
what
is
known.
Therefore,
the
selected
segment
is
a
preposition.
FIGURE
66.
Explanation
and
automatic
tagging
But
why
is
it
not
an
infinitive
marker?
Well,
if
we
select
the
sentence
lexcat
82-83,inf
in
the
Statement
Browser
and
click
Explain,
we
get
the
screen
in
Figure
67,
where
the
Explanation
Tool
explains
to
us
that
the
selected
segment
is
not
an
infinitive
marker,
since
the
basic
segment
that
follows
immediately
after
is
a
determiner.
Lemmatization
as
Abduction
241
FIGURE
67.
Explanation
and
automatic
tagging
cont.
Now,
if
the
second
row
in
the
Explanation
dialogue
is
double-clicked,
the
instance
of
the
immediately
to
the
right
of
the
segment
82-83
will
be
selected,
and
the
browser
will
contain
an
explanation
of
why
83-84
is
a
determiner.
Thus,
the
Explanation
Tool
will
always
try
to
put
its
finger
on
the
segment
currently
being
explained.
13.10
Lemmatization
as
Abduction
In
the
world
of
lexicology,
a
lemma
is
usually
thought
of
as
something
that
is
constant
across
different
inflected
forms
of
a
word,
something
closely
related
to
the
meaning
of
the
word.2
By
lemmatizing
we
mean
the
process
of
categorizing
word-long
segments
of
texts
as
belonging
to
certain
lemmas.
This
is
by
no
means
an
easy
task,
at
least
not
for
a
2.
Sometimes
this
is
called
a
lexeme
,
while
the
term
lemma
is
used
for
the
formal
paradigm
only.
242
Explanation
machine,
since
there
are
lots
of
ambiguous
forms
like
leaves
,
saw
,
and
question
.
In
this
section,
I
present
an
abductive
solution
to
the
problem
of
lemmatizing.
In
Section
4.7
see
example
85
I
hinted
at
a
formulation
of
the
logic
of
a
lexicon
that
allows
us
to
deduce
strings
or
syntactical
categories,
given
knowledge
about
lemmas.
But
if
what
we
have
is
positive
information
about
word
forms
and
what
we
want
to
infer
is
information
about
lemmas
and
this
is
the
case
when
lemmatizing
,
deduction
is
useless.
The
implication
arrow
is
pointing
in
the
wrong
direction,
and
we
can
deduce
nothing.
Kowalski
1990
argues
that
if
we
are
willing
to
employ
abduction
as
a
mode
of
inference,
we
can
represent
our
knowledge
in
a
natural
and
true
way,
and
still
be
able
to
infer
what
needs
to
be
inferred.
It
can
be
argued,
I
believe,
that
for
this
reason
the
assignment
of
lemmas
to
segments
ought
to
be
performed
by
abduction,
rather
than
by
deduction.
Thus
we
do
not
have
to
take
to
arrow
hacking
,
to
use
an
expression
coined
by
Hobbs
et
al.
1993
.
For
an
example
in
the
abstract,
consider
the
grammar
in
262
written
in
the
style
introduced
in
Section
4.7
,
the
integrity
constraint
in
263
,
and
the
declarations
in
264
.
262
syncat
P0-P2,np
:-
syncat
P0-P1,det
,
syncat
P1-P2,n
.
bstring
S,
the
:-
lemma
S,the
.
bstring
S,
saw
:-
lemma
S,saw
,
number
S,sg
.
bstring
S,
saw
:-
lemma
S,see1
,
tense
S,past
.
syncat
S,det
:-
lemma
S,the
.
syncat
S,n
:-
lemma
S,saw
.
syncat
S,v
:-
lemma
S,see1
.
263
264
lemma
S,X
:-
lemma
S,Y
,
X
Y.
assumable
lemma
,
.
assumable
tense
,
.
assumable
number
,
.
Suppose
that
we
observe
the
following:
265
bstring
1-2,
the
,
bstring
2-3,
saw
,
syncat
1-3,np
We
would
then
be
able
to
abduce
the
statement
in
266
.
266
lemma
2-3,saw
.
Lemmatization
as
Abduction
243
Figure
68
is
a
proof
tree
corresponding
to
an
abductive
proof
of
265
.
Note
that
the
assumption
in
266
is
used
twice
in
the
proof.
bstring
1-2,
the
,
bstring
2-3,
saw
,
syncat
1-3,np
bstring
1-2,
the
lemma
1-2,the
assumed
bstring
2-3,
saw
lemma
2-3,saw
assumed
number
2-3,sg
assumed
syncat
1-3,np
syncat
1-2,det
lemma
1-2,the
true
syncat
2-3,n
lemma
2-3,saw
true
FIGURE
68.
Lemmatization
as
abduction
This
is
in
fact
the
only
solution.
Why
are
there
no
alternative
incorrect
solutions?
Suppose
lemma
2-3,see1
is
assumed
as
a
possible
explanation
for
bstring
2-3,
saw
.
But
according
to
the
lexicon,
this
is
a
verb,
and
the
grammar
does
not
allow
the
last
part
of
a
noun
phrase
to
be
parsed
as
a
verb.
It
does
allow
it
to
be
parsed
as
a
noun,
however,
but
only
if
lemma
2-3,saw
is
assumed.
But
then
an
inconsistency
arises,
due
to
the
integrity
constraints,
which
means
that
this
assumption
is
actually
not
allowed.
In
fact,
the
only
set
of
assumptions
consistent
with
the
integrity
constraints
is
the
one
contained
in
the
tree
displayed
in
Figure
68.
In
my
opinion,
this
is
a
theoretically
sound
and
quite
elegant
procedure
for
automatic
lemmatization,
informed
by
both
a
lexicon
and
a
syntactic
environment.
It
is
also
consistent
with
the
common-sense
view
of
what
lexical
disambiguation
consists
in:
we
observe
that
the
segments
in
question
are
instances
of
the
strings
the
and
saw
,
respectively,
and
we
observe
that
they
form
a
noun
phrase.
The
only
set
of
assumptions
consistent
with
the
observations
and
various
other
constraints
is
that
the
instance
of
the
is
a
determiner
and
that
the
instance
of
saw
is
a
noun
rather
than
a
verb
.
To
give
a
more
concrete
example,
lemmatization
as
abduction
can
be
performed
as
follows
in
the
TagLog
environment.
In
Figure
69,
the
Statement
Browser
displays
the
statements
that
describe
the
segment
selected
in
the
Text
window.
With
Predicted
unchecked,
and
Internal
checked,
the
user
has
selected
the
statement
bstring
1-2,
saw
and
has
then
clicked
the
Explain
button
in
order
to
demand
an
explanation
of
this
statement.
Two
different
explanations
have
been
found,
and
both
of
them
are
currently
shown
in
the
proof
tree
mode
by
the
Explanation
Tool.
The
user
has
244
Explanation
found
that
the
second
one
is
actually
the
correct
explanation,
and
he
is
therefore
about
to
tell
the
system
to
accept
the
assumptions
lemma
23,saw
and
number
2-3,pl
,
and
thus
adding
them
to
the
current
theory.
FIGURE
69.
Lemmatization
as
abduction
cont.
But
the
point
of
doing
lemmatization
as
abduction
does
not
become
clear
until
the
user
selects
a
slightly
bigger
segment
of
text.
In
Figure
70,
the
user
has
done
just
that
and
has
asked
for
an
explanation
of
the
statements
that
describe
it.
Only
one
explanation
has
been
found,
shown
in
the
proof
tree
mode,
and
the
user
is
about
to
tell
the
system
to
accept
it.
As
a
result,
the
statements
lemma
1-2,the
,
lemma
2-3,saw
and
number
2-3,sg
are
added
to
the
current
theory.
FIGURE
70.
Lemmatization
as
abduction
cont.
Interpretation
as
Abduction
Revisited
245
So
we
see
that
by
looking
at
a
larger
segment
of
text
at
a
time,
the
user
did
not
have
to
make
a
manual
selection
between
two
alternative
hypotheses;
the
system
knew
enough
to
be
able
to
filter
out
the
inconsistent
alternative.
